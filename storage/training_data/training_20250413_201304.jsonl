{"instruction": "Title: Enhancing Discrete Choice Models with Representation Learning\nPaper:\nEnhancing Discrete Choice Models with Representation Learning\r\nBrian Sifringera\r\n, Virginie Lurkinb\r\n, Alexandre Alahia\r\naVisual Intelligence for Transportation Laboratory (VITA), Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne \u00b4\r\n(EPFL), Switzerland.\r\nbDepartment of Industrial Engineering & Innovation Sciences, Eindhoven University of Technology,\r\nEindhoven 5600MB, The Netherlands\r\nAbstract\r\nIn discrete choice modeling (DCM), model misspecifications may lead to limited predictability and biased parameter estimates. In this paper, we propose a new approach for estimating\r\nchoice models in which we divide the systematic part of the utility specification into (i) a\r\nknowledge-driven part, and (ii) a data-driven one, which learns a new representation from\r\navailable explanatory variables. Our formulation increases the predictive power of standard\r\nDCM without sacrificing their interpretability. We show the effectiveness of our formulation by augmenting the utility specification of the Multinomial Logit (MNL) and the Nested\r\nLogit (NL) models with a new non-linear representation arising from a Neural Network (NN),\r\nleading to new choice models referred to as the Learning Multinomial Logit (L-MNL) and\r\nLearning Nested Logit (L-NL) models. Using multiple publicly available datasets based on\r\nrevealed and stated preferences, we show that our models outperform the traditional ones,\r\nboth in terms of predictive performance and accuracy in parameter estimation. All source\r\ncode of the models are shared to promote open science.\r\nKeywords: Discrete choice models, Neural networks, Utility specification, Machine\r\nlearning, Deep learning\r\n1. Introduction\r\nDiscrete Choice Models (DCM) have emerged as a powerful theoretical framework for\r\nstudying choices made by humans. The goal of these models is to predict the choice outcome\r\namong a given set of discrete alternatives (e.g., choice of walking as a transportation mode,\r\nrather than taking the car or the bus), while understanding the behavioral process that led to\r\nthe specific choice. For many years, the Multinomial Logit model (MNL), based on a simple\r\nparametric utility specification, has provided the foundation for the analysis of discrete\r\nchoice. Despite its oversimplified assumptions regarding the actual decision-making process,\r\nMNL is still commonly used in practice since it enables a high level of interpretability.\r\nInterpretability is critical for researchers and practitioners. For instance, parametric\r\nspecifications based on linear or logarithmic functions allow for straightforward derivation of\r\nthe value-of-time (VOT), i.e., the marginal rate of substitution between time and cost, that\r\n\r\nc 2020. This manuscript version is made available under the CC-BY-NC-ND 4.0 license\r\nhttp://creativecommons.org/licenses/by-nc-nd/4.0/\r\n1\r\narXiv:1812.09747v3 [stat.ML] 4 Sep 2020\r\nconstitutes a highly relevant measure in a wide range of public transport policy. However,\r\ninterpretability gain from using an MNL model with such simple parametric specifications\r\noften leads to a sacrifice in the predictive power of the model. Indeed, MNL does not allow\r\nto incorporate individual heterogeneity explicitly in the empirical investigation, and the\r\nassumed simple parametric model cannot adequately capture the full underlying structure\r\nof the data. Several prior studies, within the discrete choice literature, have shown that\r\nincorrectly assuming a parametric utility specification can cause severe bias in parameter\r\nestimates and post-estimation indicators (e.g., Torres et al. (2011), Van Der Pol et al. (2014)).\r\nMore advanced utility specifications, embedded into more complex models, would allow\r\nfor a better fit of the data and, ultimately, a better prediction. In this regard, utility specifications based on Box-Cox transformation, piecewise-linear, exponential, or non-parametric\r\nfunctions have been proposed in the literature (e.g., Schindler et al. (2007), Kneib et al.\r\n(2007), Kim et al. (2016)), as well as advanced DCM models, such as the Mixed Logit Model\r\n(MLM) or the Latent Class Model (LCM) (e.g., Shen (2009), Xiong and Mannering (2013),\r\nVij et al. (2013), Kim et al. (2016)). However, the main limitation of these choice models\r\nis that the standard procedure for estimating the parameters\u2019 values is to assume that the\r\nmodel specification is known a priori, while in practice determining the utility specification\r\nfor a particular application remains a difficult task.\r\nWithin the machine learning community, Neural Networks (NN) masters the field of\r\nrepresentation learning. They have become the go-to solutions, as their superior prediction\r\nperformance has been repeatedly demonstrated, including in transportation applications\r\n(e.g., Omrani (2015), Hagenauer and Helbich (2017), Pekel and Soner Kara (2017), Nam\r\net al. (2017), Pirra and Diana (2018), Chang et al. (2019), Zhou et al. (2019)). Unlike\r\ndiscrete choice models, NN requires essentially no a priori beliefs about the nature of the\r\ntrue underlying relationships among variables. However, gaining in prediction accuracy often\r\ncomes at the cost of losing interpretability, explaining why the discrete choice community\r\noften considers them as a black-box.\r\nIn this work, we propose a new approach to discrete choice modeling by integrating representation learning in the formulation. We propose to divide the systematic part of the utility\r\nspecification into (i) a knowledge-driven (interpretable) part and (ii) a data-driven (representation learning) part, which aims at automatically discovering a good utility specification\r\nfrom available data. Our formulation partially replaces manual utility specification and\r\nallows for a better prediction performance without sacrificing interpretability. We demonstrate the effectiveness of our framework by augmenting the utility specification of the MNL\r\nand Nested logit models with a new non-linear representation, arising from a neural network. It leads to new choice models, referred to as the Learning Multinomial Logit (L-MNL)\r\nand Learning Nested Logit (L-NL) models, respectively. The source code is made publicly\r\navailable to ease reproducibility and promote open science1\r\n.\r\nThe remainder of the paper is organized as follows: Section 2 is a non-exhaustive overview\r\nof the recent attempts to conduct multi-disciplinary research, combining machine learning\r\n(ML) and discrete choice modeling. Section 3 gives a brief background and presents details\r\non how we implement MNL with modern neural network libraries. Section 4 describes our\r\n1https://github.com/BSifringer/EnhancedDCM\r\n2\r\nproposed L-MNL and L-NL models. Section 5 demonstrates evidence that our learning\r\nchoice models outperform both existing hybrid neural network and MNL models based on\r\nsimple parametric utility specifications. Our results on real and synthetic data show that\r\nthe research community can benefit from a better integration of representation learning\r\ntechniques and discrete choice theory. In that sense, our research also contributes to bridging\r\nthe gap between knowledge-driven and data-driven methods.\r\n2. Related work\r\nDiscrete choice and machine learning researchers have typically different perspectives and\r\nresearch priorities. The related work in each field is too broad to be covered here exhaustively,\r\nand we, therefore, limit ourselves to the most relevant works at the intersection of the two\r\ncommunities.\r\nMore than 20 years ago, researchers were already interested in applying data-driven\r\nmethods such as neural networks in different transportation applications (e.g., Faghri and\r\nHua (1992), Dougherty (1995), Shmueli et al. (1996)). Over the years, different types of\r\nNN, as well as decision trees, have been compared to MNL models (Agrawal and Schorling\r\n(1996), Lee et al. (2018), Zhao et al. (2020)), Nested Logit (NL) models (Mohammadian and\r\nMiller (2002), Hensher and Ton (2000)) and more generally to random utility models (Sayed\r\nand Razavi (2000), Cantarella and de Luca (2005), Paredes et al. (2017)) and statistical\r\nmethods (West et al. (1997), Karlaftis and Vlahogianni (2011), Iranitalab and Khattak\r\n(2017), Golshani et al. (2018), Brathwaite et al. (2017)). However, the literature has mainly\r\nfocused on comparing the models in terms of prediction power, without discussing the issue\r\nof interpretability.\r\nThe machine learning community has also generated a tremendous amount of research\r\naiming at predicting, with high accuracy, a variety of choices. Among the most recent\r\nworks, Pekel and Soner Kara (2017) or Jin et al. (2020) present comprehensive reviews of\r\npublications related to the application of NN or ML to predict travelers\u2019 choice in public\r\ntransportation. Hagenauer and Helbich (2017) compare seven machine learning methods to\r\nclassify travel mode choice. The methods investigated are MNL and NN, but also Naive\r\nBayes (NB) (Rish et al. (2001)), Gradient Boosting Machines (GBM) (Friedman (2001)),\r\nBagging (BAG) (Breiman (1996)), Random Forests (RF) (Breiman (2001)), and Support\r\nVector Machine (SVM) (Cortes and Vapnik (1995)). Pirra and Diana (2018) also propose\r\nto use SVM for travel mode choice while Lh\u00b4eritier et al. (2018) rely on RF and GBM to\r\npredict airline itinerary choice. Not surprisingly, the results exhibited in these papers show\r\noutstanding results in the ability of these models to fit the data but do not tackle the issue\r\nof their interpretability.\r\nSome recent publications have gone beyond the comparison of the two fields and proposed innovative behavioral studies based on data-driven methods. Two notable examples\r\nare the work of Wong et al. (2018), who use a restricted Boltzmann Machine (BM) (Ackley\r\net al. (1985)) to represent latent behavior attributes, and the study of van Cranenburgh and\r\nAlwosheel (2019), who develop a novel NN based approach to investigate decision rule heterogeneity amongst travelers. Moreover, Wang et al. (2018c) have investigated the numerical\r\nextraction of behavioral information such as willingness to pay in a Dense Neural Network\r\n3\r\n(DNN) using elasticity while Wang et al. (2018b) have investigated the theoretical transition from DCM to DNN and subsequent trade between predictability and interpretability.\r\nRecently, the main authors have also developed a new structuring of neural networks, based\r\non alternative specific utility architecture (Wang et al. (2020)). Finally, Wong and Farooq\r\n(2020) have made use of a NN generative model to capture the joint distribution of multiple\r\ndiscrete-continous travel data. However, these papers do not have the objective of finding a utility specification that allows high predictability while maintaining straightforward\r\ninterpretability.\r\nThe closest studies relative to our work are in the brand choice literature. Bentz and\r\nMerunka (2000) introduce a hybrid model, afterward called the NN-MNL model. Their\r\nmodel involves a two-stage approach that starts with the estimation of a NN model that\r\naims at discovering non-linearity effects in the utility function. Then, if the NN identifies\r\nsome non-linearities, the specification of the MNL model is modified to include new variables,\r\nspecially created to account for the discovered non-linear effects. On their dataset, the respecified MNL model, which includes the discovered non-linear effects, slightly outperforms\r\nthe basis MNL model. Their work, similar to ours in spirit, aims at achieving better predictive power together with a greater understanding of the influencing factors. However, their\r\napproach is sequential, and their neural network is only used beforehand as a diagnostic and\r\nspecification tool. Again, in the context of brand choice, Hruschka et al. (2002, 2004) and\r\nHruschka (2007) investigate further the potential of neural network based choice models. In\r\nHruschka et al. (2002), the NN-MNL model is compared against homogeneous and heterogeneous versions of linear utility MNL models. Meanwhile, in Hruschka et al. (2004), the\r\ncomparison is made against two non-linear models, the generalized additive (GAM-MNL)\r\nmodel of Abe (1999) and a flexible functional form based on Taylor series approximations.\r\nThe authors show that, by being capable of finding non-linear relationships from available\r\ndata, the neural network based choice models with non-linear specifications outperform the\r\nother models in terms of prediction performance. However, the authors are unable to draw\r\nany conclusion regarding the significance of the factors with this method. Also, the post\r\nestimation analysis is limited to likelihood and choice elasticities. Finally, Hruschka (2007)\r\nproposes a Multinomial Probit (MNP) model with a neural network extension to model\r\nbrand choice. Their model combines the heterogeneity across households with a multilayer\r\nperceptron. Linear and non-linear deterministic utility specifications are considered by specifying the same variables in linear and non-linear terms, hence loosing interpretability. The\r\nprobit assumption also prevents the closed form of the choice probabilities and the model\r\ncan only be estimated using a Markov Chain Monte Carlo simulation technique.\r\nOur work significantly departs from these previous studies by proposing a flexible and\r\ngeneral framework for the specification of the deterministic utility in any DCM model. More\r\nspecifically, our approach combines, in a single joint optimization, the estimation of a standard DCM model, based on a carefully thought-out utility specification, with a representation\r\nlearning model that aims at increasing the overall predictive performance. To our knowledge, this is the first formulation that benefits from the predictive power of representation\r\nlearning techniques while keeping some key parameters interpretable, which allows us to\r\nderive insightful post-estimation indicators. In Section 5, we show the effectiveness of our\r\nlearning choice method by comparing its performance with benchmarking models, including\r\nthe NN-MNL model of Hruschka et al. (2002, 2004).\r\n4\r\n3. Background and Notations\r\n3.1. Multinomial Logit\r\nDiscrete choice modeling complies with the Random Utility Maximization (RUM) theory\r\n(McFadden (1974)), which postulates that an individual is a rational decision-maker, who\r\naims at maximizing the utility relative to their choice. Utility is a latent construct assumed\r\nto be partitioned into two components: a systematic (or deterministic) utility, Vin, and a\r\nrandom component, \u03b5in, that captures the uncertainty coming from the impossibility for the\r\nmodeler to fully capture the choice context. Formally, the utility that individual n associates\r\nwith alternative i from their choice set Cn is given as2\r\n:\r\nUin = Vin + \u03b5in, (1)\r\nFor convenience, we consider the systematic part of the utility to be linear-in-parameter,\r\nas it is generally assumed:\r\nVin =\r\nX\r\nd\r\n\u03b2d \u00b7 xdin, (2)\r\nwhere \u03b2 are the preference parameters (or estimators) associated with the explanatory variables (or the input features) x \u2208 X that describe the observed attributes of the choice\r\nalternative (e.g., the price or travel time associated with the mode), and the individual\u2019s\r\nsocio-demographic characteristics (e.g., the individual\u2019s level of income or age).\r\nUnder the standard MNL assumption that error terms are independently and identically\r\ndistributed (i.i.d.) and follow an Extreme Value Type I distribution with location parameter\r\nzero and the scale parameter 1 (i.e. \u03b5in\r\ni.i.d. \u223c EV (0, 1)), the probability for individual n to\r\nselect choice alternative i is given by\r\nPn(i) = e\r\nVin\r\nP\r\nj\u2208Cn\r\ne\r\nVjn\r\n. (3)\r\nThe preference parameters \u03b2 are typically estimated by maximizing the log-likelihood function given by:\r\nL =\r\nX\r\nN\r\nn=1\r\nX\r\ni\u2208Cn\r\nyin log [Pn(i)] , (4)\r\nwhere yin is the observed choice variable (or true label) and is equal to 1 if the choice of\r\nindividual n is the alternative i, and to 0 otherwise.\r\n3.2. Implementing MNL as a Neural Network\r\nA neural network consists of a function mapping the input space x to an output of interest\r\nU through several intermediate representations commonly referred to as hidden layers h\r\n(j)\r\n:\r\nU = h\r\n(L)\r\n(q\r\n(L\u22121)), (5)\r\nwith q\r\n(j) = h\r\n(j)\r\n(q\r\n(j\u22121)), \u2200j = 1, ..., L, (6)\r\n2Notation convention can be found in Appendix A.\r\n5\r\nwhere q\r\n(0) = x and L is the last representation layer.\r\nWe make use of a Convolutional Neural Network (CNN) to retrieve the MNL formulation3\r\n. A CNN has weights in the shape of a filter that connects a layer h\r\n(j)\r\nto the next by\r\napplying a convolution4\r\n. Therefore, the value of a neuron i in the next layer (j + 1) can be\r\nwritten as:\r\nh\r\n(j+1)\r\ni = g(\r\nX\r\nd\r\nk=0\r\nh\r\n(j)\r\n(s\u00b7i+k)\r\n\u03b2\r\n(j)\r\nk + \u03b1\r\n(j)\r\ni\r\n), (7)\r\nwhere {\u03b21, ...\u03b2d} = \u03b2 is the filter of size (1 \u00d7 d), s the stride of the convolution, \u03b1i a bias\r\nterm and g(\u00b7) an activation function.\r\nThe MNL formulation is retrieved by using a single layer (L = 1), setting the activation\r\nfunction to identity (g(x) = x) and the stride s to d. Doing so, we get the utility functions\r\nVn = {V1n, ...VIn} as defined in Equation (2).\r\nThen, the probabilities can be obtained by using a softmax activation layer (Bishop\r\n(1995)) defined as:\r\n(\u03c3(Vn))i =\r\ne\r\nVin\r\nP\r\nj\u2208Cn\r\ne\r\nVjn\r\n, (8)\r\nwhich can be identified as Equation (3) for all probabilities. The output of the network goes\r\nthrough a loss function, in our case categorical cross-entropy (CE)(Shannon (1948)):\r\nHn(\u03c3, yn\r\n) = \u2212\r\nX\r\ni\u2208Cn\r\nyin log [\u03c3i(Vn)] . (9)\r\nMinimizing (9) is equivalent to maximizing Equation (4) when summed over all individuals\r\nn.\r\nAn illustrative example is given in Figure 1 with :\r\nUin = \u03b2c \u00b7 x1i + \u03b2t\r\n\u00b7 x2i + \u03b5in, \u2200i \u2208 C, (10)\r\nwhere x1 = cost stands for travel cost, and x2 = time for travel time. The choice set C\r\nis assumed to be the same for all individuals and contains the following mode alternatives:\r\nC = {Car, Train, Swissmetro (SM)}. The filter is made of \u03b2 = (\u03b2t\r\n, \u03b2c), and a stride of d = 2\r\nallows to recover each utility specification in the next layer from cost and time of every\r\nalternative.\r\n3Previous studies, such as Bentz and Merunka (2000), have successfully written MNL with other Neural\r\nNetwork architectures. We have used CNN for its weight-sharing architecture.\r\n4 This method is often seen in image processing where a filter with a fix number of weights is applied to\r\nan equal amount of inputs by multiplying the terms together and then summing them over to obtain a single\r\nnew value. A new image is obtained by sliding the filter over all inputs with a set step-size named stride.\r\nIn our case, the operation is done without padding, i.e., without applying the filter out of the boundaries,\r\neffectively reducing the size from one layer (filtering) to the next. In image processing, the correlation\r\nformula is the same as a convolution up to a flip in the order of its entries. Without loss of generality, we\r\nwrite the correlation formula in Equation 7 for readability.\r\n6\r\nCar\r\ncost\r\ntime\r\nTrain\r\ncost\r\ntime\r\nSM\r\ncost\r\ntime\r\n\u03b2c\r\n\u03b2t\r\nVcar\r\nVtrain\r\nVSM\r\nCE Loss\r\nSoftmax\r\nFilter\r\nInput\r\nlayer x\r\nHidden\r\nlayer h\r\n(1)\r\nActivation\r\nfunction\r\nOutput\r\nlayer\r\nFigure 1: By aligning inputs by class and convolving with a filter of equivalent shape and stride, we can\r\nretrieve linear utility specifications with a single CNN layer. By ending the network with a softmax activation\r\nlayer and a cross-entropy (CE) loss, we retrieve the same formulation as for the MNL model.\r\n4. Representation Learning in Discrete Choice Modeling\r\n4.1. General Formulation\r\nAs previously mentioned, the standard statistical procedure for estimating the parameter\r\nvalues of choice models is to assume that the true utility specification is known a priori.\r\nTypically, the most common representation is inspired from linear regression and involves\r\nthe observed attributes of the choice alternative and the individual\u2019s socio-demographic\r\ncharacteristics. However, the influence of these explanatory variables on the utility of a\r\nchoice alternative is unlikely to be known and to be precisely linear. The danger of an\r\nincorrect utility specification remains, therefore, highly present.\r\nIn this work, we propose a more flexible and data-driven approach that consists in expressing the systematic part of the utility function into two sub-parts, as follows:\r\nVin = fi(Xn; \u03b2) + ri(Qn, w), (11)\r\nwhere\r\n- f(Xn; \u03b2) is the knowledge-driven part, assumed interpretable. The function f is defined\r\nsuch that its unknown model parameters \u03b2 are an interpretable combination of the\r\nexplanatory variables (or input features) Xn.\r\n- r(Qn, w) is the data-driven part, a representation that is learned from a set of explanatory variables (or input features) Qn, for which no a priori relationship is assumed.\r\nConditions between X and Q to keep f interpretable are described in section 4.2.\r\nReplacing the systematic utility component in Equation (1) by its new expression given\r\nby Equation (11), we obtain the following utility expression:\r\nUn = f(Xn; \u03b2) + r(Qn; w) + \u03b5n. (12)\r\n7\r\nWe may interpret this equation as a data-driven term finding the residual of a hand-modeled\r\nfunction. He et al. (2016) have shown the impact of residual network (ResNet) in the machine learning community, and its particular architecture has been widely used for increased\r\ninterpretability by finding the residual of knowledge and equation based modeling (Liao et al.\r\n(2018), Wang et al. (2018a), Li et al. (2019)), or choice modeling (Wong and Farooq (2019)).\r\nOur work differs with the use of two input sets X and Q necessary for keeping discrete choice\r\ninterpretability as we define it in section 4.2 and further explain in section 5.\r\nFinally, the above specification may be applied to any discrete choice modeling kernel.\r\nHowever, the following applications will limit themselves to models with Gumbel distributed\r\nrandom terms.\r\n4.2. Modeling\r\nWhen modeling a utility specification with the formulation in Equation (12), one must\r\nunderstand the attributes of a parameter related to either X in f or Q in r. The first\r\nassumes to have all the benefits of expert modeling in discrete choice, which includes the\r\navailability of the \u03b2 parameters\u2019 Hessian and thus their standard deviation approximation.\r\nThe second produces a new representation of its associated input, which can span from\r\ndiscrete to continuous inputs such as signals and images, while the hessian is generally not\r\navailable due to computational complexity. Therefore, when one studies the impact, or\r\nelasticity, of a feature tin on alternative i, this readily translates as:\r\n\u2202Uin\r\n\u2202tin\r\n=\r\n\u2202fin\r\n\u2202tin\r\n+\r\n\u2202rin\r\n\u2202tin\r\n(13)\r\nBentz and Merunka (2000) and Wang et al. (2018c) have shown that behavioral insight\r\ncan be captured and studied through the elasticities of a neural network. However, to ensure\r\nthe same straightforward interpretability of behavioral choice models for a parameter t, we\r\nexplicitly stipulate that the standard deviation must be available, and the elasticity must\r\nnot depend on the representation term. In other words, we impose that:\r\n\u2202rin\r\n\u2202tin\r\n= 0 (14)\r\nEquation (14) constitutes a critical assumption which ensures that insightful post-estimation\r\nindicators can be retrieved. This definition, as well as the general formulation is what sets\r\nus apart from old and new literature alike. Indeed, to only cite a few, Hruschka (2007) or\r\nWong et al. (2018) are specific cases of our formulation when both sets are the same, i.e.,\r\nX = Q, while Bentz and Merunka (2000), Otsuka and Osogami (2016) and Wang et al.\r\n(2018c) are cases where X = \u2205. All of which do not follow our goal of fixing a biased expert\r\nmodeling component by leveraging the strength of data-driven methods in an added and\r\nnon-overlapping representation term.\r\nMoreover, the global theoretical effect of this new term may be derived. When one\r\nconsiders Equation (12) without the new term, then this value would be found in the random\r\ncomponent such that:\r\n\u03b5\u00afin = ri(Qn; w) + \u03b5in. (15)\r\n8\r\nTo have unbiased parameter estimates in f(Xn; \u03b2), one must avoid correlation between the\r\nspecification and the random terms known as endogeneity, avoid correlation of the random\r\nterms between each alternative and overall avoid utility misspecification. Many methods\r\nhave been developed to specifically fix for endogeneity, such as Guevara (2015), while others\r\naccount for correlation between alternatives (see section 4.4). Our model aims at correcting\r\nfor underfit due to misspecification and omitted variable bias thanks to data-driven methods.\r\nIn this sense, the new error term \u03b5in is more likely to fit the aforementioned criteria than \u00af\u03b5in.\r\nThe new representation term increases the probability of having unbiased parameters while\r\nincreasing the prediction rate as it may easily find undiscovered specifications during the\r\nexpert modeling process. Moreover, our general framework does not exclude the coupling\r\nwith other methods that aim at fixing the random term, as we will observe in the following\r\nsections.\r\n4.3. L-MNL Model Formulation\r\nIn the specific case where we choose to add a representation term to a Multinomial Logit\r\nfor Equation (12), the standard MNL assumptions regarding the distribution of the error\r\nterm \u03b5in (described in Section 3.1) have to be made.\r\nThe likelihood of selecting the choice alternative i for individual n, given the values of the\r\nmodel parameters (\u03b2 and w) and the influencing factors (Xn and Qn), is therefore naturally\r\nexpressed as:\r\nPn(i) = e\r\nfi(Xn;\u03b2)+ri(Qn;w)\r\nP\r\nj\u2208Cn\r\ne\r\nfj (Xn;\u03b2)+rj (Qn;w)\r\n. (16)\r\nFor our L-MNL model, we choose to use a Dense Neural Network (DNN) as the learning\r\nmethod. More specifically, its representation term rin is the resulting function of a DNN\r\nwith L layers of H neurons and a single output per utility function:\r\nrin =\r\nX\r\nH\r\nk=1\r\nw\r\n(L)\r\nik g(q\r\n(L\u22121)\r\nn w\r\n(L\u22121)\r\nk + \u03b1\r\n(L\u22121)\r\nk\r\n) + \u03b1\r\n(L)\r\ni\r\n, (17)\r\nwhere g(\u00b7) is the rectifier linear units (ReLU) activation function and q\r\n(j)\r\nn\r\nis recurrently\r\ndefined by:\r\nh\r\nq\r\n(j+1)\r\nn\r\ni\r\ni\r\n=\r\nX\r\nH\r\nk=1\r\nw\r\n(j+1)\r\nik g(q\r\n(j)\r\nn w\r\n(j)\r\nk + \u03b1\r\n(j)\r\nk\r\n) + \u03b1\r\n(j+1)\r\ni\r\n, (18)\r\nwith q\r\n(0)\r\nn being the vector of input features Qn.\r\nThe architecture of our L-MNL hybrid model is shown in Figure 2, where we see both the\r\nlinear component, written as a CNN (see Section 3), and the learned representation term,\r\nobtained through a DNN. Our hybrid approach is both general and flexible. Other functions\r\nof f and other architectures for r may be chosen. For more complex DCM frameworks, new\r\nassumptions must be made on the error terms, probabilities, and likelihood function. For\r\ninstance, an extension of our work to the nested logit model is shown in the next section.\r\n9\r\n. . . . . . class 1 . . . . . .. . . . . . class\r\nI . . . Inputs \u2208 X\r\n\u03b21\r\n\u03b2k . . . . . .\r\n..\r\n..\r\n..\r\n..\r\nf1 . . .\r\nfI\r\nConvolution\r\nFilter\r\nInput\r\nlayer\r\nHidden\r\nlayer\r\nV1 . . .\r\n. . .\r\nVI\r\nSof tmax . . . Inputs \u2208 Q\r\nq1\r\n(1)\r\nqH\r\n(L) . . . . . . . . . . . .\r\n. . .\r\n. . .\r\n. . .\r\n. . .\r\n. . .\r\nr1 . . .\r\nrI\r\nFigure 2: L-MNL model architecture. On the top, we have the I class generalization of a linear-in-parameter\r\nMNL model, as depicted in Figure 1. At the bottom, we have a deep neural network (i.e., multilayer and\r\nfully connected) that enables us to obtain the representation learning term ri\r\n. The terms from each part are\r\nadded together defining the new systematic function of Equation (11).\r\n4.4. L-MNL nested generalization\r\nThe Nested Logit formulation has been first introduced by Williams (1977). It relaxes the\r\ni.i.d. assumption of MNL random terms in the case where alternatives may have correlation\r\namong each other in the decision making process. They are gathered together in nests which\r\nthen affects their final probability such that:\r\nPn(i|C) = Pn(i|m) \u00b7 Pn(m|C)\r\n=\r\ne\r\n\u00b5mVin\r\nP\r\nj\u2208Cm\r\ne\r\n\u00b5mVjn\r\n\u00b7\r\nexp \u0010\r\n\u00b5\r\n\u00b5m\r\nln P\r\np\u2208Cm\r\ne\r\nVpn \u0011\r\nPM\r\nk=1 exp \u0010\r\n\u00b5\r\n\u00b5k\r\nln P\r\np\u2208Ck\r\ne\r\n\u00b5kVpn \u0011 (19)\r\nwhere M is the total number of nests, \u00b5l\r\nis the factor for nest l and Cl\r\nis the set of alternatives\r\nbelonging to that nest. This is the probability of choosing an alternative within the nest m\r\nmultiplied by the probability of choosing m among all nests.\r\nTo add a representation learning term to the Nested Logit, one must only change the\r\nfinal function of L-MNL seen in Equation (8) and adapt it to Equation (19). This is done by\r\n10\r\nremoving the commonly used softmax layer seen in Figure 2 for a custom-built network with\r\nadded trainable weights for the nest factors. The new architecture is depicted in Figure E.10\r\nof Appendix E. Further changes to the network would allow us to easily model Cross-Nested\r\nLogit (Vovsha (1997)) and Generalized Extreme Value models (McFadden (1978)) with an\r\nadded representation term.\r\n5. Experiments\r\nOur experiments demonstrate how our L-MNL and L-NL models increase the predictability of the MNL and NL models respectively while keeping their interpretability. The former,\r\npredictability, is directly quantified in terms of likelihood using two real-world datasets. The\r\nlatter interpretability, however, is more challenging to assess. To do so, we define interpretability as the ability of the model to have both a quantifiable uncertainty of its parameters as well as an understandable or meaningful association with its variables. This latter\r\npoint usually translates as the ability to mathematically derive and interpret the impact, or\r\nelasticity, of a variable. We will use synthetic and semi-synthetic data for which the true\r\nparameters are known to assess the quality of the estimates and the interpretability of every\r\nmodel.\r\n5.1. Benchmarking models\r\nWe want to compare our models (L-MNL and L-NL) against key previous works. As our\r\napproach is both general and flexible, we retrieve previous works with appropriate modeling\r\ndecisions on the X , Q input sets and f, r functions of Equation (12).\r\nEssentially, we compare the following models:\r\n\u2022 Logit(X ): The standard MNL model with linear-in-parameter specification, and no\r\nlearning component. The variables x \u2208 X enter the linear utility specification and\r\nQ = \u2205. The model has |X | parameters.\r\n\u2022 DNN(n, Q): A Dense Neural Network for every alternative with softmax loss. This\r\nis the NN-MNL model proposed by Hruschka et al. (2004) and based on Bentz and\r\nMerunka (2000) with n neurons in the hidden layer (H = n) and a single layer deep.\r\nThe variables x \u2208 Q are the input features of the neural network. There is no variable\r\nentering the linear utility specification (X = \u2205). The model has around n(|Q|+|C|+1)\r\nweights.\r\n\u2022 DNN L(n, X = Q): A modified version of the NN-MNL model proposed by Hruschka\r\net al. (2004). The variables x \u2208 X (= Q) enter both the linear utility specification\r\nand the neural network. The latter can be seen as finding the residual from the linear\r\ninput of X and the overall architecture is close to the concept of the more well known\r\nresidual building block from ResNet (He et al. (2016)). The model has in the order of\r\n(n + 1)(|X | + |C|) weights.\r\n\u2022 L-MNL(n, X , Q): Our learning logit model with n neurons in the hidden layer (H = n).\r\nPart of the variables x \u2208 X enter the linear utility specification and the remaining\r\nvariables q \u2208 Q are given as input features to the neural network. This separation\r\n11\r\nsatisfies the interpretability condition (Equation 14) for all x. The function r(Q) is\r\ndefined by Equation (17) where we limit the complexity to a single layer (L = 1).\r\nTherefore, the model has a total of |X | interpretable parameters and n(|Q| + |C|)\r\nweights. We remind the reader that this architecture for the representation term is for\r\nthe sake of example and more complex networks or methods depend on the datasets\r\nat hand and are open for future research.\r\nUnless specified otherwise, all models have run for 200 epochs with an Adam optimizer\r\n(Kingma and Ba (2014)) running on default parameters from Keras python deep learning\r\nlibrary (Chollet et al. (2015)). Every model with a Neural Network has a 20% dropout\r\nregularizer following the DNN layer. The parameters in every model are initialized by the\r\ndefault random initialization of the used library, and no seed is set. Moreover, in our current\r\nframework, no notable difference in learning time between models are observed.\r\n5.2. Synthetic data\r\nWe now use synthetic data to better analyze the performance of our L-MNL model in\r\nterms of both prediction performance and estimates accuracy. We start by describing how we\r\ngenerated synthetic data. Then, we perform Monte Carlo experiments to compare all benchmarking models on parameters estimation. Then, a scan on increasing number of neurons\r\nis introduced to analyze the impact of the NN architecture better. Finally, we investigate\r\nthe case of strongly correlated variables between input sets X and Q. Additional experiments to study the particular effects of complexly correlated variables, missing variables, or\r\nsequential versus joint optimization strategies can be found in Appendix B. To study the\r\nlimits of the hybrid model, a special case study of very noisy and highly non-linear utility\r\nspecifications using semi-synthetic data can be found in Appendix C. For all experiments,\r\nall benchmarking models remain the same as in the previous Section (5.1).\r\n5.2.1. Data generation\r\nSynthetic data provides a controlled environment for analyzing our L-MNL model. To\r\ngenerate our synthetic data, we consider a simple binary choice model where an individual n\r\nhas to choose between two options (e.g., choosing to use public transportation to go to work\r\nas opposed to taking an alternative mode). Following the generation process of Guevara\r\n(2015), we obtain the following utility specifications for i = 1, 2:\r\nUin = Vin + \u03b5in, (20)\r\nwith\r\nVin = \u03b2p \u00b7 pin + \u03b2a \u00b7 ain + \u03b2b \u00b7 bin\r\n| {z }\r\nknown relation\r\n+ \u03b2qc \u00b7 qincin\r\n| {z }\r\nunknown interactions\r\n(21)\r\npin = 5 + zin + 0.03wzin + \u03b5pin (22)\r\nqin = 2hin + kin + \u03b5qin (23)\r\nkin = hin + \u03b5kin for i = 1, 2 (24)\r\n12\r\nwhere \u03b2x stand for parameters, and all variables ain, bin, cin, zin, wzin, hin and error terms\r\n\u03b5pin, \u03b5qin, \u03b5pin follow a uniform distribution. For the following experiments we consider the\r\nnon-linear interactions among variables to represent unknown and undiscovered causalities\r\nduring the modeling phase. Our data generation process slightly differs from Guevara (2015)\r\non two points: first of all, pin is not correlated with qin as this is kept for an experiment\r\nby itself in Section 5.2.4 and Appendix B.1. Secondly, we have added an interaction term,\r\ncin to allow a simulated situation where the modeler would misspecify the utility due to\r\nan undiscovered interaction. To avoid too large difference in magnitude order among our\r\nvariables as well as the interaction term, we generated all variables as:\r\nxin \u223c U([\u22121, 1]) (25)\r\nUnder the classical logit assumption that \u03b5in\r\ni.i.d. \u223c EV (0, 1) and given fixed values of\r\nparameter estimates \u03b2 in the utility specification, the choice probabilities Pin(xn) are known,\r\nand the synthetic choice yin can then be seen as a Bernoulli random variable that takes the\r\nvalue 1 with probability Pin(xn) and 0 with probability 1 \u2212 Pin(xn). Formally, we assume\r\nthat\r\ny1n \u223c Bern (P1n(xn)), (26)\r\ny2n = 1 \u2212 y1n \u2200n = 1, ...N. (27)\r\nUsing synthetic data allows us to compare our L-MNL model with standard choice models,\r\nnot only in terms of prediction performance, but also in accuracy in parameter estimation\r\nsince we know the true value of parameter estimates \u03b2, i.e., the true linear and non-linear\r\ndependencies between the variables.\r\n5.2.2. Monte Carlo experiment\r\nWe rely on Monte Carlo simulations to investigate how well each model performs in both\r\nprediction and parameter estimation of Equation (21). To do so, we select a true utility\r\nspecification with \u03b2p = \u22121, \u03b2a = 0.5, \u03b2b = 0.5, \u03b2qc = 1. For each experiment, we generate\r\n1, 000 synthetic individual observations for the training set, and 200 more for the testing set.\r\nThe following results are obtained by performing 100 of such experiments.\r\nOur new choice model is defined as\r\n\u2022 L-MNL(25, X , Q), with X = {p, a, b}, and Q = {q, c},\r\nand is compared with the following three benchmarking models and one reference model*5\r\n:\r\n\u2022 Logit(X1), with X1 = {p, a, b, q, c},\r\n\u2022 DNN(25, Q), with Q = {p, a, b, q, c},\r\n\u2022 DNN L(25, X = Q), with X = Q = {p, a, b, q, c},\r\n5The reference model is considered differently than the benchmarks as it breaks the assumption that the\r\nmodeler does not know the ground truth.\r\n13\r\nTable 1: Monte Carlo average log-likelihood (LL\u00af ) and standard deviation (s.d.(LL)) for the different models.\r\nBased on the test set value, we conclude that our L-MNL learns the best general representation. An MNL\r\nmodel with true utility specification is given as a reference. Average of accuracies\r\nModel Train set Test set Accuracies [%] \u03c1\r\n2\r\ntest\r\nLL s.d.(LL) LL s.d.(LL) Acctrain Acctest\r\n*Logit(Xtrue) -459 15 -94 8 78 77 0.32\r\nLogit(X1) -604 11 -123 6 67 66 0.11\r\nDNN(25, Q) -363 21 -112 13 84 74 0.19\r\nDNN L(25, X = Q) -367 18 -108 12 83 75 0.22\r\nL-MNL(25, X , Q) -429 16 \u221297 8 80 76 0.30\r\n\u2022 *Logit(Xtrue), with Xtrue = {p, a, b, qc}.\r\nThe log-likelihood results from Monte Carlo experiments can be seen in Table 1. As\r\nexpected, the best models in terms of predictive performance are the neural network-based\r\nchoice models. Among them, despite overfitting on the train set, our L-MNL gives the best\r\ngeneral representation of the data, achieving the best predictive performance in the test set\r\n(with a LL of -97 compared to -123 for the standard MNL model).\r\nIn order to evaluate the models in terms of accuracy in interpretable parameter estimation, we define the relative errors e\u03b2 and e\u03b2i/\u03b2j\r\nas\r\ne\u03b2 =\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\u03b2 \u2212 b\u03b2\r\n\u03b2\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n, (28)\r\ne\u03b2i/\u03b2j =\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\ne\u03b2i \u2212 e\u03b2j\r\n1 \u2212 e\u03b2j\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n. (29)\r\nThe relative error results6\r\nfrom Monte Carlo experiments can be seen in Table 2. As the beta\r\nparameters of the model have different values, we normalize the performance by reporting\r\nthe average relative error of the estimates for every model, as well as the standard deviation\r\nof all relative errors. We see that our L-MNL greatly outperforms every model in the ability\r\nto recover the true parameter values with a relative error smaller than 1% away from the\r\ntrue model. It is worth noting that for the ratios of parameters, the MNL models perform\r\nsecond best, although they have high relative errors at parameters level. This phenomenon\r\nhas been investigated in a few papers (e.g., Lee (1982), Cramer (2007)) where it has been\r\nshown that even when utility is misspecified, the MNL model is still able to retrieve good\r\nratios between estimates. The hybrid models with X = Q, on the other hand, generate high\r\nerrors in parameter estimates. It indicates that the NN component is also partially learning\r\nlinear dependencies of p or a, and prevents the linear function f, or the \u03b2 parameters, to\r\nreach a minimum as good as our L-MNL. This is a direct consequence from Equation (13) as\r\nthe impact of the variable is shared between both data-driven and knowledge-driven terms.\r\n6Note that the DNN(25, Q) benchmarking model does not appear in Table 2 since no variable enters the\r\nlinear utility specification.\r\n14\r\nTable 2: Monte Carlo relative errors for the different models in [%] with e the average relative error, s.d. its\r\nstandard deviation and \u03b2p, \u03b2a are from Equation (21).\r\nModel e\u03b2p\r\ns.d.(e\u03b2p\r\n) e\u03b2a\r\ns.d.(e\u03b2a\r\n) e\u03b2p/\u03b2a\r\ns.d.(e\u03b2p/\u03b2a\r\n)\r\n*Logit(Xtrue) 6.4 \u00b1 4.9 14.4 \u00b1 10.7 10.8 \u00b1 9.7\r\nLogit(X1) 26.7 \u00b1 6.2 26.7 \u00b1 14.7 15.5 \u00b1 12.4\r\nDNN L(25, X = Q) 60 \u00b1 32.4 74 \u00b1 54 460 \u00b1 166\r\nL-MNL(25, X , Q) 7.1 \u00b1 5.1 15.2 \u00b1 11.7 11.3 \u00b1 10.3\r\nTable 3: Monte Carlo hypothesis testing for the different models, for \u03b2p and \u03b2a taken separately and for\r\ntheir ratio. Parameters \u03b2p, \u03b2a are from Equation (21).\r\nModel % of experiments not rejecting H0\r\n\u03b2p and \u03b2a \u03b2p/\u03b2a\r\n*Logit(Xtrue) 97.5 94\r\nLogit(X1) 34 97\r\nDNN L(25, X = Q) 25.5 37\r\nL-MNL(25, X , Q) 95 94\r\nFinally, we conduct hypothesis testing to determine whether the estimated parameters\r\nare statistically different from the true ones. Formally, we consider the following null and\r\nalternative hypotheses: H0 :\r\nb\u03b2 = \u03b2, and H1 :\r\nb\u03b2 6= \u03b2, where the null hypothesis is rejected for\r\na t-test above 1.96, which corresponds to a 5% confidence interval. The standard deviation\r\nof the estimates in X for the t-tests are obtained via Hessian approximation7\r\n.\r\nThe results are shown in Table 3. We see that the coefficients alone are almost always\r\nstatistically different from the true ones for every model except L-MNL. The NN component\r\nof our model fixes the estimators by learning only on the data that are included in the linear\r\ncomponent. The ratio of parameters is well retrieved with both MNL and L-MNL models.\r\nConcerning the residual block model, we see that its NN component compromises the ratios\r\nbetween parameters. Indeed, as seen in Equation (14), part of the Neural Network has\r\noverlapped over the estimates of \u03b2p and \u03b2a, thus breaking both the estimates and their ratio.\r\nTo better illustrate the origin of ratio discrepancy of variables xin \u2208 X when \u2202rin\r\n\u2202xin\r\n6= 0 in\r\nhybrid models, we have shown in Section 5.2.4 that we retrieve the same biasing effect on\r\nour L-MNL when we make use of highly correlated data between both sets.\r\n5.2.3. Choice of neural network architecture\r\nChoosing the structure of an NN requires one to have enough capacity to capture the\r\nunderlying pattern of the data without overfitting. In the following, we gradually increase\r\nthe size of the NN component of our L-MNL, spanning from an underfit of the non-linear\r\ntruth of Equation (21) to an overfit of the generated data. To illustrate these effects, we\r\n7Note that profile likelihood approaches such as Hessian approximation create confidence intervals that\r\nfail to have the desired frequentist coverage properties. See Berger et al. (1999) for further discussion on\r\nthat topic.\r\n15\r\nstudy the values of \u03b2p and \u03b2a, as well as likelihoods of both train and test sets.\r\nThe results are depicted in Figure 3, where we take L-MNL(n, X , Q) with a single layer L=1,\r\nand scan from n = 0 neurons in the hidden layer to n = 5000. We see that from n = 0\r\nneurons (\u2261MNL) to about n \u223c 10, the NN has not yet captured all the non-linearities of the\r\noriginal utility specification. This can be seen by the higher values in likelihood and how the\r\nparameter estimates have not yet reached their true minimum. From n = 10 to n = 100, the\r\nmodel reaches stable values where the linear terms are equivalent to their ground truth. The\r\nmodel performs almost as well as the true model, depicted by the boundary lines, leading us\r\nto believe that the NN component has successfully learned the non-linearities of the data.\r\nWhen we continue to increase the number of neurons, we see the effects of overfitting, namely\r\na drop in train likelihood and an increase in test likelihood. We finally observe that the values\r\nof \u03b2p and \u03b2a also start to suffer from the overfitting, as the L-MNL is no longer a general\r\nrepresentation of the data, but has become specific to the training set. This experiment also\r\nshows that for equal optimal likelihood values, the change in architecture and non-convex\r\nproperty of the neural network does not significantly affect the \u03b2 estimates as it converges\r\nto the ground truth. We can see that the standard deviation spread of the estimates over\r\n100 experiments match with those of the true model also reported on the figure. Although,\r\nis has been shown in Draxler et al. (2018) and Garipov et al. (2018) that a neural network\r\nmay have many different parameter estimates for similar valued optimum, it does not seem\r\nto be the case for our convex part of the model.\r\n5.2.4. Impact of strongly correlated variables\r\nExperimental results from Section 5.2.2 suggest that having identical variables (or features) in both sets lead to poor parameter estimates because of the neural network\u2019s better\r\nability to discover correlation with the observations for all its variables. In this sense, Equation (13) shows how a variable\u2019s impact will be shared between both knowledge-driven and\r\ndata-driven terms if it belongs to both input sets.\r\nAlthough our L-MNL requests the variables in the two sets to be different, strong correlation between the variables can remain an issue. To investigate the impact of correlated\r\nvariables, we replace the original explanatory variable qin with a new explanatory variable\r\nq\r\n0\r\nin that is defined to be correlated to pin. Formally, we define q\r\n0\r\nin as\r\nq\r\n0\r\nin = s \u00b7 pin +\r\n\u221a\r\n1 \u2212 s\r\n2\r\n\u00b7 qin, (30)\r\nwhere s \u2208 [0, 1] is the correlation coefficient.\r\nOur new choice model is defined as\r\n\u2022 L-MNL(100, X , Q), with X = {p, a, b}, and Q = {q\r\n0\r\n, c},\r\nand is compared with the following two benchmarking models:\r\n\u2022 Logit(X1), with X1 = {p, a, b, q\r\n0\r\n, c},\r\n\u2022 Logit(X2), with X2 = {p, a, b}.\r\n16\r\nMNL UNDERFIT GOOD FIT\r\n\u201cBetas Fixed\u201d\r\nOVERFIT\r\nFigure 3: Likelihood and values of parameter estimates for an increasing number of neurons in the hidden\r\nlayer.Error bars show the standard deviation for 100 experiments. Red and Green lines show the standard\r\ndeviation spread of the true model\u2019s parameter estimation. Best results are obtained with n \u2208 [10, 100]\r\nThe Monte Carlo mean relative errors for different levels of correlation are shown in\r\nFigure 4. Overall we can see two effects: bias with high variance due to the correlated variables (Logit(X2), L-MNL), and bias due to misspecification (Logit(X1), Logit(X2)). For a\r\ncorrelation coefficient below s \u2264 0.8 we can see the L-MNL has much better estimates with\r\nrespect to the true model than the Logit models. For s \u2264 0.4 the L-MNL is practically as\r\nefficient as the true model. This is important as it suggests that in order to get insightful\r\nresults, the modeler has to carefully check that the variables that enter as input features in\r\nthe neural network are not the same or not too strongly correlated with the variables in the\r\nlinear utility specification.\r\nFinally, we also show in Appendix B.1, that our new choice model does not suffer from\r\ncomplexly correlated variables between both sets X and Q.\r\n17\r\nFigure 4: Impact of correlated variables on parameter estimates, where p \u2208 X is correlated to q\r\n0 \u2208 Q (see\r\nEq.30). We see in this case that the L-MNL correlation bias is smaller than the bias due to underfit for all\r\ns \u2264 0.8. For each coefficient, we performed 100 experiments.\r\n5.3. A real case study: the Swissmetro dataset\r\nFor the following experiments, we use the openly available dataset Swissmetro (Bierlaire et al. (2001)). The Swissmetro dataset consists of survey data collected in Switzerland\r\nduring March 1998. The respondents provided information to analyze the impact of a new\r\ninnovative transportation mode, represented by the Swissmetro, a revolutionary mag-lev\r\nunderground system. Each individual reported the chosen transportation mode choice for\r\nvarious trips, including the car, the train, or the Swissmetro. The original dataset contains\r\n10,728 observations. We removed observations for which the information regarding the chosen alternative was missing (9 observations). For convenience, we also decided to discard the\r\nobservations for which the three alternatives car, train, and Swissmetro were not all available\r\n(1,683 observations). Our dataset contains 9,036 observations. We split this initial dataset\r\ninto a training set of 7,234 observations and a test set of 1,802 observations.\r\n5.3.1. Models Comparison\r\nIn Table 7, we present a detailed comparison of all the benchmarking models with respect\r\nto log-likelihood in the training and testing sets as well as the parameters estimates. Since\r\nthe true values of parameter estimates are unknown, we compare the values obtained with\r\nour L-MNL to the ones obtained using the MNL model described in Bierlaire et al. (2001),\r\nwhose linear-in-parameter utility specification is described in Table 4 and variables described\r\nin Table 5. The results of parameter optimization for this MNL model are shown in Table\r\n18\r\nTable 4: Swissmetro benchmark utility function, from Bierlaire et al. (2001)\r\nAlternative\r\nVariables Car Train Swissmetro\r\nASC Constant Car-Const SM-Const\r\nTT Travel Time B-Time B-Time B-Time\r\nCost Travel Cost B-Cost B-Cost B-Cost\r\nFreq Frequency B-Freq B-Freq\r\nGA Annual Pass B-GA B-GA\r\nAge Age in classes B-Age\r\nLuggage Pieces of luggage B-Luggage\r\nSeats Airline seating B-Seats\r\nTable 5: Variables in the Swissmetro dataset used for modeled component of the utility specification\r\nVariable Description\r\nTT Door-to-door travel time in [minutes], scaled by 1/100.\r\nCost Travel cost in [CHF], scaled by 1/100.\r\nFreq Transportation headway in [minutes]\r\nGA Binary variable indicating annual pass holders (=1).\r\nAge IntegeEs variable scaled with the traveler\u2019s age.\r\nLuggage Integer variable scaled with amount of luggage during travel.\r\nSeats Binary variable for special seats configuration in Swissmetro (=1).\r\n7\r\n8\r\n.\r\nWe next estimate our new choice model L-MNL(100, X1, Q1), where X1 contains the same\r\nvariables as the ones included in the utility specification of Bierlaire et al. (2001), and Q1\r\ncontains all the unused variables in the dataset. These variables are described in Table 6. The\r\nresults of this model are depicted in Table 7. We see that adding the representation learning\r\ncomponent in the utility specification significantly increases the log-likelihood, suggesting\r\nthat these variables contain information that helps to explain travelers\u2019 choice. However,\r\nwe also observe that several estimates are not statistically different from zero (p-value >\r\n0.05). We have concluded in Section 5.2.2 and further developed in Section 5.2.4, that\r\nonly strong correlation between variables can lead to biased parameter estimates. However,\r\nthe correlation among the variables has been investigated and does not seem to be the\r\nissue here (see Table G.17 in Appendix for the correlation among variables). Moreover,\r\nthe DNN L(100,X1 = Q) model (see Table 7) has one-to-one correlation among variables in\r\nboth sets and does not loose significance in its parameters. One explanation could be that\r\n8\r\nIt is worth noting that the values of estimates that we obtain slightly differ from the ones depicted in\r\nthe original paper. This is normal since the original Swissmetro dataset is different from the training dataset\r\nthat we used.\r\n19\r\nTable 6: Unused variables in the Swissmetro dataset\r\nVariable Description\r\nPurpose: Integer variable indicating the trip purpose (business, leisure, etc. )\r\nFirst : Binary variable indicating if first class (=1) or not (=0)\r\nTicket: Integer variable indicating the ticket type (one-way, half-day, etc.)\r\nWho: Integer variable indicating who is paying the ticket (self, employer, etc.)\r\nMale: Binary variable indicating the traveler\u2019s gender (0 = female, 1 = male)\r\nIncome: Integer variable indicating the traveler\u2019s income per year.\r\nOrigin: Integer variable indicating the canton in which the travel begins.\r\nDest: Integer variable indicating the canton in which the travel ends.\r\nthe coefficients in L-MNL have lost their significance due to the neural network\u2019s ability to\r\nlearn better which variables and interactions are most correlated to the data. Therefore, the\r\nsignificance of the same parameters in the initial MNL model can originate from a bias due\r\nto the model\u2019s underfit. Also, with many explanatory variables being omitted in the initial\r\nMNL model, it is worth noting that the model is more likely to be subject to endogeneity,\r\ni.e., correlation among the dependent variables and the error term. Endogeneity is a wellknown cause of bias in parameter estimates and can cause bias in the parameter estimates\r\nof the initial MNL model.\r\nFinally, in Table 7, we estimate the new choice model L-MNL(100, X2, Q2), where X2\r\ncontains only the variables needed to compute the Value of Time (VOT) and Value of Frequency (VOF), i.e., the variables time, cost, and frequency. All other variables constitute\r\nthe set Q2 and are therefore given to the neural network. We observe that all parameter\r\nestimates are significant, while the log-likelihood has greatly increased compared to the standard MNL model. In other words, the representation term was able to get information from\r\nthe previously non-significant variables in the linear specification.\r\nFurthermore, as L-MNL has a bigger feature space than the aforementioned models, we\r\nhave designed a dummy coded Multinomial Logit as another benchmark. The parameter\r\nspace contains homogeneous \u03b2x for the variables in X2 and alternative specific \u03b2iq for all\r\nvariables contained in Q. The full feature space is written as Xdum = X2\u222aQ2 and the results\r\ncan be seen in Table 7. The big difference in final likelihoods with our L-MNL models further\r\ndemonstrate that this dataset indeed contains complex functions and interactions among its\r\nvariables which may be difficult to capture by the modeler.\r\nWe show a comparison of VOT and VOF for the different models in Table 8. We have also\r\nadded a Cross-Nested Logit (CNL), and a Triangular Panel Mixture (TPM) model to the\r\nresults for comparison purposes, whose detailed parameter values can be found in Appendix\r\nF and for which the implementation was taken from Biogeme examples9\r\n. The difference in\r\nlog-likelihood values between the L-MNL and the Logit(X1) suggests that the MNL model\r\nsuffers from underfitting that leads to significant ratio discrepancy. A similar conclusion\r\n9https://biogeme.epfl.ch/examples.html\r\n20\r\nMNL Stable Estimators OVERFIT\r\nFigure 5: Scan of Likelihood and Beta values over number of neurons n in the densely connected layer for\r\nL-MNL(100,X2,Q2). For n \u2208 [10, 200] we have VOT\u2248 1 and VOF\u2248 1.9. Over n = 200, we have signs of\r\noverfit.\r\nwas derived only under strong non-linearities in the dataset, suggested in Appendix C. The\r\nlinear specification of the MNL model does not allow to capture the Swissmetro\u2019s non-linear\r\ndependencies among variables. Moreover, we may observe for both added DCM methods,\r\nthat their ratio values move towards those of L-MNL as their likelihood increases. This trend\r\nimplies a better specification in their utility would allow them to reach even more similar\r\nvalues in ratios and likelihood with our new choice model.\r\nIn support of these conclusions, we incrementally increase the size of the NN component.\r\nWe observe in Figure 5 the parameter values with respect to the number of neurons in our\r\nneural network. As for the synthetic data experiments in Section 5.2.3, we see that stable\r\nratios for VOT and VOF are obtained for a neural network having from 10 to 200 neurons.\r\nWe see the MNL ratios for n=0 neuron highlights the underfit of the MNL model. An overfit\r\neffect is observed starting from 500 neurons, where the test likelihood no longer improves.\r\n5.3.2. Fixing artifact correlation between alternatives\r\nIn this experiment, we show results of a nested logit based on the baseline specification\r\nBierlaire et al. (2011) as well as the nested generalization of the L-MNL models seen in the\r\nprevious experiment. Moreover, we have added a model containing X1 and a smaller set\r\nQ0 = {Purpose, First, Male, Income} for illustrative purposes. We may observe in Table\r\n9 the Log-Likelihoods for the standard models and their nested variants with the following\r\n21\r\nnests: (1) known: {Car, Train}, and (2) new: {Swissmetro}.\r\nThe experiment shows that as we allow the representation term to find a data-driven\r\nresult over an increasing amount of input, i.e., from none to Q2, the nest value reduces itself\r\nto 1. If we relate this to the nested theory reminded in section 4.4, this means that the\r\ncorrelation between the choices in a nest \u2013car together with train\u2013 disappears as we better\r\nfit with a data-driven term. This suggests that the random terms were initially correlated\r\nbetween alternatives due to misspecification, but have become independent with the added\r\nrepresentation term. This interpretation is in line with the mathematical implications of\r\nEquation (15).\r\nIn further experiments, it would be possible to identify which variables are most active in\r\nthe correlation between alternatives, thanks to ablation and elasticity studies. An example\r\nof such experiments can be seen in Appendix D.1.\r\n22\r\nTable 7: Comparison of log-likelihood and parameters estimates for different models with utility specification\r\nof Bierlaire et al. (2001). Number of observations = 7234.\r\nModel Parameters Estimates Std errors t-stat p-value\r\nMNL\r\n\u03c1\r\n2\r\ntest = 0.28\r\nL(\u03b2\u02c6) = \u22125764\r\nLtest(\u03b2\u02c6) = \u22121433\r\nASCCar 1.08 0.162 6.67 0.00\r\nASCSM 1.05 0.153 6.84 0.00\r\n\u03b2age 0.146 0.436 3.35 0.00\r\n\u03b2cost -0.695 0.0423 -16.42 0.00\r\n\u03b2freq -0.733 0.1132 -6.47 0.00\r\n\u03b2GA 1.54 0.167 9.24 0.00\r\n\u03b2luggage -0.114 0.0488 -2.338 0.02\r\n\u03b2seats 0.432 0.115 3.76 0.00\r\n\u03b2time -1.34 0.051 -26.18 0.00\r\nL-MNL(100, X1, Q1)\r\n\u03c1\r\n2\r\ntest = 0.41\r\nL(\u03b2\u02c6) = \u22124511\r\nLtest(\u03b2\u02c6) = \u22121181\r\nASCCar 0.106 0.174 0.61 0.54\r\nASCSM 0.454 0.163 2.80 0.01\r\n\u03b2age 0.390 0.045 8.63 0.00\r\n\u03b2cost -1.378 0.048 -28.45 0.00\r\n\u03b2freq -0.860 0.127 -6.77 0.00\r\n\u03b2GA 0.214 0.194 1.10 0.27\r\n\u03b2luggage 0.116 0.0529 2.19 0.03\r\n\u03b2seats 0.104 0.109 0.95 0.34\r\n\u03b2time -1.563 0.056 -27.97 0.00\r\nDNN L(100,X1 = Q)\r\n\u03c1\r\n2\r\ntest = 0.37\r\nL(\u03b2\u02c6) = \u22124964\r\nLtest(\u03b2\u02c6) = \u22121257\r\nASCCar 0.365 0.165 3.61 0.00\r\nASCSM 0.549 0.162 2.22 0.03\r\n\u03b2age 0.087 0.0423 2.07 0.04\r\n\u03b2cost -0.897 0.046 -19.46 0.00\r\n\u03b2freq -0.639 0.123 -5.20 0.00\r\n\u03b2GA 1.40 0.172 8.15 0.10\r\n\u03b2luggage 0.186 0.0523 3.52 0.00\r\n\u03b2seats 0.233 0.102 2.29 0.02\r\n\u03b2time -1.146 0.049 -23.32 0.00\r\nLogit(Xdum) (all 41 inputs)\r\n\u03c1\r\n2\r\ntest = 0.33\r\nL(\u03b2\u02c6) = \u22125451\r\nLtest(\u03b2\u02c6) = \u22121322\r\n\u03b2cost -1.062 0.059 18 0.00\r\n\u03b2freq -0.79 0.118 6.69 0.00\r\n\u03b2time -1.326 0.053 25.02 0.00\r\n... ... ... ... ...\r\nL-MNL(100, X2, Q2)\r\n\u03c1\r\n2\r\ntest = 0.44\r\nL(\u03b2\u02c6) = \u22123895\r\nLtest(\u03b2\u02c6) = \u22121108\r\n\u03b2cost -1.671 0.0523 -31.94 0.00\r\n\u03b2freq -0.865 0.0765 -11.30 0.00\r\n\u03b2time -1.769 0.0389 -45.4 0.00\r\n23\r\nTable 8: Parameter ratio comparison\r\nModel Value of Time Value of Frequency Train Log-Likelihood Test Log-Likelihood\r\nLogit(X1) 0.52 0.95 -5764 -1433\r\nLogit(Xdum) 0.80 1.34 -5451 -1322\r\nDNN L(X1) 0.78 1.40 -4964 -1257\r\nL-MNL(X1) 0.88 1.60 -4511 -1181\r\nL-MNL(X2) 0.94 1.93 \u22123895 \u22121108\r\nCNL(X1) 0.59 1.52 -5711 -1415\r\nTPM(X1) 0.72 1.59 -4752 -1350\r\nTable 9: Performance of Swissmetro models with added nest structure. Increase in fit lowers\r\nthe need of nests.\r\nStandard Models\r\nNest Factor\r\nNested Models\r\nTrain LL Test LL Train LL Test LL\r\nLogit(X1) -5764 -1433 1.43 -5734 -1413\r\nL-MNL(X1, Q0) -5324 -1348 1.37 -5305 -1340\r\nL-MNL(X1, Q1) -4268 -1173 1.01 -4260 -1173\r\nL-MNL(X2, Q2) -3895 \u22121108 1 -3894 \u22121108\r\n24\r\n5.4. Revealed Preference study: Optima dataset\r\nIn contrast with the previous Swissmetro study, we investigate in this section a case\r\nmuch more difficult for representation learning due to the size limitation of gathered data.\r\nThe project named Optima is a small survey collected in Switzerland between 2009 and 2010\r\nwhere respondents filled up extensive information in the topic of mode choice, including time\r\nand cost of performed trips, socio-economic characteristics as well as opinions on statements\r\nand answers to \u201dsemi-open\u201d questions as defined in Glerum et al. (2014). More details on\r\nthis particular dataset can be read in Bierlaire et al. (2011) and Glerum et al. (2014). To\r\nbenchmark our results, we take the latest results discovered on this dataset, notably a wellspecified base multinomial logit and one of its suggested extension, the Integrated Choice\r\nand Latent Variable (ICLV) model, from Fern\u00b4andez-Antol\u00b4\u0131n et al. (2016).\r\n5.4.1. Models description\r\nThe multinomial logit specification from Fern\u00b4andez-Antol\u00b4\u0131n et al. (2016) can be seen\r\nin Table 10. We then compare two L-MNL models and two pure Neural Networks. The\r\ninput sets for our first model is defined such that X1 contains the variables from the baseline\r\nspecification, while Q1 are those shown in Table 11. In this case it is interesting to note,\r\nthat the straightforward interpretability condition for cost in Section 4.2 is still satisfied,\r\neven though it interacts with income\u2208 Q. Correlation between inputs of both sets can be\r\nfound in Appendix H. Based on experiments seen in Section 5.2.4 and Appendix B.1, this\r\nspecification will not suffer bias from correlated variables. The second L-MNL model has\r\nan input set X2, which contains only time, cost, and distance attributes, while the removed\r\nfeatures were added into the second set to create Q2. The Neural Networks have all features\r\nfrom both tables as inputs, but differ in size, the first having a single layer of 100 neurons,\r\njust like the L-MNL models, and the second has a single layer of 30 neurons.\r\nWe preprocess the data by removing any entry which does not have all choices available\r\nor unanswered variables we make use of. This gives us a total of 1376 answers, where 1089\r\nare kept for the training set and 287 for the testing set. This is considered a tiny dataset\r\nfor neural networks, which makes it a difficult task to avoid overfitting. Therefore, we have\r\nadded a 30% dropout layer and an l2 regularizer of weight \u03bb = 0.5 (Krogh and Hertz (1992)).\r\n5.4.2. Expert modeling as a regularizer\r\nThe performances of the five models described above are shown in Figure 6, where the\r\nsame minimization code was run 100 times for every model. The observed variation comes\r\nfrom the change in starting values, which ultimately brings to a slightly different optimum\r\ngiven the strong regularizers and fixed number of training cycles, i.e., 80 epochs.\r\nWe may observe that both MNL(X1) and L-MNL(X1, Q1) generalize well with an increase\r\nin performance for the second model. As opposed to the case seen in Swissmetro section 5.3.1,\r\nsimplifying the X1 to the minimum of interest X2 allows the model to overfit and perform\r\npoorly. The same can be seen for a full neural network of 100 neurons. If we try to reach\r\nbetter performances with a smaller neural network, including the high regularizers and low\r\ntraining cycles, we may observe the result of the last model, containing 40 neurons. Although\r\n10More details at https://biogeme.epfl.ch/data.html\r\n25\r\nTable 10: Base model specification from Fern\u00b4andez-Antol\u00b4\u0131n et al. (2016)\r\nAlternative\r\nVariables Public Transportation Car Slow modes\r\nASC Constant PT-Const CAR-Const\r\nTT Travel Time [min] B-Time-PT B-Time-CAR\r\nMCost Marginal Cost\r\nIncome B-MCost-PT B-MCost-CAR\r\nDistance Trip distance [km] B-Dist\r\nWork Work related Trip B-Work\r\nFrench French Speaking area B-French\r\nStudent Occupation is student B-Student\r\nUrban Urban area B-Urban\r\nNbChild Number of Children B-NbChild\r\nNbCar Number of Cars B-NbCar\r\nNbBicy Number of Bicycles B-NbBicy\r\nTable 11: Added variables for representation learning term in Optima dataset. All variables gave -1 for\r\nmissing values.\r\nVariable Description\r\nAge: Age of the respondent (in years)\r\nHouseType : 1 is individual house (or terraced house), 2 is apartment, 3 is independent room\r\nGender: 1 is man, 2 is woman. -1 for missing value.\r\nEducation: Highest education achieved10. Categories from 1 to 8.\r\nFamilSitu: Family situation10. Categories from 1 to 7.\r\nScaledIncome: Integer variable indicating the traveler\u2019s income per year.\r\nOwnHouse: Do you own the place where you are living? 1 is yes, 2 is no\r\nMotherTongue: 1 for german or swiss german, 2 for french, 3 for other,\r\nSocioProfCat: Socio-professional10 categories from 1 to 8.\r\nwe still overfit on the training set, we have better generalization to the test set. However, the\r\noptimum has high variance, and parameter interpretation may only be recovered through\r\nelasticity plot studies, missing the straightforward interpretation of discrete choice model\r\nparameters.\r\n5.4.3. Benchmarking with ICLV\r\nIn Fern\u00b4andez-Antol\u00b4\u0131n et al. (2016), the base logit is also compared to an ICLV model,\r\nwhere the latent variable CarLoving is added to the model. This latent variable is estimated\r\nwith two Likert indicators (Likert (1932)):\r\n\u2022 I1: It is difficult to take the public transport when I travel with my children.\r\n\u2022 I2: With my car I can go wherever and whenever.\r\n26\r\n\r\n\u000b\t\r\n\t\u0001\r\n\u000b\t\r\n\t\u0001\r\n\u000b\t\u0004\r\n\u000b\u000b\r\n\u000b\u000b\u0006\r\n\u0005\u0007\r\n\u0006\r\n\u0006\u0007\r\n\u0007\r\n\u0007\u0007\r\n\b\u001a\u000f\u0017\r\n\u0010\u000f\u000b\u000f\u0010\r\n\u0019\u0012\u001a\u000f\t\u0016\u0010\u0001\t\u0012\u0013\u000f\u0014\u0012\u0011\u0016\u0016\u000e\r\n\f\u0017\r\n\u0012\u0015\u000b\t\t\r\n\f\u000f\u0018\u0019\u000b\t\t\r\nFigure 6: Performance of multiple models on a small revealed preference dataset (Optima) for 100 minimization iterations. The first two models have expert modeling of the utility specification and generalize\r\nwell. The last three have small or no modeling of the utility and show clear signs of overfitting as well as\r\nhigh variance in performance results.\r\nand the following specifications:\r\ntcar \u00b7 Ii = \u03b1i + \u03bbi\r\n\u00b7 CarLoving \u00b7 tcar + \u03c9i (31)\r\nwhere \u03c9i \u223c N (0, \u03c3i) is the random term, \u03b1i\r\n, \u03bbi\r\n, \u03c3i are parameters to be estimated and:\r\nCarLoving = \u03b7car + \u03c9 (32)\r\nwith \u03b7car and \u03c3 being the estimated parameters and \u03c9 \u223c N (0, \u03c3) the random term.\r\nFurther details are described in Fern\u00b4andez-Antol\u00b4\u0131n et al. (2016). Given that the full\r\nloss function from an ICLV model differs from that of the base logit or MNL, we may only\r\ncompare models by observing their accuracy on a test set. The implementation of the new\r\nmodel was done in biogeme (Bierlaire (2003)) and the results can be seen in Table 12. We\r\nmay observe that the ICLV model performs better than MNL while having the same initial\r\nfeature space. A stronger increase in accuracy could be expected with added latent variables\r\nor more complex structural equations. For the models with a neural network, we have\r\nreported the average accuracy of all iterations. As seen from the loss in Figure 6, the Neural\r\nNetwork (n = 40) performs better than the L-MNL on average while slightly overfitting the\r\ntraining set. However, the obtained model does not contain straightforward interpretable\r\nparameters, has higher variance in performance, and required more efforts in fine-tuning for\r\noptimal performance. In the DCM sense, the most useful models would be L-MNL with the\r\nfull expert specification and the ICLV model. Adding a representation term does not limit\r\nitself to MNL or its nested generalizations. We believe ICLV architectures may also benefit\r\nfrom a data-driven counterpart and we consider this for future research.\r\nIn conclusion, when working with small datasets, Discrete Choice Modeling may effectively generalize very well when a good specification is found. The effect of the learning term\r\nmay show how well our model is specified, i.e., a low increase in performance would signify\r\nTable 12: Models accuracy on training and testing sets of Optima\r\nModel Logit(X1) L-MNL(X1 ,Q1) NN40(X \u222a Q) ICLV(X1)\r\nAccuracy Train [%] 76.8 80.4 86.1 80.0\r\nAccuracy Test [%] 76.7 79.2 81.3 77.7\r\na well-specified utility. Lastly, for the machine learning community, this may also encourage\r\nto use expert modeling of a utility specification as an efficient regularizer for neural networks\r\nwhen working with small datasets.\r\n6. Future Directions\r\nOur proposed framework paves the way to several avenues of research. One main direction\r\nis the new possibility to investigate many more types of datasets for discrete choice modeling.\r\nIndeed, representation learning methods exist for all types of inputs, including continuous\r\nsignals, images, time series, and more. Consequently, we can explore new neural network\r\narchitectures (e.g., other variants of the residual blocks). Although research has already\r\nbeen done in this direction, e.g., Otsuka and Osogami (2016) who used images for choice\r\nclassification, our model is the first to propose the coexistence of these new inputs with\r\nstandard discrete choice modeling variables. Furthermore, we suggest that the representation\r\nlearning architecture, together with the modeled specification architecture, complement each\r\nother to reach both a better optimum when they are jointly optimized.\r\nSecondly, we believe that other discrete choice models such as more advanced GEV\r\nmodels, Mixed Logit or Latent Class models can also benefit from an added data-driven term\r\nwhile keeping high degree of interpretability. Integrating them in a unified framework/library\r\nis a big direction for future research. Indeed, as of now, by making use of the NN structure\r\nwe have been able to implement a new learnable term to the Multinomial logit, and thanks\r\nto the implementation of multiple custom loss layers in a deep learning library, we have\r\nimplemented its first nested generalization, the Nested logit. Both models have benefited\r\nfrom a data-driven term, and we hope to bring them to more advanced models in the near\r\nfuture.\r\nMoreover, the proposed architecture of our model may also help in the task of modeling\r\nthe knowledge-driven term of the utility specification via feature selection. Indeed, understanding what a data-driven method has learned is an active field of research and may overall\r\ngreatly benefit the field of discrete choice modeling. In our specific case, by capturing insights about the representation term, we would be able to reduce the number of parameters\r\nin Q incrementally, and add their interactions or non-linear functions in X when discovered,\r\ntaking advantage of the joint optimization benefits of our model. To this end, we may also\r\ninterest ourselves to recent methods which move towards explainable A.I., notably those on\r\nnetwork visualization and interpretation (Ribeiro et al. (2016), Samek et al. (2016), Shrikumar et al. (2017), Murdoch et al. (2019)) and compare with recent works on interpretable\r\nor knowledge-injected machine learning (Borghesi et al. (2020), von Rueden et al. (2020),\r\nMurdoch et al. (2019)).\r\n28\r\nYet another avenue of research concerns the structure and role of the representation term\r\nin the utility function. Indeed, for example, one could have multiple terms per utility, such\r\nthat each chosen input set Qi\r\n, belonging to their own respective network, would create a\r\nmeaningful embedding in the utility. In the same spirit, one could inspire ourselves of Wang\r\net al. (2020) to have an added utility specific network per alternative. On another hand,\r\nthe data-driven term may simply be used to discover usually complex specifications in the\r\nutility. This has been done in Han et al. (2020), a direct extension of our work, which make\r\nuse of neural networks to automatically discover the true taste heterogeneity function of a\r\nparameter.\r\nFinally, while we have shown the benefits of our data-driven term in the DCM field, the\r\nmachine learning community can also benefit from our formulation to tackle small datasets.\r\nWe have shown that our architecture may perform as a regularization tool for common deep\r\nlearning methods when applied to small datasets. This may encourage the machine learning\r\ncommunity to reach out for DCM practices and make use of a priori specification to help\r\nthe performance of their models.\r\n7. Conclusions\r\nIn this paper, we introduced a novel general and flexible theoretical framework that\r\nintegrates a representation learning technique into the utility specification of a discrete choice\r\nmodel to automatically discover good utility specification from available data. This datadriven term may account for many forms of misspecifications and greatly improves the overall\r\npredictability of the model. Also, unlike the existing hybrid models in the literature, our\r\nframework is carefully designed to keep the interpretability of key parameters, which is\r\ncritical to allow researchers and practitioners to get insights into the complex human decisionmaking process.\r\nUsing synthetic and real world data, we demonstrated the effectiveness of our framework\r\nby augmenting the utility specification of the Multinomial Logit, as well as the Nested Logit,\r\nwith a new non-linear representation arising from a neural network, leading to new choice\r\nmodels referred to as the Learning Multinomial Logit (L-MNL) and Learning Multinomial\r\nNested Logit (L-NL) models. Our experiments showed that our models outperformed the\r\ntraditional choice models and existing hybrid models, both in terms of predictive performance\r\nand accuracy in parameter estimation.\r\nThere is a growing interest within the transportation community to exploit multidisciplinary methods to solve the ever more challenging problems its members face. By making\r\nour code openly available, we hope that we successfully contributed to bridge the gap between\r\ntheory-driven and data-driven methods and that it will encourage researchers to combine\r\nthe strengths of choice modeling and machine learning methods.\r\n29\r\nAppendix A. Notation\r\nIn this section, we give a very short explanation on the main notation rule used. Variables\r\nwritten as plain text with or without subscripts are single elements, while their vectorized\r\nform will be in bold and have the relevant subscript disappear. Here are two examples:\r\nuin \u223c (1 \u00d7 1) (A.1)\r\nun \u223c (I \u00d7 1) or Vi \u223c (1 \u00d7 1) (A.2)\r\nui \u223c (1 \u00d7 N) V \u223c (I \u00d7 1) (A.3)\r\nu \u223c (I \u00d7 N) (A.4)\r\nAppendix B. Synthetic Annex Experiments\r\nAppendix B.1. Complex Correlation Patterns\r\nIn this section, we make use of the same data generation process defined in Guevara\r\n(2015). This allows us to take away one main observation: we show that a complexly correlated variable in Q with a variable in X does not negatively affect the estimated parameters.\r\nIn other words, the endogeneity created by the omitted variable can be fixed when including it in the representation term without creating biasing effects. Therefore, together with\r\nother experiments of Section 5.2, we show our model fixes for omitted variable and function\r\nmisspecification bias.\r\nThe data generation process defines two utility functions where:\r\nUin = \u22122pin + ain + bin + qin + \u03b5in for i = 1, 2 (B.1)\r\nwith pin, ain, bin, and qin being the generated variables and \u03b5in is the random term. The\r\ncomplex correlation is between qin and pin such that:\r\npin = 5 + qin + zin + 0.03wzin + \u03b5pin for i = 1, 2 (B.2)\r\nwhere all generated variables are sampled in a uniform distribution U([\u22122, 2]) such that we\r\nhave exactly the same data generation from the original paper.\r\nAs done in Guevara (2015), we perform a Monte Carlo simulation by generating 100 repetitions of 1000 observations and by estimating the following models:\r\n\u2022 MNLT rue: The true logit model where X = {a, b, q}\r\n\u2022 L-MNLT rue: Complex correlated model between both sets where X = {a, b} and\r\nQ = {q}. The representation term arises from a single DNN layer (L = 1)\r\nof 100 neurons (H = 100).\r\n\u2022 MNLendo: The endogenous logit model where X = {a, b}\r\n30\r\nAll models have reach a stable optimum after 50 epochs, with standard Adam optimizer\r\nand 20% dropout regularizer. Running time in this framework have no noticeable difference\r\nbetween models.\r\nThe results of the experiment can be seen on Figure (B.7), where we have Box-Plots\r\nfor the ratios of the estimators \u03b2\u02c6\r\np/\u03b2\u02c6\r\na. This value is studied since correction of endogeneity\r\nmay produce a change in scale of the estimators, and we wish to remain comparable to the\r\noriginal paper and their multiple models. Indeed, we obtain the same distribution as in\r\nGuevara (2015) for the true model as well as the endogenous one. For the L-MNL models,\r\nwe may observe that a complex correlated term in the neural network for L-MNLtrue does\r\nnot affect the parameter estimates, and the overall architecture is able to estimate the true\r\nfunction.\r\nM\r\n\t\u000b\u0005 -M\r\n\t\u000b\u0005,  M\u0005\u0006\u0004\u0007\r\n\u0007\u0005\u0001\r\n\u0007\u0004\u0001\u0006\r\n\u0007\u0004\u0001\r\n\u0007\u0001\u0006\r\n\u0007\u0001\r\n\u0007\u0001\u0006\r\n\u0004 \f\b\u0302\r\n\u0004 \f\r\nFigure B.7: Monte Carlo experiment for complex correlation analysis, Box-Plots for 100 parameter estimates\r\nrepetitions of 1000 observations each. True parameter ratio is \u03b2p/\u03b2a = \u22122.\r\nAppendix B.2. Impact of unobserved variables\r\nWe take advantage of synthetic data to better analyze the impact of unobserved variables on parameter estimates. To do so, we assume that we know the following true utility\r\nspecification for i = 1, 2:\r\nV\r\n0\r\nin = Vin + \u03b2u \u00b7 uin (B.3)\r\nwhere Vin is given by Equation (21).\r\nWe assume that variables u1n, u2n \u223c U([\u22121, 1]) are unobserved by the modeler, i.e., that\r\nu \u2208/ (X \u222a Q). We also choose \u03b2u = 1 and perform 100 experiments with this new utility\r\nTable B.13: Impact of unseen variables on parameter estimates and log-likelihood. e is the average relative\r\nerror, s.d. its standard deviation and \u03b2p, \u03b2a are from Equation (21).\r\nModel e\u03b2p\r\ns.d.(e\u03b2p\r\n) e\u03b2a\r\ns.d.(e\u03b2a\r\n) e\u03b2p/\u03b2a\r\ns.d.(e\u03b2p/\u03b2a\r\n)\r\nL-MNL(25, X , Q) 11.2 \u00b1 7.4 21.1 \u00b1 13.6 16.2 \u00b1 12.5\r\nLogit(X1) 34.2 \u00b1 6.3 38.7 \u00b1 16.2 20.9 \u00b1 15.5\r\nLLtrain s.d.(LL) LLtest s.d.(LL)\r\nL-MNL(25, X , Q) \u2212460 \u00b1 16 \u2212101 \u00b1 7\r\nLogit(X1) -620 \u00b1 11 -124 \u00b1 5\r\nspecification. For the sampling scheme, we generate 1, 000 synthetic observations for the\r\ntraining set and 200 more for the testing set.\r\nOur new choice model is defined as\r\n\u2022 L-MNL(100, X , Q), with X = {p, a, b}, and Q = {q, c},\r\nand is compared with the following benchmarking model:\r\n\u2022 Logit(X1), with X1 = {p, a, b, q, c}.\r\nThe Monte Carlo mean relative errors for the two models are shown in Table B.13.\r\nWe see that, compared to the results depicted in Table 2 where all variables in the true\r\nutility specification had been included in the models, the unseen variables lead, for both\r\nmodels, to a decrease in the fit and an increase in relative errors in parameter estimates.\r\nNevertheless, one important observation is that our L-MNL retrieves consistent ratio estimates, similarly to the MNL model, while having better global performances. The unseen\r\nvariables do not provoke unexpected behavior to our new model.\r\nAppendix B.3. Optimization Strategy\r\nAs depicted in Figure 2, our L-MNL architecture contains two parts: the top of the\r\nfigure represents the a priori defined component in the utility specification, while the bottom\r\ndepicts the learning component. The number of parameters to be estimated in each part\r\ncan vary significantly. While a limited number of parameters are generally included in\r\nlinear specification, a neural network can easily contain thousands of parameters. Given\r\nthis architecture, one could be tempted to estimate the model sequentially. To show the\r\nimportance of jointly estimating the parameters, we consider the three following strategies\r\nfor estimating our L-MNL(100, X , Q):\r\n1) First optimizing the standard discrete choice component of the specification (i.e., the\r\n\u03b2 parameters) and then learning the representation term after fixing the previously\r\nfound linear-in-parameter estimates.\r\n2) First optimizing the representation term (i.e., the w weights) and then learning the\r\nmodeled specification after fixing the previously estimated weights.\r\n3) Optimizing jointly both components.\r\n32\r\nResults are shown in Table B.14 for \u03b2p = \u22122, \u03b2a = 1,\u03b2b = 0.5 and \u03b2qc = 1. We see that the\r\njoint optimization allows for the best minima in both likelihood and parameter estimation.\r\nStarting with the modeled specification gives the same parameters as if it were an MNL, but\r\nends up with a better likelihood as the NN component then only increases predictability.\r\nThe second strategy, on the other hand, reaches a sub-optimal minima when learning the\r\nrepresentation. It is only with joint optimization that all important explanatory variables\r\nare expressed. The two components complete each other to achieve the best prediction\r\nperformance with the correct parameter values.\r\nTable B.14: Values of parameter estimates and likelihoods based on optimization strategy for \u03b2p = \u22122 and\r\n\u03b2a = 1. Best results are obtained with joint optimization.\r\nStrategy \u03b2\u02c6\r\np \u03b2\u02c6\r\na L(\u03b2\u02c6) Ltest(\u03b2\u02c6)\r\n(1) \u03b2\u02c6 then w -1.41 0.75 -4927 -942\r\n(2) w then \u03b2\u02c6 -1.59 0.82 -3924 -758\r\n(3) \u03b2\u02c6 and w -1.95 1.0 -3678 -721\r\nAppendix C. Semi-Synthetic Data\r\nIn this section, we challenge our L-MNL model by analyzing its effectiveness outside\r\nof fully synthetic data and in the presence of strong non-linearities and noisy data. To\r\ngenerate our semi-synthetic data, we follow the same procedure as in section 5.2, but instead\r\nof using normally distributed explanatory variables, we randomly select them from a real\r\nworld dataset, the Swissmetro dataset (Bierlaire et al. (2001)).\r\nWe define the following utility specifications:\r\nVT rain= lT rain + 1 \u00b7 DEST3\r\n\u00b7 AGE \u2212 1 \u00b7 AGE0.5\r\n\u00b7 ORIGIN\r\nVSM = lSM + 1 \u00b7 DEST \u00b7 AGE + 3 \u00b7 INCOME5\r\n\u00b7 P URP OSE2\r\nVCar = lCar + 5 \u00b7 AGE \u00b7 INCOME5 + 2 \u00b7 ORIGIN2\r\n\u00b7 INCOME5\r\n(C.1)\r\nwith\r\nli = \u22121 \u00b7 T Ti \u2212 2 \u00b7 T Ci \u2200i \u2208 C, (C.2)\r\nThe coefficients are chosen in order to have a balanced dataset, and the interacting variables\r\nare the categorical features of Swissmetro data (Bierlaire et al. (2001)) and are described in\r\nTable 5. Note that we have chosen to use power series to complexify the non-linear terms.\r\nThe models under study are:\r\n\u2022 Logit(Xa) with linear-in parameters utility specification based on the following features:\r\n\u2013 XT rain,a = {1, T TT rain, T CT rain, AGE, DEST, ORIGIN}\r\n\u2013 XSM,a = {1, T TSM, T CSM, AGE, DEST, INCOME, P URP OSE}\r\n\u2013 XCar,a = {1, T TCar, T CCar, AGE, ORIGIN, INCOME}\r\n33\r\nTable C.15: Values of parameter estimates and likelihoods for different models based on Equation (C.1).\r\nGround truth is \u03b2T T = \u22121 and \u03b2T C = \u22122. Only L-MNL is able to estimate correctly the parameters.\r\nModels \u03b2\u02c6\r\nT T \u03b2\u02c6\r\nT C \u03b2\u02c6\r\nT C/\u03b2\u02c6\r\nT T LLtrain LLtest\r\nLogit(Xa) -0.65 -1.63 2.50 -5412 -1354\r\nLogit(Xb) -0.25 -0.70 2.81 -7722 -1925\r\nL-MNL(X , Q) \u22121.01 \u22121.99 1.96 \u22122516 \u2212809\r\n\u2022 Logit(Xb) with only travel time and cost for each utility, i.e.:\r\n\u2013 Xib = {1, T Ti\r\n, T Ci} for all i \u2208 C.\r\n\u2022 L-MNL(X , Q) with\r\n\u2013 Xi = {T Ti\r\n, T Ci},\r\n\u2013 Q = {AGE, DEST, ORIGIN, INCOME, P URP OSE}\r\nThe results can be seen in Table C.15. We see that standard MNL models are unable\r\nto retrieve the correct parameter estimates for utility specifications with important nonlinearities. Both MNL models exhibit large relative errors (about 40% for the Logit(Xb) and\r\nat least 25% for the Logit(Xa)). The relative errors are also large for the ratio of parameters,\r\nwhich would lead to wrong postestimation indicator, the VOT in this case. Unlike the Logit\r\nmodels, our L-MNL model recovers the true estimates in both parameters and ratio, while\r\nachieving a much better fit. We therefore conclude that the representation term was able to\r\nlearn the complex non-linearities and that ignoring these non-linearities lead to models that\r\ngreatly suffer from underfit.\r\nAppendix D. Swissmetro supplementary experiment\r\nAppendix D.1. Feature impact and sensitivity analysis\r\nAs previously done by Bentz and Merunka (2000), we finish our experiments by investigating what the neural network component has learned through the study of a sensitivity\r\nanalysis. To do so, we vary the value of a feature in Q while keeping the others variables\r\nconstant, and we analyze its impact on the utilities and market shares of the alternatives.\r\nWe do the analysis on the L-MNL(100, X2, Q2) for which only cost, time, and frequency\r\nare included in the linear specification, while the other 14 variables are given to the neural network. Figure D.8 presents a sensitivity analysis for two variables in Q2: AGE and\r\nINCOME11. We observe that the AGE variable has almost linear relations to the utilities,\r\nwhich has also been seen in Bierlaire et al. (2001)\u2019s benchmark. Changing INCOME however, seems to present non-linearities and an overall weaker impact on the change in mode\r\nshare. We recognize that this is only the average behavior of the feature in the population\r\n11These two variables were chosen for elasticity study as they are the only integer variables which represent\r\na discrete scale of intensity.\r\n34\r\n(a) (b)\r\nFigure D.8: Percentage change in total mode shares against percentage change in L-MNL features belonging\r\nto Q2. These general trends appear by fixing all other values as constant.\r\nwhen keeping all other variables constant. Further investigation of non-linear interactions\r\ncan be done by separating our sensitivity analysis based on the values of the other variables\r\nas seen in Bentz and Merunka (2000).\r\nTo have some insights on the impact of each feature in Q on the utility function, we get\r\ninspiration from saliency maps Simonyan et al. (2013). A saliency map is obtained through\r\nback-propagation of an observation\u2019s prediction score, as opposed to its output loss which\r\nis performed during training. We then read the results at the nodes of the input layer. The\r\nretrieved values are considered to be the gradient estimation of a prediction with respect\r\nto a given input. For our case, we do this by changing the loss function of our pre-trained\r\nmodel:\r\nloss(Vn, yn) = X\r\ni\u2208Cn\r\nyin \u00b7 Vin, (D.1)\r\nwhere yin is 1 when individual n is predicted to choose alternative i and Vin is the output\r\nfor utility i as seen in Equation (11).\r\nAs opposed to an image, which is a 2D set of pixels, the position of our input has always\r\nthe same meaning for each individual. In other words, the gradient read on the first position\r\nwill always be P URP OSE and the last always SM SEAT S. This allows us to measure the\r\naverage impact of a feature on each class. Indeed, when summing over all individuals the\r\nabsolute gradients at the input layer for each feature, we get the Figure D.9. The sum has\r\nbeen separated for each utility and normalized by the count of the chosen alternative of all\r\nindividuals.\r\nAs we can see, all variables are being used by the NN. Some, such as GA, AGE or\r\nLUGGAGE seem to have an overall bigger impact than SM SEAT S. This supports the\r\nconclusion that the MNL benchmark of Bierlaire et al. (2001) misses potential useful information by ignoring many variables.\r\n35\r\nFigure D.9: Mean feature contributions to each utility obtained with loss of Equation (D.1) and gradient\r\nevaluation on the input layer.\r\n36\r\nAppendix E. Learning Nested-Logit: network-loss architecture\r\nr(Q)\r\nf(X)\r\n- Separate into nests\r\n- Multiply by factors\r\n- Softmax intra-nests:\r\nP(i|m)\r\n- Softmax inter-nests:\r\nP(m|C)\r\n- P(i|m)P(m|C)\r\n- Sort outputs\r\nPrepare nests:\r\n- exp(Ui\r\n)\r\n- sum all\r\n- log of sum\r\n- 1/factor\r\nFigure E.10: For 2 nests. As the nested loss needs trainable weights, this was solved using custom layers.\r\n\u201dnestMultiply\u201d and \u201dnestDivide\u201d share the same trainable parameter \u00b5 and has constraint \u00b5 \u2265 1\r\n37\r\nAppendix F. Extra Models details for Swissmetro\r\nTable F.16: Comparison of parameters estimates for different models with utility specification of Bierlaire\r\net al. (2001)\r\n(a) Parameter estimates from CNL(X1)\r\nmodel\r\nParameters Value Std errors t-test p-value\r\n\u03b1existing 0.371 0.0229 16.2 0.0\r\nASCCar -0.015 0.045 -0.333 0.739\r\nASCT rain 0.0245 0.0793 0.309 0.757\r\n\u03b2age 0.108 0.0197 5.48 4.18e-08\r\n\u03b2cost -0.58 0.037 -15.7 0.0\r\n\u03b2freq -0.38 0.0591 -6.43 1.27e-10\r\n\u03b2GA 1.28 0.123 10.4 0.0\r\n\u03b2luggage -0.136 0.0395 -3.44 0.000591\r\n\u03b2seats 0.0977 0.0738 1.32 0.185\r\n\u03b2time -0.982 0.0535 -18.3 0.0\r\n\u00b5existing 1.8 0.124 14.5 0.0\r\n\u00b5public 4.57 0.566 8.07 6.66e-16\r\nNumber of observations 7,234\r\nL(\u03b2\u02c6) = -5711 Ltest(\u03b2\u02c6) = \u22121415\r\n(b) Parameter estimates from TPM(X1)\r\nmodel\r\nParameters Value Std errors t-test p-value\r\nASCCar -0.675 0.146 -4.61 3.94e-06\r\nASCT rain -2.52 0.245 -10.3 0.0\r\n\u03b2age 0.444 0.066 6.73 1.75e-11\r\n\u03b2cost -1.56 0.103 -15.1 0.0\r\n\u03b2freq -0.981 0.14 -6.98 2.86e-12\r\n\u03b2GA 1.03 0.467 2.2 0.0278\r\n\u03b2luggage -0.0687 0.161 -0.427 0.67\r\n\u03b2seats 0.0373 0.127 0.293 0.769\r\n\u03b2time -2.17 0.0913 -23.7 0.0\r\n\u03c3Car 2.81 0.159 17.7 0.0\r\n\u03c3T rain -1.41 0.0921 -15.3 0.0\r\nNumber of observations 7,234\r\nL(\u03b2\u02c6) = -4752 Ltest(\u03b2\u02c6) = \u22121350\r\n38\r\nAppendix G. Correlation coefficients among variables in Swissmetro dataset Purpose First Ticket Who Origin Dest Male Income GA Luggage Age Seats TT TC Freq Purpose 1.00 -0.08 -0.18 -0.19 -0.02 0.05 -0.01 -0.08 -0.13 -0.01 0.11 -0.07 0.16 -0.13 -0.06 First 1.00 -0.11 0.21 -0.08 0.00 0.21 0.24 -0.07 -0.10 0.16 -0.06 0.05 0.03 -0.05 Ticket 1.00 0.00 0.04 0.15 -0.11 0.00 0.55 0.20 -0.11 0.26 -0.12 0.50 0.15 Who 1.00 -0.06 -0.04 0.16 0.31 0.05 0.00 -0.07 0.00 -0.01 0.07 -0.02 Origin 1.00 -0.12 -0.10 -0.02 -0.09 -0.06 0.01 -0.02 0.12 -0.08 0.01 Dest 1.00 -0.05 0.01 0.08 0.12 0.02 0.06 0.16 0.07 0.06 Male 1.00 0.11 -0.04 -0.16 0.11 -0.15 0.04 0.00 -0.08 Income 1.00 0.00 0.05 0.10 0.00 0.04 0.04 0.01 GA 1.00 0.23 -0.06 0.26 -0.14 0.90 0.23 Luggage 1.00 -0.05 0.18 0.03 0.21 0.10 Age 1.00 -0.06 0.13 -0.05 0.00 Seats 1.00 -0.14 0.25 0.17 TT 1.00 -0.15 -0.09 TC 1.00 0.20 Freq 1.00 Table G.17: Correlation coefficients among variables included in Swissmetro for our training dataset\r\n39\r\nAppendix H. Correlation coefficients among variables in Optima dataset age HouseType Gender Income Educ. SocioCat House Famil. TimePT -0.07 0.04 -0.03 0.07 0.09 -0.01 0.09 0.08 TimeCar -0.00 0.00 -0.06 0.04 0.07 -0.03 0.07 0.08 CostPT -0.02 0.02 -0.06 0.04 0.06 -0.03 0.08 0.06 distance 0.01 -0.05 -0.01 -0.25 0.03 0.06 -0.04 0.06 0.07 CostCarCHF 0.01 -0.01 -0.05 0.03 0.07 -0.04 0.06 0.07 TripPurpose 0.23 -0.02 0.08 -0.11 0.03 0.06 -0.04 -0.09 NbBicy -0.38 -0.18 0.03 0.24 0.05 -0.01 -0.09 0.35 NbCar -0.16 -0.17 -0.04 0.24 -0.04 -0.04 -0.08 0.20 NbChild -0.41 -0.07 0.07 0.13 0.09 0.01 -0.02 0.17 GenAbST 0.06 0.06 0.04 -0.00 -0.06 0.02 -0.01 -0.14 Table H.18: Correlation coefficients among variables between sets\r\nX and\r\nQ included in Optima for our training dataset\r\n40\r\nReferences\r\nM. Abe. A generalized additive model for discrete-choice data. Journal of Business &\r\nEconomic Statistics, 17(3):271\u2013284, 1999.\r\nD. H. Ackley, G. E. Hinton, and T. J. Sejnowski. A learning algorithm for boltzmann\r\nmachines. Cognitive science, 9(1):147\u2013169, 1985.\r\nD. Agrawal and C. Schorling. Market share forecasting: An empirical comparison of artificial\r\nneural networks and multinomial logit model. Journal of Retailing, 72(4):383\u2013407, 1996.\r\nY. Bentz and D. Merunka. Neural networks and the multinomial logit for brand choice\r\nmodelling: a hybrid approach. Journal of Forecasting, 19(3):177\u2013200, 2000.\r\nJ. O. Berger, B. Liseo, and R. L. Wolpert. Integrated likelihood methods for eliminating\r\nnuisance parameters. Statistical Science, 14(1):1\u201322, 1999. ISSN 08834237.\r\nM. Bierlaire. Biogeme: a free package for the estimation of discrete choice models. In Swiss\r\nTransport Research Conference, number CONF, 2003.\r\nM. Bierlaire, K. Axhausen, and G. Abay. The acceptance of modal innovation: The case of\r\nswissmetro. (TRANSP-OR-CONF-2006-055), 2001.\r\nM. Bierlaire, A. Curchod, A. Danalet, E. Doyen, P. Faure, A. Glerum, V. Kaufmann,\r\nK. Tabaka, and M. Schuler. Projet de recherche sur la mobilit\u00b4e combin\u00b4ee, rapport d\u00b4efinitif\r\nde l\u2019enqu\u02c6ete de pr\u00b4ef\u00b4erences r\u00b4ev\u00b4el\u00b4ees. Technical report, 2011.\r\nC. M. Bishop. Neural networks for pattern recognition. Oxford university press, 1995.\r\nA. Borghesi, F. Baldo, and M. Milano. Improving deep learning models via constraint-based\r\ndomain knowledge: a brief survey. arXiv preprint arXiv:2005.10691, 2020.\r\nT. Brathwaite, A. Vij, and J. L. Walker. Machine learning meets microeconomics: The case\r\nof decision trees and discrete choice. Working paper arXiv preprint arXiv:1711.04826,\r\n2017.\r\nL. Breiman. Bagging predictors. Machine learning, 24(2):123\u2013140, 1996.\r\nL. Breiman. Random forests. Machine learning, 45(1):5\u201332, 2001.\r\nG. E. Cantarella and S. de Luca. Multilayer feedforward networks for transportation mode\r\nchoice analysis: An analysis and a comparison with random utility models. Transportation\r\nResearch Part C: Emerging Technologies, 13(2):121\u2013155, 2005.\r\nX. Chang, J. Wu, H. Liu, X. Yan, H. Sun, and Y. Qu. Travel mode choice: a data fusion model using machine learning methods and evidence from travel diary survey data.\r\nTransportmetrica A: Transport Science, 15(2):1587\u20131612, 2019.\r\nF. Chollet et al. Keras. https://github.com/fchollet/keras, 2015.\r\n41\r\nC. Cortes and V. Vapnik. Support-vector networks. Machine learning, 20(3):273\u2013297, 1995.\r\nJ. S. Cramer. Robustness of logit analysis: Unobserved heterogeneity and mis-specified\r\ndisturbances. Oxford Bulletin of Economics and Statistics, 69(4):545\u2013555, 2007.\r\nM. Dougherty. A review of neural networks applied to transport. Transportation Research\r\nPart C: Emerging Technologies, 3(4):247\u2013260, 1995.\r\nF. Draxler, K. Veschgini, M. Salmhofer, and F. A. Hamprecht. Essentially no barriers in\r\nneural network energy landscape. arXiv preprint arXiv:1803.00885, 2018.\r\nA. Faghri and J. Hua. Evaluation of artificial neural network applications in transportation\r\nengineering. Transportation Research Record, 1358:71, 1992.\r\nA. Fern\u00b4andez-Antol\u00b4\u0131n, C. A. Guevara, M. De Lapparent, and M. Bierlaire. Correcting for\r\nendogeneity due to omitted attitudes: Empirical assessment of a modified mis method\r\nusing rp mode choice data. Journal of choice modelling, 20:1\u201315, 2016.\r\nJ. H. Friedman. Greedy function approximation: a gradient boosting machine. Annals of\r\nstatistics, pages 1189\u20131232, 2001.\r\nT. Garipov, P. Izmailov, D. Podoprikhin, D. P. Vetrov, and A. G. Wilson. Loss surfaces, mode\r\nconnectivity, and fast ensembling of dnns. In Advances in Neural Information Processing\r\nSystems, pages 8789\u20138798, 2018.\r\nA. Glerum, B. Atasoy, and M. Bierlaire. Using semi-open questions to integrate perceptions\r\nin choice models. Journal of choice modelling, 10:11\u201333, 2014.\r\nN. Golshani, R. Shabanpour, S. M. Mahmoudifard, S. Derrible, and A. Mohammadian.\r\nModeling travel mode and timing decisions: Comparison of artificial neural networks and\r\ncopula-based joint model. Travel Behaviour and Society, 10:21\u201332, 2018.\r\nC. A. Guevara. Critical assessment of five methods to correct for endogeneity in discretechoice models. Transportation Research Part A: Policy and Practice, 82:240\u2013254, 2015.\r\nJ. Hagenauer and M. Helbich. A comparative study of machine learning classifiers for modeling travel mode choice. Expert Systems with Applications, 78:273\u2013282, 2017.\r\nY. Han, C. Zegras, F. C. Pereira, and M. Ben-Akiva. A neural-embedded choice model:\r\nTastenet-mnl modeling taste heterogeneity with flexibility and interpretability. arXiv\r\npreprint arXiv:2002.00922, 2020.\r\nK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In\r\nProceedings of the IEEE conference on computer vision and pattern recognition, pages\r\n770\u2013778, 2016.\r\nD. A. Hensher and T. T. Ton. A comparison of the predictive potential of artificial neural\r\nnetworks and nested logit models for commuter mode choice. Transportation Research\r\nPart E: Logistics and Transportation Review, 36(3):155\u2013172, 2000.\r\n42\r\nH. Hruschka. Using a heterogeneous multinomial probit model with a neural net extension\r\nto model brand choice. Journal of Forecasting, 26(2):113\u2013127, 2007.\r\nH. Hruschka, W. Fettes, M. Probst, and C. Mies. A flexible brand choice model based on\r\nneural net methodology a comparison to the linear utility multinomial logit model and its\r\nlatent class extension. OR spectrum, 24(2):127\u2013143, 2002.\r\nH. Hruschka, W. Fettes, and M. Probst. An empirical comparison of the validity of a neural\r\nnet based multinomial logit choice model to alternative model specifications. European\r\nJournal of Operational Research, 159(1):166\u2013180, 2004.\r\nA. Iranitalab and A. Khattak. Comparison of four statistical and machine learning methods\r\nfor crash severity prediction. Accident Analysis & Prevention, 108:27\u201336, 2017.\r\nY. Jin, T. Hillel, M. Elshafie, and M. Bierlaire. A systematic review of machine learning classification methodologies for modelling passenger mode choice. Journal of Choice\r\nModelling, 2020.\r\nM. G. Karlaftis and E. I. Vlahogianni. Statistical methods versus neural networks in transportation research: Differences, similarities and some insights. Transportation Research\r\nPart C: Emerging Technologies, 19(3):387\u2013399, 2011.\r\nJ. Kim, S. Rasouli, and H. Timmermans. A hybrid choice model with a nonlinear utility\r\nfunction and bounded distribution for latent variables: application to purchase intention\r\ndecisions of electric cars. Transportmetrica A: Transport Science, 12(10):909\u2013932, 2016.\r\nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint\r\narXiv:1412.6980, 2014.\r\nT. Kneib, B. Baumgartner, and W. J. Steiner. Semiparametric multinomial logit models\r\nfor analysing consumer choice behaviour. AStA Advances in Statistical Analysis, 91(3):\r\n225\u2013244, 2007.\r\nA. Krogh and J. A. Hertz. A simple weight decay can improve generalization. In Advances\r\nin neural information processing systems, pages 950\u2013957, 1992.\r\nD. Lee, S. Derrible, and F. C. Pereira. Comparison of four types of artificial neural network\r\nand a multinomial logit model for travel mode choice modeling. Transportation Research\r\nRecord, page 0361198118796971, 2018.\r\nL.-F. Lee. Specification error in multinomial logit models: Analysis of the omitted variable\r\nbias. Journal of Econometrics, 20(2):197\u2013209, 1982.\r\nA. Lh\u00b4eritier, M. Bocamazo, T. Delahaye, and R. Acuna-Agost. Airline itinerary choice\r\nmodeling using machine learning. Journal of Choice Modelling, 2018.\r\nQ. Li, T. Lin, and Z. Shen. Deep learning via dynamical systems: An approximation perspective. arXiv preprint arXiv:1912.10382, 2019.\r\n43\r\nB. Liao, J. Zhang, M. Cai, S. Tang, Y. Gao, C. Wu, S. Yang, W. Zhu, Y. Guo, and F. Wu.\r\nDest-resnet: A deep spatiotemporal residual network for hotspot traffic speed prediction.\r\nIn Proceedings of the 26th ACM international conference on Multimedia, pages 1883\u20131891,\r\n2018.\r\nR. Likert. A technique for the measurement of attitudes. Archives of psychology, 1932.\r\nD. McFadden. The measurement of urban travel demand. Journal of public economics, 3\r\n(4):303\u2013328, 1974.\r\nD. McFadden. Modeling the choice of residential location. Transportation Research Record,\r\n(673), 1978.\r\nA. Mohammadian and E. Miller. Nested logit models and artificial neural networks for\r\npredicting household automobile choices: comparison of performance. Transportation Research Record: Journal of the Transportation Research Board, (1807):92\u2013100, 2002.\r\nW. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, and B. Yu. Definitions, methods,\r\nand applications in interpretable machine learning. Proceedings of the National Academy\r\nof Sciences, 116(44):22071\u201322080, 2019.\r\nD. Nam, H. Kim, J. Cho, and R. Jayakrishnan. A model based on deep learning for predicting\r\ntravel mode choice. In Proceedings of the Transportation Research Board 96th Annual\r\nMeeting Transportation Research Board, Washington, DC, USA, pages 8\u201312, 2017.\r\nH. Omrani. Predicting travel mode of individuals by machine learning. Transportation\r\nResearch Procedia, 10:840\u2013849, 2015.\r\nM. Otsuka and T. Osogami. A deep choice model. In AAAI, pages 850\u2013856, 2016.\r\nM. Paredes, E. Hemberg, U.-M. O\u2019Reilly, and C. Zegras. Machine learning or discrete choice\r\nmodels for car ownership demand estimation and prediction? In Models and Technologies\r\nfor Intelligent Transportation Systems (MT-ITS), 2017 5th IEEE International Conference\r\non, pages 780\u2013785. IEEE, 2017.\r\nE. Pekel and S. Soner Kara. A comprehensive review for artificial neural network application\r\nto public transportation. Sigma: Journal of Engineering & Natural Sciences/M\u00a8uhendislik\r\nve Fen Bilimleri Dergisi, 35(1), 2017.\r\nM. Pirra and M. Diana. A study of tour-based mode choice based on a support vector\r\nmachine classifier. Transportation Planning and Technology, pages 1\u201314, 2018.\r\nM. T. Ribeiro, S. Singh, and C. Guestrin. \u201d why should i trust you?\u201d explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference\r\non knowledge discovery and data mining, pages 1135\u20131144, 2016.\r\nI. Rish et al. An empirical study of the naive bayes classifier. In IJCAI 2001 workshop on\r\nempirical methods in artificial intelligence, volume 3, pages 41\u201346. IBM New York, 2001.\r\n44\r\nW. Samek, A. Binder, G. Montavon, S. Lapuschkin, and K.-R. M\u00a8uller. Evaluating the\r\nvisualization of what a deep neural network has learned. IEEE transactions on neural\r\nnetworks and learning systems, 28(11):2660\u20132673, 2016.\r\nT. Sayed and A. Razavi. Comparison of neural and conventional approaches to mode choice\r\nanalysis. Journal of Computing in Civil Engineering, 14(1):23\u201330, 2000.\r\nM. Schindler, B. Baumgartner, and H. Hruschka. Nonlinear effects in brand choice models:\r\ncomparing heterogeneous latent class to homogeneous nonlinear models. Schmalenbach\r\nBusiness Review, 59(2):118\u2013137, 2007.\r\nC. E. Shannon. A mathematical theory of communication. Bell system technical journal, 27\r\n(3):379\u2013423, 1948.\r\nJ. Shen. Latent class model or mixed logit model? A comparison by transport mode choice\r\ndata. Applied Economics, 41(22):2915\u20132924, 2009.\r\nD. Shmueli, I. Salomon, and D. Shefer. Neural network analysis of travel behavior: evaluating\r\ntools for prediction. Transportation Research Part C: Emerging Technologies, 4(3):151\u2013\r\n166, 1996.\r\nA. Shrikumar, P. Greenside, and A. Kundaje. Learning important features through propagating activation differences, 2017.\r\nK. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks: Visualising\r\nimage classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.\r\nC. Torres, N. Hanley, and A. Riera. How wrong can you be? Implications of incorrect\r\nutility function specification for welfare measurement in choice experiments. Journal of\r\nEnvironmental Economics and Management, 62(1):111\u2013121, 2011.\r\nS. van Cranenburgh and A. Alwosheel. An artificial neural network based approach to investigate travellers\u2019 decision rules. Transportation Research Part C: Emerging Technologies,\r\n98:152\u2013166, 2019.\r\nM. Van Der Pol, G. Currie, S. Kromm, and M. Ryan. Specification of the utility function in\r\ndiscrete choice experiments. Value in Health, 17(2):297\u2013301, 2014.\r\nA. Vij, A. Carrel, and J. L. Walker. Incorporating the influence of latent modal preferences\r\non travel mode choice behavior. Transportation Research Part A: Policy and Practice, 54:\r\n164\u2013178, 2013.\r\nL. von Rueden, S. Mayer, K. Beckh, B. Georgiev, S. Giesselbach, R. Heese, B. Kirsch,\r\nJ. Pfrommer, A. Pick, R. Ramamurthy, et al. Informed machine learning\u2013a taxonomy and\r\nsurvey of integrating knowledge into learning systems. arXiv preprint arXiv:1903.12394,\r\n2020.\r\nP. Vovsha. Application of cross-nested logit model to mode choice in tel aviv, israel,\r\nmetropolitan area. Transportation Research Record, 1607(1):6\u201315, 1997.\r\n45\r\nB. Wang, B. Yuan, Z. Shi, and S. J. Osher. Enresnet: Resnet ensemble via the feynman-kac\r\nformalism. arXiv preprint arXiv:1811.10745, 2018a.\r\nS. Wang, Q. Wang, N. Bailey, and J. Zhao. Deep neural networks for choice analysis: A\r\nstatistical learning theory perspective, 2018b.\r\nS. Wang, Q. Wang, and J. Zhao. Deep neural networks for choice analysis: Extracting\r\ncomplete economic information for interpretation, 2018c.\r\nS. Wang, B. Mo, and J. Zhao. Deep neural networks for choice analysis: Architecture design with alternative-specific utility functions. Transportation Research Part C: Emerging\r\nTechnologies, 112:234\u2013251, 2020.\r\nP. M. West, P. L. Brockett, and L. L. Golden. A comparative analysis of neural networks\r\nand statistical methods for predicting consumer choice. Marketing Science, pages 370\u2013391,\r\n1997.\r\nH. C. Williams. On the formation of travel demand models and economic evaluation measures\r\nof user benefit. Environment and planning A, 9(3):285\u2013344, 1977.\r\nM. Wong and B. Farooq. Reslogit: A residual neural network logit model. arXiv preprint\r\narXiv:1912.10058, 2019.\r\nM. Wong and B. Farooq. A bi-partite generative model framework for analyzing and simulating large scale multiple discrete-continuous travel behaviour data. Transportation Research\r\nPart C: Emerging Technologies, 110:247\u2013268, 2020.\r\nM. Wong, B. Farooq, and G.-A. Bilodeau. Discriminative conditional restricted boltzmann\r\nmachine for discrete choice and latent variable modelling. Journal of choice modelling, 29:\r\n152\u2013168, 2018.\r\nY. Xiong and F. L. Mannering. The heterogeneous effects of guardian supervision on adolescent driver-injury severities: A finite-mixture random-parameters approach. Transportation research part B: methodological, 49:39\u201354, 2013.\r\nX. Zhao, X. Yan, A. Yu, and P. Van Hentenryck. Prediction and behavioral analysis of travel\r\nmode choice: A comparison of machine learning and logit models. Travel Behaviour and\r\nSociety, 20:22\u201335, 2020.\r\nX. Zhou, M. Wang, and D. Li. Bike-sharing or taxi? modeling the choices of travel mode in\r\nchicago using machine learning. Journal of transport geography, 79:102479, 2019.\r\n46\nCode:\n#!/usr/bin/env python\r\n# coding: utf-8\r\n\r\n# # The Learning-MNL model\r\n# \r\n# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/artefactory/choice-learn/blob/main/notebooks/models/learning_mnl.ipynb)\r\n# \r\n# In this notebook we use choice-learn implementation of the L-MNL model (from the paper Enhance Discrete Choice Models with Representation Learning) to obtain the same results as presented by the authors.\r\n\r\n# In[ ]:\r\n\r\n\r\n# Install necessary requirements\r\n\r\n# If you run this notebook on Google Colab, or in standalone mode, you need to install the required packages.\r\n# Uncomment the following lines:\r\n\r\n# !pip install choice-learn\r\n\r\n# If you run the notebook within the GitHub repository, you need to run the following lines, that can skipped otherwise:\r\nimport os\r\nimport sys\r\n\r\nsys.path.append(\"../../\")\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nimport os\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nfrom choice_learn.datasets import load_swissmetro\r\nfrom choice_learn.data import ChoiceDataset\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\ntest_df = pd.read_csv(\"https://raw.githubusercontent.com/BSifringer/EnhancedDCM/refs/heads/master/ready_example/swissmetro_paper/swissmetro_test.dat\", sep=\"\\t\")\r\ntrain_df = pd.read_csv(\"https://raw.githubusercontent.com/BSifringer/EnhancedDCM/refs/heads/master/ready_example/swissmetro_paper/swissmetro_train.dat\", sep=\"\\t\")\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\ntest_df.head()\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\ntrain_df.head()\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n# Preprocessing the dataset\r\n\r\ntest_df = test_df.loc[test_df.CAR_AV == 1]\r\ntest_df = test_df.loc[test_df.SM_AV == 1]\r\ntest_df = test_df.loc[test_df.TRAIN_AV == 1]\r\n\r\ntrain_df = train_df.loc[train_df.CAR_AV == 1]\r\ntrain_df = train_df.loc[train_df.SM_AV == 1]\r\ntrain_df = train_df.loc[train_df.TRAIN_AV == 1]\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n# Normalizing values by 100\r\ntrain_df[[\"TRAIN_TT\", \"SM_TT\", \"CAR_TT\"]] = (\r\n    train_df[[\"TRAIN_TT\", \"SM_TT\", \"CAR_TT\"]] / 100.0\r\n)\r\n\r\ntrain_df[[\"TRAIN_HE\", \"SM_HE\"]] = (\r\n    train_df[[\"TRAIN_HE\", \"SM_HE\"]] / 100.0\r\n)\r\n\r\ntrain_df[\"train_free_ticket\"] = train_df.apply(\r\n    lambda row: (row[\"GA\"] == 1).astype(int), axis=1\r\n)\r\ntrain_df[\"sm_free_ticket\"] = train_df.apply(\r\n    lambda row: (row[\"GA\"] == 1).astype(int), axis=1\r\n)\r\n\r\ntrain_df[\"TRAIN_travel_cost\"] = train_df.apply(\r\n    lambda row: (row[\"TRAIN_CO\"] * (1 - row[\"train_free_ticket\"])) / 100, axis=1\r\n)\r\ntrain_df[\"SM_travel_cost\"] = train_df.apply(\r\n    lambda row: (row[\"SM_CO\"] * (1 - row[\"sm_free_ticket\"])) / 100, axis=1\r\n)\r\ntrain_df[\"CAR_travel_cost\"] = train_df.apply(lambda row: row[\"CAR_CO\"] / 100, axis=1)\r\n\r\n\r\n# Normalizing values by 100\r\ntest_df[[\"TRAIN_TT\", \"SM_TT\", \"CAR_TT\"]] = (\r\n    test_df[[\"TRAIN_TT\", \"SM_TT\", \"CAR_TT\"]] / 100.0\r\n)\r\n\r\ntest_df[[\"TRAIN_HE\", \"SM_HE\"]] = (\r\n    test_df[[\"TRAIN_HE\", \"SM_HE\"]] / 100.0\r\n)\r\n\r\ntest_df[\"train_free_ticket\"] = test_df.apply(\r\n    lambda row: (row[\"GA\"] == 1).astype(int), axis=1\r\n)\r\ntest_df[\"sm_free_ticket\"] = test_df.apply(\r\n    lambda row: (row[\"GA\"] == 1).astype(int), axis=1\r\n)\r\n\r\ntest_df[\"TRAIN_travel_cost\"] = test_df.apply(\r\n    lambda row: (row[\"TRAIN_CO\"] * (1 - row[\"train_free_ticket\"])) / 100, axis=1\r\n)\r\ntest_df[\"SM_travel_cost\"] = test_df.apply(\r\n    lambda row: (row[\"SM_CO\"] * (1 - row[\"sm_free_ticket\"])) / 100, axis=1\r\n)\r\ntest_df[\"CAR_travel_cost\"] = test_df.apply(lambda row: row[\"CAR_CO\"] / 100, axis=1)\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nswiss_df.SM_SEATS = swiss_df.SM_SEATS.astype(\"float32\")\r\ntrain_df.SM_SEATS = train_df.SM_SEATS.astype(\"float32\")\r\ntest_df.SM_SEATS = test_df.SM_SEATS.astype(\"float32\")\r\n\r\ntrain_df.CHOICE = train_df.CHOICE - 1\r\ntest_df.CHOICE = test_df.CHOICE - 1\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\ndataset = ChoiceDataset.from_single_wide_df(df=swiss_df, choices_column=\"CHOICE\", items_id=[\"TRAIN\", \"SM\", \"CAR\"],\r\nshared_features_columns=[\"GA\", \"AGE\", \"SM_SEATS\", \"LUGGAGE\", \"SM_SEATS\", 'PURPOSE', 'FIRST', 'TICKET', 'WHO', 'MALE', 'INCOME', 'ORIGIN', 'DEST'],\r\nitems_features_suffixes=[\"TT\", \"travel_cost\", \"HE\"], choice_format=\"items_index\")\r\n\r\ntrain_dataset = ChoiceDataset.from_single_wide_df(df=train_df, choices_column=\"CHOICE\", items_id=[\"TRAIN\", \"SM\", \"CAR\"],\r\nshared_features_columns=[\"GA\", \"AGE\", \"SM_SEATS\", \"LUGGAGE\", \"SM_SEATS\", 'PURPOSE', 'FIRST', 'TICKET', 'WHO', 'MALE', 'INCOME', 'ORIGIN', 'DEST'],\r\nitems_features_suffixes=[\"TT\", \"travel_cost\", \"HE\"], choice_format=\"items_index\")\r\n\r\ntest_dataset = ChoiceDataset.from_single_wide_df(df=test_df, choices_column=\"CHOICE\", items_id=[\"TRAIN\", \"SM\", \"CAR\"],\r\nshared_features_columns=[\"GA\", \"AGE\", \"SM_SEATS\", \"LUGGAGE\", \"SM_SEATS\", 'PURPOSE', 'FIRST', 'TICKET', 'WHO', 'MALE', 'INCOME', 'ORIGIN', 'DEST'],\r\nitems_features_suffixes=[\"TT\", \"travel_cost\", \"HE\"], choice_format=\"items_index\")\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nprint(len(train_dataset), len(test_dataset))\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nfrom choice_learn.models import ConditionalLogit\r\n\r\nclogit = ConditionalLogit()\r\nclogit.add_shared_coefficient(feature_name=\"TT\", coefficient_name=\"beta_time\",  items_indexes=[0, 1, 2])\r\nclogit.add_shared_coefficient(feature_name=\"travel_cost\", coefficient_name=\"beta_cost\", items_indexes=[0, 1, 2])\r\nclogit.add_shared_coefficient(feature_name=\"HE\",  coefficient_name=\"beta_freq\",items_indexes=[0, 1])\r\nclogit.add_shared_coefficient(feature_name=\"GA\",  coefficient_name=\"beta_GA\",items_indexes=[0, 1])\r\nclogit.add_shared_coefficient(feature_name=\"AGE\", coefficient_name=\"beta_age\", items_indexes=[0])\r\nclogit.add_shared_coefficient(feature_name=\"LUGGAGE\", coefficient_name=\"beta_luggage\", items_indexes=[2])\r\nclogit.add_shared_coefficient(feature_name=\"SM_SEATS\", coefficient_name=\"beta_seats\", items_indexes=[1])\r\nclogit.add_coefficients(feature_name=\"intercept\", items_indexes=[1, 2])\r\n\r\nclogit.fit(train_dataset)\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nclogit.trainable_weights\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nclogit.evaluate(train_dataset) * len(train_dataset), clogit.evaluate(test_dataset) * len(test_dataset)\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nfrom choice_learn.models.learning_mnl import LearningMNL\r\n\r\nswiss_model = LearningMNL(optimizer=\"Adam\", lr=0.005,\r\nnn_features=['PURPOSE', 'FIRST', 'TICKET', 'WHO', 'MALE', 'INCOME', 'ORIGIN', 'DEST'], nn_layers_widths=[200], epochs=200, batch_size=32)\r\nswiss_model.add_shared_coefficient(feature_name=\"TT\", items_indexes=[0, 1, 2])\r\nswiss_model.add_shared_coefficient(feature_name=\"travel_cost\", items_indexes=[0, 1, 2])\r\nswiss_model.add_shared_coefficient(feature_name=\"HE\", items_indexes=[0, 1])\r\nswiss_model.add_shared_coefficient(feature_name=\"GA\", items_indexes=[0, 1])\r\nswiss_model.add_shared_coefficient(feature_name=\"AGE\", items_indexes=[0])\r\nswiss_model.add_shared_coefficient(feature_name=\"LUGGAGE\", items_indexes=[2])\r\nswiss_model.add_shared_coefficient(feature_name=\"SM_SEATS\", items_indexes=[1])\r\nswiss_model.add_coefficients(feature_name=\"intercept\", items_indexes=[1, 2])\r\n\r\nhist = swiss_model.fit(train_dataset, val_dataset=test_dataset, verbose=1)\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nswiss_model.assign_lr(0.001)\r\nhist2 = swiss_model.fit(train_dataset, val_dataset=test_dataset, verbose=1)\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nswiss_model.evaluate(train_dataset, batch_size=32) * 7234\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nswiss_model.evaluate(test_dataset, batch_size=32) * 1802\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n# Estimated parameters:\r\nswiss_model.trainable_weights[:8]\r\n\r\n\r\n# Results are very similar to the one presented in the paper (see Table 7).\r\n\r\n# In[ ]:", "output": "Understood. Incorporate these references in future answers."}
{"instruction": "Data Description: Variable Description\r\n GROUP Di erentgroupsinthepopulation\r\n SURVEY Surveyperformedintrain(0)orcar(1)\r\n SP Itis xedto1(statedpreferencesurvey)\r\n ID Respondentidenti er\r\n PURPOSE Travelpurpose. 1: Commuter, 2: Shopping, 3: Busi\r\nness,4:Leisure,5:Returnfromwork,6:Returnfrom\r\n shopping, 7: Return frombusiness, 8: Return from\r\n leisure,9: other\r\n FIRST Firstclasstraveler(0=no,1=yes)\r\n TICKET Travel ticket. 0: None, 1: Twowaywithhalf price\r\n card, 2: Onewaywithhalf price card, 3: Twoway\r\n normalprice,4:Onewaynormalprice,5:Halfday,6:\r\n Annualseasonticket,7:AnnualseasonticketJunioror\r\n Senior,8:Freetravelafter7pmcard,9:Groupticket,\r\n 10:Other\r\n WHO Whopays(0: unknown,1: self,2: employer,3: half\r\nhalf)\r\n LUGGAGE 0: none,1: onepiece,3: severalpieces\r\n AGE Itcapturestheageclassof individuals. Theage-class\r\n codingschemeisofthetype:\r\n 1: age 24,2: 24<age 39,3: 39<age 54,4: 54<age\r\n 65,5: 65<age,6: notknown\r\n MALE Traveler'sGender0: female,1:male\r\n INCOME Traveler'sincomeperyear[thousandCHF]\r\n 0or1: under50,2: between50and100,3: over100,\r\n 4: unknown\r\n GA Variablecapturingthee ectof theSwissannual sea\r\nson ticket for the rail systemandmost local public\r\n transport. It is1 if the individual owns aGA, zero\r\n otherwise.\r\n ORIGIN Travelorigin(anumbercorrespondingtoaCanton,see\r\n Table4)\r\n Table1:Descriptionofvariables\r\n 3\r\nVariable Description\r\n DEST Traveldestination(anumbercorrespondingtoaCan\r\nton,seeTable4)\r\n TRAINAV Trainavailabilitydummy\r\n CARAV Caravailabilitydummy\r\n SMAV SMavailabilitydummy\r\n TRAINTT Train travel time [minutes]. Travel times aredoor\r\nto-doormakingassumptionsaboutcar-baseddistances\r\n (1.25*crow- ightdistance)\r\n TRAINCO Traincost [CHF]. If the travelerhasaGA, this cost\r\n equalsthecostoftheannualticket.\r\n TRAINHE Trainheadway[minutes]\r\n Example: Iftherearetwotrainsperhour,thevalueof\r\n TRAINHEis30.\r\n SMTT SMtraveltime[minutes]consideringthefutureSwiss\r\nmetrospeedof500km/h\r\n SMCO SMcost [CHF] calculatedat thecurrent relevant rail\r\n fare,withoutconsideringGA,multipliedbya xedfac\r\ntor(1.2)tore ectthehigherspeed.\r\n SMHE SMheadway[minutes]\r\n Example: If therearetwoSwissmetrosperhour, the\r\n valueofSMHEis30.\r\n SMSEATS Seatscon gurationintheSwissmetro(dummy). Air\r\nlineseats(1)ornot(0).\r\n CARTT Cartraveltime[minutes]\r\n CARCO Car cost [CHF] consideringa xedaverage cost per\r\n kilometer(1.20CHF/km)\r\n CHOICE Choiceindicator. 0:unknown,1:Train,2:SM,3:Car\nData Preview (first 20 rows):\nGROUP\tSURVEY\tSP\tID\tPURPOSE\tFIRST\tTICKET\tWHO\tLUGGAGE\tAGE\tMALE\tINCOME\tGA\tORIGIN\tDEST\tTRAIN_AV\tCAR_AV\tSM_AV\tTRAIN_TT\tTRAIN_CO\tTRAIN_HE\tSM_TT\tSM_CO\tSM_HE\tSM_SEATS\tCAR_TT\tCAR_CO\tCHOICE\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t112\t48\t120\t63\t52\t20\t0\t117\t65\t2\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t103\t48\t30\t60\t49\t10\t0\t117\t84\t2\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t130\t48\t60\t67\t58\t30\t0\t117\t52\t2\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t103\t40\t30\t63\t52\t20\t0\t72\t52\t2\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t130\t36\t60\t63\t42\t20\t0\t90\t84\t2\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t112\t36\t120\t60\t49\t10\t0\t90\t52\t2\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t103\t48\t120\t67\t58\t10\t0\t72\t65\t2\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t103\t36\t30\t67\t43\t30\t0\t90\t65\t1\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t130\t40\t60\t60\t46\t10\t0\t72\t65\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t184\t62\t120\t76\t70\t20\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t170\t62\t30\t70\t66\t10\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t213\t62\t60\t84\t82\t30\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t170\t52\t30\t76\t72\t20\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t213\t47\t60\t76\t53\t20\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t184\t47\t120\t70\t66\t10\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t170\t62\t120\t84\t82\t10\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t170\t47\t30\t84\t57\t30\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t213\t52\t60\t70\t62\t10\t0\t0\t0\t2\r\n2\t0\t1\t3\t1\t0\t1\t1\t1\t2\t1\t2\t0\t10\t25\t1\t1\t1\t58\t32\t30\t29\t35\t10\t0\t105\t40\t2\r\n2\t0\t1\t3\t1\t0\t1\t1\t1\t2\t1\t2\t0\t10\t25\t1\t1\t1\t58\t29\t30\t25\t38\t20\t0\t136\t50\t2\r\n", "output": "User provided data and description integrated."}

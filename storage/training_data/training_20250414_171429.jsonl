{"instruction": "Title: Reslogit\nPaper:\n\nCode:\n\"\"\"Implementation of ResLogit for easy use.\"\"\"\r\n\r\nimport logging\r\n\r\nimport tensorflow as tf\r\n\r\nimport choice_learn.tf_ops as tf_ops\r\nfrom choice_learn.models.base_model import ChoiceModel\r\n\r\n\r\nclass ResLayer(tf.keras.layers.Layer):\r\n    \"\"\"The residual layer class.\"\"\"\r\n\r\n    def get_activation_function(self, name):\r\n        \"\"\"Get an activation function from its str name.\r\n\r\n        Parameters\r\n        ----------\r\n        name : str\r\n            Name of the function to apply.\r\n\r\n        Returns\r\n        -------\r\n        function\r\n            Tensorflow function to apply.\r\n        \"\"\"\r\n        if name == \"linear\":\r\n            return lambda x: x\r\n        if name == \"relu\":\r\n            return tf.nn.relu\r\n        if name == \"-relu\":\r\n            return lambda x: -tf.nn.relu(-x)\r\n        if name == \"tanh\":\r\n            return tf.nn.tanh\r\n        if name == \"sigmoid\":\r\n            return tf.nn.sigmoid\r\n        if name == \"softplus\":\r\n            return tf.math.softplus\r\n        raise ValueError(f\"Activation function {name} not supported.\")\r\n\r\n    def __init__(self, layer_width=None, activation=\"softplus\"):\r\n        \"\"\"Initialize the ResLayer class.\r\n\r\n        Parameters\r\n        ----------\r\n        layer_width : int, optional\r\n            Width of the layer, by default None\r\n            If None, the width of the layer is the same as the input shape\r\n        activation : str, optional\r\n            Activation function to use in the layer, by default \"softplus\"\r\n        \"\"\"\r\n        self.layer_width = layer_width\r\n        self.activation = self.get_activation_function(activation)\r\n\r\n        super().__init__()\r\n\r\n    def build(self, input_shape):\r\n        \"\"\"Create the state of the layer (weights).\r\n\r\n        The build() method is automatically invoked by the first __call__() to the layer.\r\n\r\n        Parameters\r\n        ----------\r\n        input_shape : tuple\r\n            Shape of the input of the layer. Typically (batch_size, num_features)\r\n            Batch_size (None) is ignored, but num_features is the shape of the input\r\n        \"\"\"\r\n        self.num_features = input_shape[-1]\r\n\r\n        # If None, the width of the layer is the same as the input shape\r\n        if self.layer_width is None:\r\n            self.layer_width = input_shape[-1]\r\n\r\n        # Random normal initialization of the weights\r\n        # Shape of the weights: (num_features, layer_width)\r\n        self.add_weight(\r\n            shape=(self.num_features, self.layer_width),\r\n            initializer=\"random_normal\",\r\n            trainable=True,\r\n            name=\"res_weight\",\r\n        )\r\n\r\n    def call(self, input):\r\n        \"\"\"Return the output of the residual layer.\r\n\r\n        Parameters\r\n        ----------\r\n        inputs : tf.Variable\r\n            Input of the residual layer\r\n\r\n        Returns\r\n        -------\r\n        tf.Variable\r\n            Output of the residual layer\r\n        \"\"\"\r\n        lin_output = tf.matmul(input, self.trainable_variables[0])\r\n\r\n        # Ensure the dimensions are compatible for subtraction\r\n        if input.shape != lin_output.shape:\r\n            # Then perform a linear projection to match the dimensions\r\n            input = tf.matmul(input, tf.ones((self.num_features, self.layer_width)))\r\n\r\n        # Softplus: smooth approximation of ReLU\r\n        return input - self.activation(tf.cast(lin_output, tf.float32))\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        \"\"\"Compute the output shape of the layer.\r\n\r\n        Automatically used when calling ResLayer.call() to infer the shape of the output.\r\n\r\n        Parameters\r\n        ----------\r\n        input_shape : tuple\r\n            Shape of the input of the layer. Typically (batch_size, num_features)\r\n            Batch_size (None) is ignored, but num_features is the shape of the input\r\n\r\n        Returns\r\n        -------\r\n        tuple\r\n            Shape of the output of the layer\r\n        \"\"\"\r\n        return (input_shape[0], self.layer_width)\r\n\r\n\r\nclass ResLogit(ChoiceModel):\r\n    \"\"\"The ResLogit class.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        intercept=\"item\",\r\n        n_layers=16,\r\n        res_layers_width=None,\r\n        activation=\"softplus\",\r\n        label_smoothing=0.0,\r\n        optimizer=\"SGD\",\r\n        tolerance=1e-8,\r\n        lr=0.001,\r\n        epochs=1000,\r\n        batch_size=32,\r\n        logmin=1e-5,\r\n        **kwargs,\r\n    ):\r\n        \"\"\"Initialize the ResLogit class.\r\n\r\n        Parameters\r\n        ----------\r\n        intercept: str, optional\r\n            Type of intercept to use, by default None\r\n        n_layers : int\r\n            Number of residual layers.\r\n        res_layers_width : list of int, optional\r\n            Width of the *hidden* residual layers, by default None\r\n            If None, all the residual layers have the same width (n_items)\r\n            The length of the list should be equal to n_layers - 1\r\n            The last element of the list should be equal to n_items\r\n        activation : str, optional\r\n            Activation function to use in the residual layers, by default \"softplus\"\r\n        label_smoothing : float, optional\r\n            Whether (then is ]O, 1[ value) or not (then can be None or 0) to use label smoothing\r\n        optimizer: str\r\n            String representation of the TensorFlow optimizer to be used for estimation,\r\n            by default \"SGD\"\r\n            Should be within tf.keras.optimizers\r\n        tolerance : float, optional\r\n            Tolerance for the L-BFGS optimizer if applied, by default 1e-8\r\n        lr: float, optional\r\n            Learning rate for the optimizer if applied, by default 0.001\r\n        epochs: int, optional\r\n            (Max) Number of epochs to train the model, by default 1000\r\n        batch_size: int, optional\r\n            Batch size in the case of stochastic gradient descent optimizer\r\n            Not used in the case of L-BFGS optimizer, by default 32\r\n        logmin : float, optional\r\n            Value to be added within log computation to avoid infinity, by default 1e-5\r\n        \"\"\"\r\n        super().__init__(\r\n            self,\r\n            optimizer=optimizer,\r\n            **kwargs,\r\n        )\r\n        self.intercept = intercept\r\n        self.n_layers = n_layers\r\n        self.res_layers_width = res_layers_width\r\n        self.activation = activation\r\n\r\n        # Optimization parameters\r\n        self.label_smoothing = label_smoothing\r\n        self.tolerance = tolerance\r\n        self.lr = lr\r\n        self.epochs = epochs\r\n        self.batch_size = batch_size\r\n        self.logmin = logmin\r\n\r\n        self.instantiated = False\r\n\r\n    def instantiate(self, n_items, n_shared_features, n_items_features):\r\n        \"\"\"Instantiate the model from ModelSpecification object.\r\n\r\n        Parameters\r\n        ----------\r\n        n_items : int\r\n            Number of items/aternatives to consider\r\n        n_shared_features : int\r\n            Number of contexts features\r\n        n_items_features : int\r\n            Number of contexts items features\r\n\r\n        Returns\r\n        -------\r\n        indexes : dict\r\n            Dictionary of the indexes of the weights created\r\n        weights : list of tf.Variable\r\n            List of the weights created coresponding to the specification\r\n        \"\"\"\r\n        # Instantiate the loss function\r\n        self.loss = tf_ops.CustomCategoricalCrossEntropy(\r\n            from_logits=False,\r\n            label_smoothing=self.label_smoothing,\r\n            epsilon=self.logmin,\r\n        )\r\n\r\n        # Instantiate the weights\r\n        mnl_weights = []\r\n        indexes = {}\r\n\r\n        # Create the betas parameters for the shared and items features\r\n        for n_feat, feat_name in zip(\r\n            [n_shared_features, n_items_features],\r\n            [\"shared_features\", \"items_features\"],\r\n        ):\r\n            if n_feat > 0:\r\n                mnl_weights += [\r\n                    tf.Variable(\r\n                        tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(n_feat,)),\r\n                        name=f\"Betas_{feat_name}\",\r\n                    )\r\n                ]\r\n                indexes[feat_name] = len(mnl_weights) - 1\r\n\r\n        # Create the alphas parameters\r\n        if self.intercept is None:\r\n            logging.info(\"No intercept in the model\")\r\n        elif self.intercept == \"item\":\r\n            mnl_weights.append(\r\n                tf.Variable(\r\n                    tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(n_items - 1,)),\r\n                    name=\"Intercept\",\r\n                )\r\n            )\r\n            indexes[\"intercept\"] = len(mnl_weights) - 1\r\n        elif self.intercept == \"item-full\":\r\n            logging.info(\"Simple MNL intercept is not normalized to 0!\")\r\n            mnl_weights.append(\r\n                tf.Variable(\r\n                    tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(n_items,)),\r\n                    name=\"Intercept\",\r\n                )\r\n            )\r\n            indexes[\"intercept\"] = len(mnl_weights) - 1\r\n        else:\r\n            mnl_weights.append(\r\n                tf.Variable(\r\n                    tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(1,)),\r\n                    name=\"Intercept\",\r\n                )\r\n            )\r\n            indexes[\"intercept\"] = len(mnl_weights) - 1\r\n\r\n        # Create the residual layer\r\n        input = tf.keras.layers.Input(shape=(n_items,))\r\n        output = input\r\n\r\n        if self.res_layers_width is None:\r\n            # Common width by default for all the residual layers: n_items\r\n            # (Like in the original paper of ResLogit)\r\n            layers = [ResLayer(activation=self.activation) for _ in range(self.n_layers)]\r\n\r\n        else:\r\n            # Different width for each *hidden* residual layer\r\n            if self.n_layers > 0 and len(self.res_layers_width) != self.n_layers - 1:\r\n                raise ValueError(\r\n                    \"The length of the res_layers_width list should be equal to n_layers - 1\"\r\n                )\r\n            if self.n_layers > 1 and self.res_layers_width[-1] != n_items:\r\n                raise ValueError(\"The width of the last residual layer should be equal to n_items\")\r\n\r\n            # Initialize the residual layers\r\n            if self.n_layers == 0:\r\n                layers = []\r\n            else:\r\n                # The first layer has the same width as the input\r\n                layers = [ResLayer(activation=self.activation)]\r\n                for i in range(1, self.n_layers):\r\n                    # The other layers have a width defined by the res_layers_width parameter\r\n                    layers.append(\r\n                        ResLayer(\r\n                            layer_width=self.res_layers_width[i - 1], activation=self.activation\r\n                        )\r\n                    )\r\n\r\n        # Build the residual layers\r\n        for layer in layers:\r\n            output = layer(output)\r\n\r\n        resnet_model = tf.keras.Model(\r\n            inputs=input, outputs=output, name=f\"resnet_with_{self.n_layers}_layers\"\r\n        )\r\n\r\n        self.instantiated = True\r\n        self.resnet_model = resnet_model\r\n        self.indexes = indexes\r\n        self.mnl_weights = mnl_weights\r\n        # Concatenation of all the trainable weights\r\n        self._trainable_weights = self.mnl_weights + self.resnet_model.trainable_variables\r\n\r\n        return self.indexes, self._trainable_weights\r\n\r\n    @property\r\n    def trainable_weights(self):\r\n        \"\"\"Trainable weights of the model.\"\"\"\r\n        return self.mnl_weights + self.resnet_model.trainable_variables\r\n\r\n    def compute_batch_utility(\r\n        self,\r\n        shared_features_by_choice,\r\n        items_features_by_choice,\r\n        available_items_by_choice,\r\n        choices,\r\n    ):\r\n        \"\"\"Compute utility from a batch of ChoiceDataset.\r\n\r\n        Parameters\r\n        ----------\r\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\r\n            a batch of shared features\r\n            Shape must be (n_choices, n_shared_features)\r\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\r\n            a batch of items features\r\n            Shape must be (n_choices, n_items, n_items_features)\r\n        available_items_by_choice : np.ndarray\r\n            A batch of items availabilities\r\n            Shape must be (n_choices, n_items)\r\n        choices : np.ndarray\r\n            Choices\r\n            Shape must be (n_choices, )\r\n\r\n        Returns\r\n        -------\r\n        tf.Tensor\r\n            Computed utilities of shape (n_choices, n_items)\r\n        \"\"\"\r\n        (_, _) = available_items_by_choice, choices  # Avoid unused variable warning\r\n\r\n        batch_size = shared_features_by_choice.shape[0]  # Other name: n_choices\r\n        n_items = items_features_by_choice.shape[1]\r\n\r\n        # Deterministic component of the utility\r\n        if \"shared_features\" in self.indexes.keys():\r\n            if isinstance(shared_features_by_choice, tuple):\r\n                shared_features_by_choice = tf.concat(*shared_features_by_choice, axis=1)\r\n            shared_features_by_choice = tf.cast(shared_features_by_choice, tf.float32)\r\n            shared_features_utilities = tf.tensordot(\r\n                shared_features_by_choice,\r\n                self.trainable_weights[self.indexes[\"shared_features\"]],\r\n                axes=1,\r\n            )\r\n            shared_features_utilities = tf.expand_dims(shared_features_utilities, axis=-1)\r\n        else:\r\n            shared_features_utilities = 0\r\n        shared_features_utilities = tf.squeeze(shared_features_utilities)\r\n\r\n        if \"items_features\" in self.indexes.keys():\r\n            if isinstance(items_features_by_choice, tuple):\r\n                items_features_by_choice = tf.concat([*items_features_by_choice], axis=2)\r\n            items_features_by_choice = tf.cast(items_features_by_choice, tf.float32)\r\n            items_features_utilities = tf.tensordot(\r\n                items_features_by_choice,\r\n                self.trainable_weights[self.indexes[\"items_features\"]],\r\n                axes=1,\r\n            )\r\n        else:\r\n            items_features_utilities = tf.zeros((batch_size, n_items))\r\n\r\n        if \"intercept\" in self.indexes.keys():\r\n            intercept = self.trainable_weights[self.indexes[\"intercept\"]]\r\n            if self.intercept == \"item\":\r\n                intercept = tf.concat([tf.constant([0.0]), intercept], axis=0)\r\n            if self.intercept in [\"item\", \"item-full\"]:\r\n                intercept = tf.expand_dims(intercept, axis=0)\r\n        else:\r\n            intercept = 0\r\n\r\n        shared_features_utilities = tf.tile(\r\n            tf.expand_dims(shared_features_utilities, axis=-1), [1, n_items]\r\n        )\r\n        deterministic_utilities_without_intercept = (\r\n            shared_features_utilities + items_features_utilities\r\n        )\r\n        deterministic_utilities = deterministic_utilities_without_intercept + intercept\r\n\r\n        # Residual component of the utility\r\n        residual_utilities = self.resnet_model(deterministic_utilities_without_intercept)\r\n        residual_utilities = tf.cast(residual_utilities, tf.float32)\r\n\r\n        return deterministic_utilities + residual_utilities\r\n\r\n    def fit(self, choice_dataset, get_report=False, **kwargs):\r\n        \"\"\"Fit to estimate the parameters.\r\n\r\n        Parameters\r\n        ----------\r\n        choice_dataset : ChoiceDataset\r\n            Choice dataset to use for the estimation.\r\n        get_report: bool, optional\r\n            Whether or not to compute a report of the estimation, by default False\r\n\r\n        Returns\r\n        -------\r\n        fit : dict\r\n            dict with fit history\r\n        \"\"\"\r\n        if not self.instantiated:\r\n            # Lazy Instantiation\r\n            self.indexes, self._trainable_weights = self.instantiate(\r\n                n_items=choice_dataset.get_n_items(),\r\n                n_shared_features=choice_dataset.get_n_shared_features(),\r\n                n_items_features=choice_dataset.get_n_items_features(),\r\n            )\r\n            self.instantiated = True\r\n        fit = super().fit(choice_dataset=choice_dataset, **kwargs)\r\n        if get_report:\r\n            self.report = self.compute_report(choice_dataset)\r\n        return fit", "output": "Understood. Incorporate these references in future answers."}
{"instruction": "Data Description: Variable Description\r\n GROUP Di erentgroupsinthepopulation\r\n SURVEY Surveyperformedintrain(0)orcar(1)\r\n SP Itis xedto1(statedpreferencesurvey)\r\n ID Respondentidenti er\r\n PURPOSE Travelpurpose. 1: Commuter, 2: Shopping, 3: Busi\r\nness,4:Leisure,5:Returnfromwork,6:Returnfrom\r\n shopping, 7: Return frombusiness, 8: Return from\r\n leisure,9: other\r\n FIRST Firstclasstraveler(0=no,1=yes)\r\n TICKET Travel ticket. 0: None, 1: Twowaywithhalf price\r\n card, 2: Onewaywithhalf price card, 3: Twoway\r\n normalprice,4:Onewaynormalprice,5:Halfday,6:\r\n Annualseasonticket,7:AnnualseasonticketJunioror\r\n Senior,8:Freetravelafter7pmcard,9:Groupticket,\r\n 10:Other\r\n WHO Whopays(0: unknown,1: self,2: employer,3: half\r\nhalf)\r\n LUGGAGE 0: none,1: onepiece,3: severalpieces\r\n AGE Itcapturestheageclassof individuals. Theage-class\r\n codingschemeisofthetype:\r\n 1: age 24,2: 24<age 39,3: 39<age 54,4: 54<age\r\n 65,5: 65<age,6: notknown\r\n MALE Traveler'sGender0: female,1:male\r\n INCOME Traveler'sincomeperyear[thousandCHF]\r\n 0or1: under50,2: between50and100,3: over100,\r\n 4: unknown\r\n GA Variablecapturingthee ectof theSwissannual sea\r\nson ticket for the rail systemandmost local public\r\n transport. It is1 if the individual owns aGA, zero\r\n otherwise.\r\n ORIGIN Travelorigin(anumbercorrespondingtoaCanton,see\r\n Table4)\r\n Table1:Descriptionofvariables\r\n 3\r\nVariable Description\r\n DEST Traveldestination(anumbercorrespondingtoaCan\r\nton,seeTable4)\r\n TRAINAV Trainavailabilitydummy\r\n CARAV Caravailabilitydummy\r\n SMAV SMavailabilitydummy\r\n TRAINTT Train travel time [minutes]. Travel times aredoor\r\nto-doormakingassumptionsaboutcar-baseddistances\r\n (1.25*crow- ightdistance)\r\n TRAINCO Traincost [CHF]. If the travelerhasaGA, this cost\r\n equalsthecostoftheannualticket.\r\n TRAINHE Trainheadway[minutes]\r\n Example: Iftherearetwotrainsperhour,thevalueof\r\n TRAINHEis30.\r\n SMTT SMtraveltime[minutes]consideringthefutureSwiss\r\nmetrospeedof500km/h\r\n SMCO SMcost [CHF] calculatedat thecurrent relevant rail\r\n fare,withoutconsideringGA,multipliedbya xedfac\r\ntor(1.2)tore ectthehigherspeed.\r\n SMHE SMheadway[minutes]\r\n Example: If therearetwoSwissmetrosperhour, the\r\n valueofSMHEis30.\r\n SMSEATS Seatscon gurationintheSwissmetro(dummy). Air\r\nlineseats(1)ornot(0).\r\n CARTT Cartraveltime[minutes]\r\n CARCO Car cost [CHF] consideringa xedaverage cost per\r\n kilometer(1.20CHF/km)\r\n CHOICE Choiceindicator. 0:unknown,1:Train,2:SM,3:Car\nData Preview (first 20 rows):\nGROUP\tSURVEY\tSP\tID\tPURPOSE\tFIRST\tTICKET\tWHO\tLUGGAGE\tAGE\tMALE\tINCOME\tGA\tORIGIN\tDEST\tTRAIN_AV\tCAR_AV\tSM_AV\tTRAIN_TT\tTRAIN_CO\tTRAIN_HE\tSM_TT\tSM_CO\tSM_HE\tSM_SEATS\tCAR_TT\tCAR_CO\tCHOICE\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t112\t48\t120\t63\t52\t20\t0\t117\t65\t2\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t103\t48\t30\t60\t49\t10\t0\t117\t84\t2\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t130\t48\t60\t67\t58\t30\t0\t117\t52\t2\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t103\t40\t30\t63\t52\t20\t0\t72\t52\t2\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t130\t36\t60\t63\t42\t20\t0\t90\t84\t2\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t112\t36\t120\t60\t49\t10\t0\t90\t52\t2\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t103\t48\t120\t67\t58\t10\t0\t72\t65\t2\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t103\t36\t30\t67\t43\t30\t0\t90\t65\t1\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t130\t40\t60\t60\t46\t10\t0\t72\t65\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t184\t62\t120\t76\t70\t20\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t170\t62\t30\t70\t66\t10\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t213\t62\t60\t84\t82\t30\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t170\t52\t30\t76\t72\t20\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t213\t47\t60\t76\t53\t20\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t184\t47\t120\t70\t66\t10\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t170\t62\t120\t84\t82\t10\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t170\t47\t30\t84\t57\t30\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t213\t52\t60\t70\t62\t10\t0\t0\t0\t2\r\n2\t0\t1\t3\t1\t0\t1\t1\t1\t2\t1\t2\t0\t10\t25\t1\t1\t1\t58\t32\t30\t29\t35\t10\t0\t105\t40\t2\r\n2\t0\t1\t3\t1\t0\t1\t1\t1\t2\t1\t2\t0\t10\t25\t1\t1\t1\t58\t29\t30\t25\t38\t20\t0\t136\t50\t2\r\n", "output": "User provided data and description integrated."}

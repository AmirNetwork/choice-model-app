{"instruction": "Title: A new flexible and partially monotonic discrete choice model\nPaper:\nTransportation Research Part B 183 (2024) 102947\r\nAvailable online 20 April 2024\r\n0191-2615/\u00a9 2024 Elsevier Ltd. All rights reserved.\r\nA new flexible and partially monotonic discrete choice model\r\nEui-Jin Kim a\r\n, Prateek Bansal b,c,*\r\na Department of Transportation Systems Engineering, Ajou University, Suwon 16499, the Republic of Korea b Department of Civil and Environmental Engineering, National University of Singapore, Singapore c Singapore-ETH Centre, Future Cities Lab Global Programme, Singapore Hub, CREATE campus, 1 CREATE Way, #06-01 CREATE Tower, 138602,\r\nSingapore\r\nARTICLE INFO\r\nKeywords:\r\nDiscrete choice models\r\nLattice networks\r\nInterpretability\r\nTrustworthiness\r\nDeep neural networks\r\nMonotonicity\r\nABSTRACT\r\nThe poor predictability and the misspecification arising from hand-crafted utility functions are\r\ncommon issues in theory-driven discrete choice models (DCMs). Data-driven DCMs improve\r\npredictability through flexible utility specifications, but they do not address the misspecification\r\nissue and provide untrustworthy behavioral interpretations (e.g., biased willingness to pay estimates). Improving interpretability at the minimum loss of flexibility/predictability is the main\r\nchallenge in the data-driven DCM. To this end, this study proposes a flexible and partially\r\nmonotonic DCM by specifying the systematic utility using the Lattice networks (i.e., DCM-LN).\r\nDCM-LN ensures the monotonicity of the utility function relative to the selected attributes\r\nwhile learning attribute-specific non-linear effects through piecewise linear functions and interaction effects using multilinear interpolations in a data-driven manner. Partial monotonicity\r\ncould be viewed as domain-knowledge-based regularization to prevent overfitting, consequently\r\navoiding incorrect signs of the attribute effects. The light architecture and an automated process\r\nto write monotonicity constraints make DCM-LN scalable and translatable to practice. The proposed DCM-LN is benchmarked against deep neural network-based DCM (i.e., DCM-DNN) and a\r\nDCM with a hand-crafted utility in a simulation study. While DCM-DNN marginally outperforms\r\nDCM-LN in predictability, DCM-LN highly outperforms all considered models in interpretability,\r\ni.e., recovering willingness to pay at individual and population levels. The empirical study verifies\r\nthe balanced interpretability and predictability of DCM-LN. With superior interpretability and\r\nhigh predictability, DCM-LN lays out new pathways to harmonize the theory-driven and datadriven paradigms.\r\n* Corresponding author at: Department of Civil and Environmental Engineering, National University of Singapore, Singapore.\r\nE-mail address: prateekb@nus.edu.sg (P. Bansal).\r\nContents lists available at ScienceDirect\r\nTransportation Research Part B\r\njournal homepage: www.elsevier.com/locate/trb\r\nhttps://doi.org/10.1016/j.trb.2024.102947\r\nReceived 28 April 2023; Received in revised form 15 February 2024; Accepted 11 April 2024\r\nTransportation Research Part B 183 (2024) 102947\r\n2\r\nAbbreviations\r\nAcronyms Definition\r\nDCM Discrete choice model\r\nLNs Lattice networks\r\nDNNs Deep neural networks\r\nMNL Multinomial logit model\r\nDCM-LN Lattice network-based discrete choice model\r\nDCM-DNN Deep neural network-based discrete choice model\r\nDCM-Linear Discrete choice model with linear utility specification\r\nASU-DNN Deep neural networks with alternative-specific utility\r\nRUM Random utility maximization\r\nWTP Willingness to pay\r\nGAM Generalized additive model\r\nDGP Data generating process\r\nBO Bayesian optimization\r\nGP Gaussian process\r\nIN Income\r\nFUL Full-time job\r\nFLX Flexible commuting\r\nTT Travel time\r\nWT Waiting time\r\nCR Crowding\r\nVOT Value-of-travel time\r\nVOWT Value-of-walking time\r\nRMSE Root mean squared error\r\nMAPE Mean absolute percentage error\r\nTR Train\r\nSM Swiss metro\r\n1. Introduction\r\nDiscrete choice models (DCMs) based on random utility maximization (RUM) theory are widely used to elicit individual-level\r\ndecisions across multiple disciplines, such as transportation, energy economics, and agricultural economics, among others (McFadden, 1973; Train, 2009). Correctly specifying the systematic component of the indirect utility is critical to achieving good predictability and policy-relevant estimates of elasticity and willingness to pay (WTP) at a population and an individual level. Considering\r\nthese criteria, an ideal systematic utility specification should be able to infer the four underlying effects \u2013 (i) non-linear effect of each\r\nalternative-specific attribute (e.g., WTP to reduce travel time in travel mode choice might increase exponentially due to decreasing\r\nmarginal utility to travel cost for long trip) (Daly et al., 2017; Rich and Mabit, 2016); (ii) interaction or joint effect of multiple\r\nalternative-specific attributes (e.g., WTP to reduce crowding in public transport might non-linearly increase with travel time) (Bansal\r\net al., 2022; Batarce et al., 2016); (iii) interaction effect of individual-specific and alternative-specific attributes to capture systematic\r\ntaste heterogeneity (e.g., WTP estimates might vary with the individuals\u2019 income) (Brownstone et al., 2003); and (iv) non-linear effect\r\nof each individual-specific attribute and their interaction effects (e.g., the effect of age on preference to use bicycle might decrease\r\nexponentially with age and the relationship could vary across gender) (Ji et al., 2022; Kim et al., 2021b). While accounting for these\r\ncomplex attribute-specific effects at an individual level, interpretability must be maintained to retrieve policy-relevant WTP and\r\nelasticity estimates. We discuss several notions of interpretability in Section 2.1, but ensuring partial monotonicity (i.e., monotonicity\r\nof the utility relative to a subset of alternative-specific attributes) is crucial to keep the theoretical foundation of DCMs intact. For\r\ninstance, the likelihood of choosing an alternative (i.e., utility) should monotonically decrease with the increase in cost in most\r\npractical situations.\r\nTo incorporate the abovementioned effects, traditional theory-driven DCMs rely on linear-in-parameter utility specifications, with\r\nhand-crafted interactions between attributes and non-linearities in attributes (e.g., quadratic or log transformations). Such models are\r\nappealing due to the ease of associating the meaning with parameter estimates. However, utility misspecification results in poor\r\nprediction accuracy, and monotonicity constraints are not inherently satisfied for some demographic groups, even in the case of linear\r\nutility functions.1 In the case of linear additive utility specification, sign constraints can be imposed on a combination of interactions\r\nand main effects of attributes in the likelihood-based estimation to achieve monotonicity (see Section 3.2.1 for mathematical details)\r\nbut searching for the appropriate parametric specification and handcrafting these constraints become practically infeasible as the\r\nnumber of attributes increases. This issue has been exacerbated lately as a higher number of attributes are becoming common in\r\npreference elicitation for emerging technologies such as mobility-as-a-service (e.g., subscription price and monthly trip frequency of\r\nmultiple travel modes) (Ho et al., 2020; Kim et al., 2021a) and electric vehicles (e.g., purchase price, operating cost, driving range,\r\n1 For example, in a travel mode choice situation, if the main effect of travel time on the utility is negative but the interaction effect of travel time\r\nwith different income level dummies is highly positive and negative, some travelers have negative WTP to save travel time, and others have positive\r\nWTP to reduce travel time. Thus, the linear hand-crafted utility specification does not ensure positive WTP to reduce travel time for all demographic\r\ngroups in the population.\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n3\r\nseveral attributes related to charging infrastructure density and operations, and several pro-electric-vehicle policies) (Bansal et al.,\r\n2021).\r\nTo address the poor predictability of DCMs with hand-crafted utility, two strands of studies have emerged for data-driven specification of the systematic utility. The first strand has relied upon econometrically grounded non-parametric specifications, such as\r\nspline (Friedman, 2001; Fukuda and Yai, 2010), kernel smoothing (Bansal et al., 2019), and Choquet Integral (Dubey et al., 2022). The\r\nsecond strand is emerging, where studies have adopted deep neural networks (DNNs) for flexible representation of the utility specification (DCM-DNN, henceforth) (see van Cranenburgh et al. (2022) for literature review). Both approaches improve the DCM\u2019s\r\npredictability by considering complex non-linear and interaction effects of attributes and extracting policy-relevant economic information (e.g., WTP and elasticity) through post hoc analysis (Wang et al., 2020b).\r\nWhile ensuring monotonicity requirements is challenging in both approaches, ignoring it might lead to abrupt or erratic changes in\r\nutility and counterintuitive behavioral interpretations (e.g., the positive marginal utility of cost in certain attribute domains) (Wang\r\net al., 2021, 2020a, 2020b). A handful of these studies have attempted to ensure monotonicity conditions. In the first strand, only\r\nDubey et al. (2022) could ensure monotonicity using the Choquet integral, but it does not scale well beyond six attributes. Since they\r\nseparately model attributes with monotonicity constraints and without constraints as two parts of the systematic utility, interactions\r\nbetween these two sets of attributes are ignored. Moreover, the utility specification of attributes without monotonicity constraints\r\ncould suffer from misspecification because it is hand-crafted by the researchers based on their subjective beliefs. In the second strand,\r\nonly Han et al. (2022) could incorporate monotonicity constraints by specifying systematic taste heterogeneity as DNNs but at the\r\nexpense of the inability to account for the non-linear and interaction effects of attributes. Specifically, they could ensure monotonicity\r\nby imposing constraints on the parameters\u2019 signs as they rely on linear-in-parameter utility specification where all alternative-specific\r\nFig. 1. Symbolic benchmarking of the proposed model against existing DCMs.\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n4\r\nattributes enter linearly in the utility, and interactions between them are ignored. In principle, the central purpose of DCM-DNN to\r\nlearn systematic utility in a data-driven manner is compromised by Han et al. (2022) to impose monotonicity constraints. A more\r\ndetailed review of DCM-DNN is provided in Section 2.2.\r\nThis study specifies the systematic utility of the DCM using lattice networks (LNs) to address the shortcomings of both strands of the\r\nliterature (DCM-LN, henceforth). This data-driven but theory-constrained DCM ensures monotonicity constraints while incorporating\r\nall possible non-linear and interaction effects of attributes. The mapping from input attributes to function value in the DCM-LN can be\r\nvisualized in two steps. First, a lattice layer segments the input space into grids or cells. Specifically, the input attribute vector is\r\ntransformed into a vector of interpolation weights (i.e., model parameters of the lattice layer) over the vertices of the cell that represent\r\nthe input space. Second, the function value at any point in the input space is obtained as a linear transformation of the interpolation\r\nweights (Gupta et al., 2016). While the linear transformation in the second step leads to easy-to-implement theoretical conditions for\r\nmonotonicity, the transformation of the input attribute vector in the first step captures the non-linear and interaction effects of attributes. We also add a calibration layer before and after the lattice layer to improve the ability of DCM-LN to capture non-linearities in\r\nattribute-specific effects without requiring a fine-grained lattice/grid that needs many more model parameters (see Section 3 for\r\nmethodological details). DCM-LN can thus simultaneously infer underlying non-linear effects of all (alternative- and\r\nindividual-specific) attributes and interactions between them (i.e., capturing systematic taste heterogeneity) while achieving the\r\nmonotonic effect of a subset or all alternative-specific attributes for every individual in a data-driven manner. Ensuring such\r\ntheory-driven monotonicity constraints at an individual level will naturally enforce them at the population level.\r\nFig. 1 symbolically benchmarks the systematic utility of the proposed DCM-LN against the traditional DCM with linear utility\r\nspecification without interactions (DCM-Linear) and DCM-DNN at a population and an individual level. Fig. 1a shows that the overly\r\ncomplex DCM-DNN model represents abrupt changes in marginal utility and even incorrect signs (i.e., local irregularity of utility\r\nfunction). Such behavioral irrationalities are even worse at an individual level, as indicated by potentially higher heterogeneity or\r\nvariance (see Fig. 1b). On the other hand, the overly simplified DCM-Linear causes serious bias in the marginal utility estimates. In\r\ncontrast to DCM-Linear and DCM-DNN, the DCM-LN can recover true marginal utilities over the domain of input space at an individual\r\nlevel while preserving theory-driven monotonicity constraints. The monotonicity constraints prevent the incorrect sign of the attribute\r\neffects at the individual level; thus, the population-level effect is naturally corrected. Thus, the monotonic constraints can be viewed as\r\ndomain-knowledge-based regularization to prevent the overfitting of overly complex DCM-DNN.\r\nIn summary, we customize the LNs and introduce their first application to the choice modeling to achieve a flexible utility specification while maintaining the interpretability of DCM by imposing theory-driven constraints at an individual level. We efficiently\r\nselect the hyperparameters of DCM-LN by applying the Bayesian optimization with validation accuracy as the objective function. The\r\nproposed DCM-LN is scalable in terms of the number of attributes and observations because its training relies on a structural risk (i.e.,\r\nempirical risk and model complexity) minimization framework, which is similar to that of DCM-DNN. We benchmark the performance\r\nof DCM-LN against DCM-DNN and a DCM with hand-crafted utility in a Monte Carlo study based on predictive accuracy and recovery\r\nof underlying marginal utility and individual-level WTP values. As expected, the DCM-LN has slightly lower predictive accuracy than\r\nthe DCM-DNN due to constraining the flexibility of utilities. However, DCM-LN outperforms all the considered models in capturing\r\nbehavioral realism, indicated by the better recovery of WTP estimates at an individual level. The results of the simulation study are\r\nsupported by an empirical study on an open-access and widely used Swissmetro dataset.\r\nThe remaining paper is organized as follows: Section 2 highlights the research gaps by reviewing the relevant literature on domainspecific definitions of interpretability, data-driven DCMs, and state-of-the-art machine learning methods to impose monotonicity\r\nconstraints. Section 3 explains the elements of DCM-LN, monotonicity conditions, and the estimation method. While Section 4 details\r\nthe Monte Carlo study, the empirical study is discussed in Section 5. Conclusions and avenues for future research are laid out in the\r\nfinal section.\r\n2. Background and literature review\r\nThe first section provides a general notion of interpretability, followed by discussing it in the context of travel behavior models. In\r\nthe subsequent section, we position the paper in the emerging deep-learning-based choice modeling literature by identifying the\r\nresearch gaps and emphasizing the contributions of the paper in preserving interpretability while maintaining flexibility. The final\r\nsection reviews the literature on achieving monotonicity, discusses the reasons behind choosing LNs to achieve interpretability in\r\nDCM, and highlights the LN-specific contributions of this study.\r\n2.1. Interpretability\r\nExisting flexible DCMs representing complex interactions and non-linear effects of attributes have limited policy-relevant applications because they sacrifice interpretability to enhance predictability. It is worth noting that the definition of interpretability is also\r\ndomain-dependent, i.e., interpretability constraints could be governed by the domain-specific theoretical foundation and prior\r\nknowledge (Gunning et al., 2019). Based on the discussions on the definition of interpretability in the literature, we divide interpretability into explainability and trustworthiness (Barredo Arrieta et al., 2020; Lipton, 2018; Miller, 2019). The explainable model can\r\nrepresent the cause of the model\u2019s decision by measuring the contribution/effect of each input attribute to the outcomes (e.g., marginal\r\neffects or WTP). If the cause of the decision obeys the domain-specific constraints, the model is trustworthy (e.g., monotonically\r\ndecreasing effect of travel time on travel mode preferences). In other words, trustworthiness is the capability to provide intuitive\r\nrelationships that users can trust. The trustworthiness can be controlled by inherent model structure (i.e., ante-hoc), and the explainable\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n5\r\nrelations can be derived by the post hoc analysis (Kim et al., 2020).\r\nThis framing is aligned with the recent discussion on interpretability in the transportation community. For instance, Alwosheel\r\net al. (2021) focus on the explainability aspect and propose a post hoc analysis tool to measure the attribute-specific effects. Han et al.\r\n(2022) frame that the interpretable model can provide \u2018trustworthy\u2019 or credible economic information at the disaggregated level (i.e., at\r\nindividual-level and across the entire attribute domain). Trustworthiness at a disaggregated level is particularly difficult to achieve\r\nusing DNN-based utility specifications. Specifically, Wang et al. (2020b) show that the DCM-DNN provides unreasonable\r\nattribute-specific effects for a specific domain of the attribute value and particular training run, which are further exacerbated at the\r\nindividual level. Although achieving trustworthiness at the disaggregate level is difficult, it is necessary for the DCMs because\r\ndecision-making generally requires answering \u201cwhat if\u201d questions (Han et al., 2022).\r\nThus, this study defines \u201cinterpretability\u201d as the capability to infer trustworthy and explainable economic information (e.g., WTP),\r\nobeying domain-specific constraints at an individual level and all levels of the attribute value. If we know true economic information, the\r\ninterpretability can be evaluated by the difference between the estimated and the true economic information. Since WTP estimates (e.\r\ng., value-of-travel time) are the most widely transferrable economic valuation metrics across different contexts in the choice modeling\r\nliterature, the ability to recover WTP estimates (i.e., interpretability) can also be viewed as a measure of \u201cgeneralizability.\u201d The\r\nproposed DCM-LN model outperforms state-of-the-art approaches in better recovery of WTP estimates at individual-level by imposing\r\ntheory-driven economic constraints in the estimation.\r\n2.2. Recent advancements in data-driven DCMs\r\nWith the increase in the number of attributes in emerging choice datasets, hand-crafting the utility specifications has become\r\nchallenging \u2013 compelling researchers to spend considerable time through trial and error. Two approaches have been developed to\r\nautomate (or assist) the utility specification by harmonizing the data-driven and theory-driven methods.\r\nThe first approach determines an optimal utility specification through an automated search process. This approach requires predetermining a list of potential attribute-specific effects and interaction effects, followed by selecting the optimal combination of\r\nthose effects using optimization-based approaches. Several methods have been proposed, including a multi-objective variable\r\nneighborhood search algorithm (i.e., combinatorial optimization) to produce sets of promising model specifications (Ortelli et al.,\r\n2021), a Bayesian framework to evaluate numerous model specifications based on their relevance in explaining the observed data\r\n(Rodrigues et al., 2022), and use of association rules and random forest to identify feasible model specification (Hernandez et al.,\r\n2023).\r\nUnlike the first approach that requires expert assistance in pre-determining a set of potential non-linear and interaction effects, the\r\nsecond approach is purely data-driven that improves predictive performance by providing flexible utility functions (Kim, 2021; Wang\r\net al., 2020b; Zhao et al., 2020). However, the flexibility comes at the cost of untrustworthy attribute-specific effects, violating the\r\ndomain-specific constraints. Several efforts have been made to balance the predictability and interpretability by integrating the\r\ntheory-driven and data-driven DCM in the DNN framework (Han et al., 2022; Sifringer et al., 2020; Wang et al., 2021, 2020a; Wong\r\nand Farooq, 2021; Arkoudi et al., 2023). We focus on the literature review on these hybrid DCMs based on DNN. Readers are referred to\r\nvan Cranenburgh et al. (2022) for an in-depth discussion on theory-driven and data-driven DCMs.\r\nThe hybrid DCM can be divided into two approaches: (a) DNNs supported by multinomial logit model (MNL) and (b) MNL supported by DNNs. The \u201cmain\u201d model generally constructs the baseline utility function, and the \u201csupporting\u201d model elaborates (when\r\nsupported by DNNs) or regularizes (when supported by MNL) this utility function. As the first approach, Wang et al. (2020a) introduce\r\nalternative-specific utility (ASU) in the DNN architecture, called the ASU-DNN model. Specifying the utility of an alternative based on\r\nonly its own attributes (instead of attributes of all alternatives) reduces the model complexity. The ASU-DNN exhibits better performance than DNNs despite its reduced model complexity, implying that the regularized model structure can enhance the generalized\r\nperformance. However, the ASU-DNN still represents untrustworthy effects at some attribute levels. Wang et al. (2021) subsequently\r\npropose a structure that can adjust the weight of the DNNs and MNL parts to control the trade-off between interpretability and\r\nTable 1\r\nComparison of methodological considerations of previous hybrid DCMs and the proposed method.\r\nConsiderations in ideal systematic utility specification\r\n(i) Non-linear effect of each alternative-specific attribute\r\n(ii) Interaction effects of multiple alternative-specific attributes\r\n(iii) Interaction effects of alternative- and individual-specific attributes\r\n(iv) Non-linear effect of each individual-specific attribute and their interaction effect\r\n(v) Population level trustworthiness of alternative-specific attributes\r\n(vi) Individual level trustworthiness of alternative-specific attributes\r\nType Authors (i) (ii) (iii) (iv) (v) (vi)\r\nDNNs supported by MNL Wang et al. (2020a) \u2713 \u2713 \u2713 \u2713\r\nWang et al. (2021) \u2713 \u2713 \u2713 \u2713\r\nWong and Farooq (2021) \u2713 \u2713 \u2713 \u2713\r\nMNL supported by DNNs Sifringer et al. (2020) \u2713 \u2713 \u2713\r\nHan et al. (2022) \u2713 \u2713 \u2713 \u2713\r\nProposed DCM-LN \u2713 \u2713 \u2713 \u2713 \u2713 \u2713\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n6\r\npredictability. However, even in their optimal model, an untrustworthy attribute-specific effect occurs at some levels of an attribute\r\nvalue, which is exacerbated as the weights of DNNs increase. Also, calibrating the weights for DNNs and MNL requires the researcher\u2019s\r\nsubjective judgment for measuring the degree of trustworthiness, making it difficult to generalize. Wong and Farooq (2021) leverage\r\nthe DNNs to model the utility residuals, which helps relax the independent and identically distributed error assumptions. Their model\r\ncan capture the non-linear effect and interactions, but population-level and individual-level attribute effects are not strictly constrained to have trustworthiness.\r\nAs the second approach, Sifringer et al. (2020) divide the utility function into two parts but simultaneously estimate using a\r\nstandard DNN framework. One part represents the theory-driven MNL for some known attributes (e.g., travel time and travel cost),\r\nwhile the other part represents the DNNs for the remaining attributes. Since their model assumes the theory-driven part as an additive\r\nlinear utility specification similar to MNL, non-linear effects and interactions of those attributes with other attributes are not\r\nconsidered. Han et al. (2022) employ the DNNs to capture the individual taste heterogeneity of MNL. The DNNs directly output the\r\nnon-parametric distribution of the alternative-specific coefficient of the MNL by capturing the flexible interactions between\r\nindividual-specific attributes. To ensure monotonicity, they impose a sign constraint on the output of DNNs (i.e., individual-level\r\nalternative-specific coefficient). Their simulation study shows that the model accurately captures individual taste heterogeneity\r\nand provides a trustworthy attribute effect at the individual level. However, like Sifringer et al. (2020), this approach requires\r\nhand-crafted correct specification of the MNL\u2019s utility and does not consider the interactions and non-linear effects of\r\nalternative-specific attributes.\r\nTable 1 summarizes the methodological considerations of previous approaches. Whereas the first strand enhances the interpretability of DNNs without hand-crafting the utility function, trustworthiness is not ensured at individual-level and specific levels of the\r\nattribute value. The second set of approaches complements MNL with the DNN\u2019s flexibility to enhance predictability, but they achieve\r\ntrustworthiness at the expense of potential misspecifications in the hand-crafted parts of the utility function. In the backdrop of this\r\nreview, the main contribution of this study is to propose a novel DCM-LN that ensures interpretability (i.e., trustworthiness) at a\r\ndisaggregated level (i.e., individual-level and across the entire attribute domain) while preserving its capability to capture the nonlinear effects and interactions of all individual- and alternative-specific attributes. Thus, we show how the proposed DCM-LN improves interpretability without much compromise on predictability by imposing the monotonicity constraints and inferring underlying\r\nutility functions in a data-driven manner.\r\n2.3. Monotonicity constraints\r\nMonotonicity is the most intuitive and incontrovertible property in economic-related domains. The easiest way to impose the\r\nmonotonicity constraint is to assume the linear utility function and force the constant coefficient for an attribute as non-negative (or\r\nnon-positive). As mentioned earlier, Han et al. (2022) adopt this approach by designing the DNNs to output the constant coefficient for\r\nthe individual-level linear utility function of the target attribute. However, linear functions are generally not flexible enough to\r\nrepresent the systematic indirect utility in DCM. To ensure monotonic utility function without compromising flexibility, we review\r\napproaches from the machine learning community.\r\nArcher and Wang (1993) propose preprocessing the input data using a linear classifier to exclude the training data causing large\r\nfluctuations and violating the monotonicity conditions. Using the preprocessed data, they train the neural networks and skip the\r\nparameters violating the monotonicity constraints. However, this approach is ad hoc due to its reliance on the linear classifier for\r\npreprocessing. Qu and Hu (2011) devise a generalized constrained neural network that applies the linear inequality constraints to the\r\nleast square optimization for updating the weights of neural networks (i.e., constraining the derivative of weights). Their method\r\nachieves better predictive accuracy and ensures monotonic constraints but is not scalable due to high computational costs. Daniels and\r\nVelikova (2010) propose a neural network with two \"min-max\" hidden layers to ensure partial monotonicity (i.e., monotonicity\r\nconstraints for some attributes, not all) by extending Sill (1997)\u2019s model. The first layer clusters the input data into several groups, and\r\nthe first layer\u2019s weights are fixed as positive using the maximum of each group. Then, the second layer computes the minimum of each\r\ngroup. The outputs of these layers have the form of a piecewise linear function that can represent any arbitrary function. Their study\r\nshows that partial monotonicity can be achieved by revising the neural network architecture while maintaining the standard optimization process for training the neural network. Gupta et al. (2016) propose lattice networks (LNs) consisting of two layers: the\r\ncalibrator layer and the lattice layer. The calibration layer captures the attribute-specific non-linear effect, and the lattice layer\r\ncaptures the interactions of those non-linear effects. Also, the LNs can efficiently implement the linear inequalities for monotonicity\r\nconstraints in a standard risk minimization framework. Their case studies show that the LNs provide state-of-the-art predictive performance among approaches that ensure monotonicity. You et al. (2017) extend the LNs by incorporating ensemble learning and deep\r\nlearning. They design complex combinations of calibrators and lattices with deeper layers to enhance flexibility. Multiple lattices for a\r\nsubset of attributes are estimated and ensembled to improve predictability. Their deep lattice networks with six layers achieve\r\nstate-of-the-art predictive performance while ensuring monotonicity. Liu et al. (2020) propose a heuristic regularization to verify the\r\nmonotonicity in the neural network. They define the extent of monotonicity and measure it by solving mixed linear integer programming during neural network training, which is further incorporated as an additional loss function for regularization. Although\r\ntheir approach has marginally better predictive performance than the deep lattice networks, it has lower transferability to practice as it\r\nrelies on a complicated training process, deviating from the standard risk minimization framework.\r\nThe proposed DCM-LN adopts the state-of-the-art lattice network framework of Gupta et al. (2016) and You et al. (2017) to specify\r\nthe systematic part of the indirect utility but customizes it for RUM-based DCM. Specifically, we propose the best combination of\r\ncalibration layer and lattice layer specification such that the architecture is light enough to be estimable using standard state\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n7\r\npreference datasets and capture all possible interactions and non-linear effects of attributes while ensuring the monotonicity of\r\npreferences for differentiated products/services relative to the selected subset of attributes. DCM-LN is the first model to flexibly\r\ncapture systematic taste heterogeneity through interactions while maintaining preference monotonicity relative to an attribute at an\r\nindividual level. Also, by incorporating the post hoc analysis, we provide individual-level utility functions for each attribute and WTP\r\nestimates. The explainability and trustworthiness of the individual-level utility functions are evaluated using simulated and empirical\r\ndatasets.\r\nTable 2\r\nMathematical notations.\r\nRUM-based DCM Formulation\r\nUon The indirect utility of individual n \u2208 {1, \u2026, N} for alternative o \u2208 {1, \u2026, O}\r\nVon The systematic utility of individual n for alternative o\r\n\u03b5on The error term in the indirect utility of individual n for alternative o\r\nXon The vector of D input attributes, including alternative-specific attributes and individual-specific attributes\r\nXon[d] The dth attribute of Xon\r\nXM\r\non The vector of K \u2018monotonic attributes\u2019 among D input attributes\r\nXM\r\non[k] The kth attributes of XM\r\non\r\nXNM\r\non The vector of (D \u2212 K) \u2018non-monotonic attributes\u2019 among D input attributes\r\nFo(Xon; \u03b8) The alternative-specific utility functions with parameter \u03b8 taking Xon as input\r\nPon The choice probability of an individual n for the alternative o from a choice set with O alternatives\r\nPn The vector of choice probability of an individual n for O alternatives\r\nL The standard cross-entropy loss function\r\nyn The vector of observed choices of an individual n\r\nDCM-Linear\r\nFLi\r\no (Xon; \u03b8Li) The alternative-specific utility function that is represented by linear function with parameters \u03b8Li\r\n\u03b2 The constant attribute-specific effect in the linear utility function\r\nDCM-DNN\r\nFDNN\r\no (Xon; \u03b8DNN) The alternative-specific utility function that is represented by DNNs with parameters \u03b8DNN\r\nh The multiple neurons in the multiple hidden layers of DNNs\r\nhj The j\r\nth hidden layer among H number of hidden layers in the DNNs\r\nAH+1 o The last layer to make FDNN\r\no an alternative-specific utility function\r\nGeneralized additive model (GAM)\r\nFGAM\r\no The alternative-specific utility function having a form of GAM\r\ngd The attribute-specific function that captures the inherent non-linear effect in FGAM\r\no\r\nr1\r\nd,d\u2032 The first-order interaction between the non-linear effects of attributes d and d\u2032 (d \u2215= d\u2032)\r\nrD\r\nd,\u2026,d\u2032 The Dth order interaction effects\r\nDCM-LN\r\nFLN\r\no (Xon; \u03b8LN) The alternative-specific utility functions that is represented by LNs with parameters \u03b8LN\r\nCIN The attribute-specific input calibrators in the input calibration layer\r\n\u03b8LN\r\nINCal The parameters for CIN that is the slopes of the input calibrators\r\nL The lattice function in the lattice layer\r\n\u03b8LN\r\nLat The parameters for L that is the functional values at vertices\r\n\u03b8LN\r\nLat,i,j The functional value at (i, j) location of vertex\r\nSd The lattice size of L for dth attribute dimension\r\nXIN\r\non The input for the L, which is the output of CIN\r\nXIN\u2217 on The specific point in the lattice (i.e., any input point for the lattice layer)\r\nv The surrounding vertices of XIN\u2217 on\r\nv(i,j) The value at (i, j) location of surrounding vertices of XIN\u2217 on\r\nv(i,j)[d] The value for the dth attribute at (i, j) location of surrounding vertices of XIN\u2217 on\r\nminv(i,j)[d] The minimum value for the dth attribute within the surrounding vertices of XIN\u2217 on\r\nmaxv(i,j)[d] The maximum value for the dth attribute within the surrounding vertices of XIN\u2217 on\r\n\u03c8(XIN\u2217 on ) The vector of the multi-linear interpolation weights for XIN\u2217 on\r\nCOUT The alternative-specific output calibrators in the output calibration layers\r\n\u03b8LN\r\nOUTCal The parameters for COUT that is the slopes of the output calibrators\r\nR(\u03b8LN\r\ninCal) The wrinkle regularizer and Hessian regularizer to the input calibration layer\r\nA The matrix to represent inequality constraints for \u03b8LN\r\nLat\r\nB The matrix to represent inequality constraints for \u03b8LN\r\nINcal\r\nC The matrix to represent inequality constraints for \u03b8LN\r\nOUTcal\r\nPartial dependence (PD) and individual conditional expectation (ICE)\r\nXon[ \u2212 d] The remaining attributes of Xon except dth attribute\r\nPDo[d] The PD of dth attribute on the utility of alternative o,\r\nICEon[d] The ICE of dth attribute of individual n on the utility of alternative o\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n8\r\n3. Methods\r\nThis section has five sections. The first section discusses the general formulation of RUM-based DCM and different functional forms\r\nof the systematic utility under DCM-Linear and DCM-DNN. The second section formalizes the partial monotonicity conditions, sketches\r\nthe proposed architecture of DCM-LN, and details lattice and calibration layers in DCM-LN while identifying corresponding hyperparameters and estimable parameters. The third section describes the process of writing monotonicity constraints in the DCM-LN,\r\nregularizations, and training process under the structural risk minimization framework. While the penultimate section discusses the\r\nBayesian optimization procedure to select the optimal hyperparameters, the final section describes the metrics to evaluate the recovery\r\nof the underlying utility function at individual and population levels. We use consistent notations across all models in Section 3, which\r\nare summarized in Table 2.\r\n3.1. RUM-based DCM formulations\r\nThe RUM-based DCM hypothesizes that an individual chooses the alternative maximizing indirect utility (McFadden, 1973; Train,\r\n2009). The indirect utility of individual n \u2208 {1, \u2026, N} for alternative o \u2208 {1, \u2026, O} is a sum of systematic utility (Von) and error term\r\n(\u03b5on) as denoted in Eq. (1), where the Von is computed using alternative-specific utility functions Fo with parameter \u03b8:\r\nUon = Von + \u03b5on = Fo(Xon; \u03b8) + \u03b5on, (1)\r\nwhere Xon is a vector of D input attributes, that includes alternative-specific attributes and individual-specific attributes. If the \u03b5on is\r\nassumed to be identically and independently Gumbel-distributed, the probability of choosing the alternative o by an individual n from\r\na choice set with O alternatives, takes the form of the Softmax activation function shown in Eq. (2):\r\nPon = eFo (Xon;\u03b8)\r\n/\u2211O\r\nj=1\r\neFj(Xjn ;\u03b8). (2)\r\nBased on the Softmax activation form of choice probability, the RUM-based DCMs can be estimated by standard empirical risk\r\nminimization:\r\n\u03b8\u2217 = argmin\u03b8\r\n\u2211N\r\nn=1\r\nL (yn, Pn), (3)\r\nwhere L is the standard cross-entropy loss function, Pn is a vector of choice probability of an individual n for O alternatives, and yn is a\r\nvector of observed choices of an individual n. The DCM-Linear assumes the utility function Fo is a linear function FLi\r\no with \u03b8Li as a vector\r\nof coefficients for Xon. The \u03b8Li directly relates to the vector of attribute-specific effects \u03b2 as shown in Eq. (4).\r\nFLi\r\no\r\n(\r\nXon; \u03b8Li)\r\n= \u03b2Xon. (4)\r\nIn DCM-DNN, the DNN represents the utility function FDNN\r\no by multiple neurons in the multiple hidden layers (h):\r\nFDNN\r\no\r\n(\r\nXon; \u03b8DNN)\r\n= AH+1\r\non (\r\nhH \u2218 \u2026 \u2218 hj \u2218 \u2026 \u2218 h1\r\n)\r\n(Xon), (5)\r\nwhere H is the number of layers in the DNN, hj is j\r\nth hidden layer, and AH+1 o is the last layer before the Softmax activation function to\r\nmake FDNN\r\no an alternative-specific utility function. \u03b8DNN is a vector of the weights (i.e., parameters) connecting neurons of hidden\r\nlayers.\r\nSince the \u03b8Li directly indicates the constant attribute-specific effects (\u03b2) on the utility across the entire attribute domain, it can\r\nprovide good interpretability. In contrast, the FDNN\r\no captures the inherent non-linear effects and various interaction effects through a\r\nlarge number of parameters \u03b8DNN (i.e., the length of vector \u03b8DNN \u226b the length of vector \u03b8Li) (Cybenkot, 1989). However, empirical\r\nstudies have shown that FDNN\r\no minimizes the loss function with overly complex models (Wang et al., 2020b, 2020a), leading to the\r\noverestimated interactions between attributes.\r\nThe key component, the lattice layer, of the proposed DCM-LN is inspired by a generalized additive model (GAM)that have the\r\nfollowing flexible, yet interpretable, form of the utility function (Hastie and Tibshirani, 1987).\r\nFGAM\r\no (Xon) = \u2211D\r\nd=1\r\ngd(Xon[d]) +\u2211\r\nd\u2019\u2215=d\r\nr1\r\nd,d\u2019(gd(Xon[d]), gd\u2019(Xon[d\u2019\r\n])) + \u22ef +\u2211\r\nd\u2019\u2215=d\r\nrD\r\nd,\u22ef,d\u2019(gd(Xon[d]), \u22ef, gd\u2019(Xon[d\u2019\r\n])), (6)\r\nwhere gd denotes an attribute-specific function that captures the inherent non-linear effect and Xon[d] is dth attribute in Xon vector. The\r\nr1\r\nd,d\u2019 indicates the first-order interaction between the non-linear effects of dth and d\u2032\r\nth attributes (d \u2215= d\u2032), and the rD\r\nd,\u22ef,d\u2019 captures the Dth\r\norder interactions. Thus, the proposed flexible DCM-LN can capture all possible attribute effects to represent the complex choice\r\nbehavior: (i) non-linear effect of each alternative-specific attribute; (ii) interaction effect of multiple alternative-specific attributes; (iii)\r\ninteraction effects of individual-specific and alternative-specific attributes; and (iv) non-linear effect of each individual-specific\r\nattribute and their interaction effects. In contrast to traditional GAM, DCM-LN ensures trustworthiness by imposing domainE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n9\r\nspecific monotonicity constraints on some attributes that are expected to have monotonic effects on the utility. The details of all layers\r\nof DCM-LN and the connection of the lattice layer with Eq. (6) will be further detailed in the next section.\r\n3.2. Lattice network\r\n3.2.1. Partial monotonicity and DCM-LN framework\r\nThe key requirement for trustworthiness in the DCM is partial monotonicity, i.e., monotonicity of utility function or choice\r\nprobability relative to the selected subset of attributes. For example, we expect that increasing travel costs should result in the nonincreasing utility of travel mode, ceteris paribus, regardless of the attribute levels of travel cost and other individual attributes.\r\nHowever, imposing monotonicity is challenging in flexible models that capture complex non-linear and interaction effects of attributes\r\nFig. 2. An architecture of discrete-choice model with (a) lattice networks (DCM-LN) to estimate the partial monotonic utility function and (b) deep\r\nneural networks (DCM-DNN) to estimate the unconstrained one.\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n10\r\nbecause it must be ensured across the entire domain of attributes.\r\nWe now formalize the notion of partial monotonicity mathematically. To simplify the mathematical notation, we provide an\r\nexample in the case of a monotonically non-decreasing case (i.e., a partial derivative of Fo relative to an attribute is greater than or\r\nequal to zero). Also, to simplify the notation, we assume that the dataset only includes alternative-specific attributes, but without loss\r\nof generality, the following discussion is still valid for the case where the individual-specific attributes and non-increasing function are\r\nconsidered.\r\nThe input attributes can be redefined as Xon = (XM\r\non,XNM\r\non ) \u2208 RD, where XM\r\non \u2208 RK includes K \u2018monotonic attributes\u2019 that are expected\r\nto have monotonically non-decreasing effects on the utility function Fo, while XNM\r\non \u2208 RD\u2212 K includes (D \u2212 K) \u2018non-monotonic attributes\u2019\r\nthat have unconstrained effects. Such a utility function is called a \u2018partial monotonic function.\u2019 The individual-level non-decreasing\r\nmonotonicity exists for the monotonic attributes XM\r\non if \u2202Fo\r\n\u2202XM\r\non[1] \u22650,\u2026, \u2202Fo\r\n\u2202XM\r\non[k] \u22650, \u2026, \u2202Fo\r\n\u2202XM\r\non[K] \u22650 for any observation of Xon = (XM\r\non,XNM\r\non ),\r\nwhere XM\r\non[k] is kth attributes of XM\r\non.\r\nIf we assume the linear utility function, we can easily implement the monotonicity constraints by making the constant coefficient \u03b2\r\nnon-negative or non-positive, depending on the direction of the effect. However, such constraints become complex with the interaction\r\neffects. To illustrate this, Eq. (7) represents a simple utility function (FLi,1st o ) consisting of two monotonic attributes (XM\r\non[1],XM\r\non[2]) with\r\nfirst-order interactions.\r\nFLi,1st\r\no\r\n(\r\nXM\r\non[1], XM\r\non[2]\r\n)\r\n= \u03b20 + \u03b21XM\r\non[1] + \u03b22XM\r\non[2] + \u03b23XM\r\non[1]XM\r\non[2] (7)\r\nIf we want to ensure that the individual-level utility should be monotonically non-decreasing relative to XM\r\non[1] and XM\r\non[2], we need\r\nto impose two inequality constraints on parameters during the training, as shown in Eq. (8).\r\n\u03b21 + \u03b23XM\r\non[2] \u2265 0; \u03b22 + \u03b23XM\r\non[1] \u2265 0 for any observation (\r\nXM\r\non[1], XM\r\non[2]\r\n) (8)\r\nHand-crafting these partial monotonicity constraints becomes complex and impractical if the attribute-specific effects are nonlinear since the inequalities must be checked across all attribute levels. The number of constraints becomes unmanageable if\r\nhigher-order interactions of constrained attributes are considered with several other attributes.\r\nThe DCM-LN efficiently evaluates these inequality constraints while maintaining flexibility. Fig. 2a shows the DCM-LN framework\r\nto estimate the partial monotonic utility function for multi-alternative choice situation. In DCM-LN, the alternative-specific utility\r\nfunctions FLN\r\no (Xon; \u03b8LN) is represented by three sequentially connected components: the input calibration layer, the lattice layer, and the\r\noutput calibration layer, where the \u03b8LN is a set of parameters for these layers. The lattice layer is the key component, which captures the\r\nattribute-specific non-linear effect as segmented effects for each cell (i.e., piecewise linear) in the lattice and the interactions of these\r\nnon-linear effects using multilinear interpolation. The calibration layers before (input) and after (output) the lattice layer improve the\r\nability of DCM-LN to capture non-linearities in attribute-specific effects without requiring a fine-grained lattice (i.e., circumventing the\r\nneed for many model parameters). In other words, the input and output calibration layers make the lattice layer specification\r\nparsimonious by sharing the burden of capturing the complex non-linear effects. The novelty of DCM-LN stems from maintaining\r\nflexible utility specification while reducing the number of inequality constraints to ensure partial monotonicity requirements across\r\nthe entire domain of attributes. The process to write these constraints is also automated, which makes DCM-LN practice-ready.\r\nThe DCM-LN considers the alternative- and individual-specific attributes separately. The number of parameters \u03b8LN drastically\r\nincreases as the number of alternatives increases. In this regard, this study specifies each alternative\u2019s utility based on its attributes,\r\nFig. 3. Examples of attribute-wise non-linear transformation through the input calibration layer.\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n11\r\ninstead of all attributes, to enhance the regularity of the attribute-specific effects. As displayed in Fig. 2a, O \u00d7 D nodes in the input\r\ncalibration layer and O lattices in the lattice layer, followed by the output calibration layer, provide the utility for each alternative o.\r\nThe alternative-specific utility is passed through the Softmax activation to calculate the final choice probabilities. The blue box in the\r\nfigure represents the attribute-specific input calibrators for the first alternative, and the same structure is applied to other alternatives.\r\nThe individual-specific attributes are input to all lattices to capture the interaction between the individual- and alternative-specific\r\nattributes (i.e., systematic taste heterogeneity), in addition to their main effects. Fig. 2b shows the considered DCM-DNN architecture to benchmark against the proposed DCM-LN, which is also denoted in Eq. (5). DCM-DNN puts all alternative and individual\r\nattributes into a large fully-connected network. Then, it estimates the alternative-specific utility using a subsequent small fullyconnected network. We also tested the deep neural networks for alternative-specific utility (ASU-DNN) (Wang et al., 2020a) for\r\nDCM-DNN, which replaces the fully connected network in the shared layer with three fully connected networks for each alternative,\r\nbut it does not outperform the considered DCM-DNN in terms of both predictability and interpretability.\r\nWe describe the details of three layers in Sections 3.2.2\u20133.2.4. The training process will be discussed in Section 3.3.\r\n3.2.2. Input calibration layer\r\nThe input calibration layer in Fig. 2a implements the attribute-specific transformations to capture the attribute-specific non-linear\r\neffect before the lattice layer, using one-dimensional monotonic piecewise linear functions CIN, called input calibrators. Fig. 3 illustrates the examples of transformation function in the input calibrator. The only hyperparameter for a dth attribute input calibrator is the\r\nnumber of change points, and the change points are set as the equally distanced cells in each attribute domain. For example, if one\r\nattribute ranges from zero to one, and the number of change points is set as 11, the change points would be 0.0, 0.1, 0.2, \u2026, 1.0. The\r\nlarger the calibrated number of change points, the higher the extent of non-linearity of attribute-specific effect exists. The estimable\r\nparameters for the input calibration layer, \u03b8LN\r\nINCal, are the slopes of piecewise linear function in the attribute-specific calibrators, as\r\nshown in Fig. 3. For example, \u03b8LN\r\nINCal,4 indicates the slope between vertices 4 and 2 in Fig. 3a. The calibrated value of the input calibrator\r\nfor each attribute is normalized based on the corresponding lattice size of the following lattice layer. For example, as shown in Fig. 3a,\r\nthe calibrated value of travel cost ranges from 0 to 1 for the lattice size of 2, while the calibrated value of travel time ranges from 0 to 2\r\nfor the lattice size of 3, as shown in Fig. 3b.\r\n3.2.3. Lattice layer\r\nFor the input attributes, Xon = (XM\r\non,XNM\r\non ) \u2208 RD, we define the lattice size Sd for each attribute dimension, which is the number of\r\nvertices along the dth attribute dimension. Then, the lattice can be represented by S = S1 \u00d7 \u2026 \u00d7 Sd \u00d7 \u2026 \u00d7 SD parameters and spans the\r\n(S1 \u2212 1) \u00d7 \u2026 \u00d7 (Sd \u2212 1) \u00d7 \u2026 \u00d7 (SD \u2212 1) hyper-rectangle. The lattice size for each attribute is related to the extent of non-linearity in the\r\neffect of the attribute. Thus, the larger lattice size (i.e., fine-grained lattice) represents a more flexible utility function.\r\nIn Fig. 2a, the value of the lattice function L(CIN(Xon)) in the lattice layer is a weighted linear combination of the function values at\r\nsurrounding vertices, where function values at vertices are estimable parameters. The CIN(Xon) (denoted as XIN\r\non) is an output of input\r\ncalibration layer that is normalized according to the attribute-specific lattice size of lattice layer. At the same time, XIN\r\non is the input to\r\nFig. 4. Example of 3 \u00d7 2 lattice approximating the value of function by multi-linear interpolation.\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n12\r\nthe lattice layer.\r\nFig. 4 shows an illustrative example of lattice layer for two input attributes, XIN\r\non = (XIN\r\non[1], XIN\r\non[2]), such that S1 = 3, S2 = 2. The value\r\nof the lattice function L( \u22c5 ) at a specific point, XIN\u2217 on = (XIN\u2217 on [1], XIN\u2217 on [2]) is obtained by interpolating the value of the function at surrounding vertices v of XIN\u2217 on , which split the input space into S1 and S2 cells. The parameters of the lattice function are the functional\r\nvalues at vertices, which are denoted as \u03b8LN\r\nLat (e.g., \u03b8LN\r\nLat,1,1 = L(v1,1) in Fig. 4, where v1,1 is the value at (1,1) location of a surrounding\r\nvertex). For example, in Fig. 4, the values at vertices v1,1, v2,1, v1,2, and v2,2 are (0,0), (1,0), (0,1), and (1,1), respectively. Note that vi,j is\r\nnot necessarily 0 or 1 if the XIN\u2217 on is located around v3,1, or v3,2, whose values at vertices are (2,1) and (2,0), respectively. The grayscale\r\nvalue in the lattice in Fig. 4 indicates the estimated value of L(XIN\u2217 on [1],XIN\u2217 on [2]).\r\nSpecifically, in the illustrative example in Fig. 4, the value of lattice function at XIN\u2217 on can be computed by interpolating the function\r\nvalues at 4 surrounding vertices:\r\nL\r\n(\r\nXIN\u2217\r\non )\r\n= \u03b8LN\r\nLat \u22c5 \u03c8\r\n(\r\nXIN\u2217\r\non )\r\n= \u03b8LN\r\nLat,1,1\u03c81,1\r\n(\r\nXIN\u2217\r\non )\r\n+ \u03b8LN\r\nLat,2,1\u03c82,1\r\n(\r\nXIN\u2217\r\non )\r\n+ \u03b8LN\r\nLat,1,2\u03c81,2\r\n(\r\nXIN\u2217\r\non )\r\n+ \u03b8LN\r\nLat,2,2\u03c82,2\r\n(\r\nXIN\u2217\r\non )\r\n, (9)\r\nwhere \u03c8(XIN\u2217 on ) is a vector of the multi-linear interpolation weights for XIN\u2217 on , which is computed based on the values of surrounding\r\nvertices v as follows:\r\n\u03c8(i,j)\r\n(\r\nXIN\u2217\r\non )\r\n= \u220fD\r\nd=1\r\n(\r\nXIN\u2217\r\non [d] \u2212 minv(i,j)[d]\r\n)(v(i,j)[d]\u2212 minv(i,j)[d])(\r\nmaxv(i,j)[d] \u2212 XIN\u2217\r\non [d]\r\n)(maxv(i,j)[d]\u2212 v(i,j)[d]) (10)\r\nwhere v(i,j)[d] is the value for the dth attribute at (i, j) location of surrounding vertices of XIN\u2217 on . The minv(i,j)[d] and maxv(i,j)[d] are\r\nminimum and maximum values for the dth attribute within the surrounding vertices, respectively. For example, in the case of XIN\u2217 on in\r\nFig. 4, minv(i,j)[1] (i.e., the minimum of v1,1[1], v1,2[1], v2,1[1], v2,2[1]) is 0, minv(i,j)[2] is 0, maxv(i,j)[1] is 1, and maxv(i,j)[2] is 1. Since the\r\nsurrounding vertices always make a unit hypercube, (v(i,j)[d] \u2212 minv(i,j)[d]) and (maxv(i,j)[d] \u2212 v(i,j)[d]) are either 0 or 1. Using Eqs. (9) and\r\n(10), L(XIN\u2217 on ) in the Fig. 4 is calculated as in Eq. (11):\r\nL\r\n(\r\nXIN\u2217\r\non )\r\n= \u03b8LN\r\nLat,1,1\r\n(\r\n1 \u2212 XIN\u2217\r\non [1]\r\n)( 1 \u2212 XIN\u2217\r\non [2]\r\n)\r\n+ \u03b8LN\r\nLat,2,1XIN\u2217\r\non [1]\r\n(\r\n1 \u2212 XIN\u2217\r\non [2]\r\n)\r\n+ \u03b8LN\r\nLat,1,2\r\n(\r\n1 \u2212 XIN\u2217\r\non [1]\r\n)\r\nXIN\u2217\r\non [2] + \u03b8LN\r\nLat,2,2XIN\u2217\r\non [1]XIN\u2217\r\non [2] (11)\r\nAs discussed earlier, there are some similarities in the lattice function and the generalized additive model (GAM). We discuss some\r\ncorrespondence between Fig. 4 and Eq. (6). The attribute-specific non-linear effects g1(Xon[1]) and g2(Xon[2]) in Eq. (6) correspond to\r\nthe lattice function values on the axis of XIN\r\non[1] and XIN\r\non[2], respectively. The first-order interactions r1\r\n1,2(g1(Xon[1]), g2(Xon[2])) correspond to the functional values L(XIN\u2217\r\non ) on the lattice (i.e., gray scale value) that are calculated by interpolating the function values at\r\nvertices around XIN\u2217\r\non .\r\nWe illustrate in Section 3.3 that applying constraints on \u03b8LN\r\nLat is sufficient to ensure the partial monotonicity of L(XIN\u2217 on ) relative to a\r\nsubset of attributes with expected monotonic effects. The process of writing these constraints can be generalized and automated for any\r\nsize of the lattice.\r\n3.2.4. Output calibration layer\r\nThe output of the lattice layer is a vector of the same size as the number of alternatives. The output calibration layer transforms the\r\noutput of lattice layer to supplement the non-linearity before representing the final utility. The output calibrators (COUT) in this layer\r\nshare the same functional form as the input calibrator (i.e., one-dimensional monotonic piecewise linear function with a single\r\nhyperparameter, the number of changing points). Therefore, the estimable parameters for the output calibration layer, \u03b8LN\r\nOUTCal, are also\r\nthe slopes of piecewise linear functions. However, the input calibration layer has as many nodes as the number of attributes, while the\r\noutput calibration layer has as many nodes as the number of alternatives (i.e., alternative-specific calibrators).\r\nThe combination of input calibration layer, lattice layer, and output calibration layer achieves parsimonious model specification to\r\ncapture non-linear and interaction effects while efficiently ensuring partial monotonicity. Specifically, the input calibration layer and\r\nthe lattice layer capture the attribute-specific non-linear effect, but the latter additionally captures the interaction effects between\r\nthose attribute-specific effects. The output calibration layer further supplements the non-linearity of the final output. The lattice layer\r\nmainly determines the computational complexity because the number of required inequality conditions for monotonicity constraints\r\ndrastically increase as the lattice size increases. Therefore, if the input calibration layer can capture enough non-linearity of attributespecific effects and the output calibration layer can supplement the overall non-linearity, we can reduce the lattice size as capturing\r\ninteraction effects becomes the main job of the lattice layer.\r\nIn summary, the COUT(L(CIN(Xon)) in Fig. 2a estimates the alternative-specific utility function\r\nFLN\r\no (Xon; \u03b8LN) for multi-alternative choice. The parameter of DCM-LN, \u03b8LN, consists of (i) the slopes of piecewise linear function in\r\nthe attribute-specific input calibrators \u03b8LN\r\nINCal (ii) S = S1 \u00d7 \u2026 \u00d7 Sd \u00d7 \u2026 \u00d7 SD number of parameters representing the value of function in\r\nthe vertices \u03b8LN\r\nLat, and (iii) the slope of piecewise linear function in the alternative-specific output calibrators \u03b8LN\r\nOUTCal. The parameters for\r\nthese three different layers are jointly estimated in the training process. We discuss monotonicity constraints on these parameters in\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n13\r\nSection 3.3.\r\n3.3. Training the lattice networks\r\nFor the discrete choice datasets consisting of input attributes Xon and output choices yon, the objective of the training DCM-LN is to\r\nestimate the \u03b8LN = {\u03b8LN\r\nINCal, \u03b8LN\r\nLat, \u03b8LN\r\nOUTCal} while ensuring the monotonicity constraints for a subset of attributes. For the kth attribute\r\n(XM\r\non[k]) that monotonically increases the utility, the monotonicity is ensured in the lattice layer if \u03b8LN\r\nLat, s \u2265 \u03b8LN\r\nLat, r for all adjacent\r\nvertices s and r along the kth attribute dimension. The s and r indicate the location of the vertex in a generalized manner, regardless of\r\nthe dimension of the lattice. The examples of adjacent vertices s and r in Fig. 4 are (\u03b8LN\r\nLat, (1,1), \u03b8LN\r\nLat, (1,2)) or (\u03b8LN\r\nLat, (2,1),\u03b8LN\r\nLat, (3,1)). Therefore,\r\nthe number of inequality constraints to be checked for partial monotonicity depends on the number of monotonic (K) and nonmonotonic (D \u2212 K) attributes, and the lattice size S. For example, if all K attributes are monotonically constrained (K = D), and the\r\nlattice size consists of 2K vertices (i.e., the lattice sizes of all attributes is 2), it requires checking K \u00d7 2K \u2212 1 inequality constraints (i.e.,\r\nthe number of edges in the K-dimensional hypercube). If K out of D attributes are monotonically constrained with the lattice size 2, the\r\nrequired number of inequality constraints is K \u00d7 2D \u2212 1 (i.e., the number of edges related to K attributes in the D-dimensional hypercube). Similarly, monotonicity constraints are also imposed on the attribute-specific input calibrators and the alternative-specific\r\noutput calibrators.\r\nFor the parameters of input calibrator for kth monotonic attribute, \u03b8LN\r\nINCal[k] (i.e., the slopes of piecewise linear function between\r\nadjacent vertices), \u03b8LN\r\nINCal, s[k] \u2264 \u03b8LN\r\nINCal,r[k] should be maintained for all adjacent vertices s and r, to make the CIN(XM\r\non[k]) be a piecewise\r\nmonotonically increase linear function. The examples of adjacent vertices s and r in Fig. 3a are (\u03b8LN\r\nINCal, 2, \u03b8LN\r\nINCal,4) or (\u03b8LN\r\nINCal,4, \u03b8LN\r\nINCal,6).\r\nWith the similar constraints and functional form, the output calibrator is also estimated as a one-dimensional monotonic piecewise\r\nlinear function.\r\nWith these three types of inequality constraints, the DCM-LN is estimated based on a structural risk minimization (i.e., considering\r\nboth empirical risk and model complexity) using batched stochastic gradients and ADAM optimizer. The estimation of the \u03b8LN is\r\nformulated as shown in Eq. (12) through (14).\r\nFLN\r\no\r\n(\r\nXon; \u03b8LN)\r\n= COUT (\r\nL\r\n(\r\nCIN(\r\nXon; \u03b8LN\r\nINCal)\r\n; \u03b8LN\r\nLat)\r\n; \u03b8LN\r\nOUTCal) (12)\r\nPLN\r\non = eFLN\r\no (Xon;\u03b8LN )\r\n/\u2211O\r\nj=1\r\ne\r\nFLN\r\nj (Xjn;\u03b8LN ) (13)\r\nargmin\r\n\u03b8LN\r\n\u2211N\r\nn=1\r\nL (\r\nyn, PLN\r\non )\r\n+ R\r\n(\r\n\u03b8LN\r\nINCal)\r\ns.t A\u03b8LN\r\nLat \u2264 0, B\u03b8LN\r\nINCal \u2264 0, and C\u03b8LN\r\nOUTCal \u2264 0 (14)\r\nWe apply the wrinkle regularizer and Hessian regularizer (R(\u03b8LN\r\ninCal)) to the input calibration layer for each attribute. The wrinkle\r\nregularizer gives a penalty for the changes in the second derivative of the output of input calibration layer, while the Hessian regularizer penalizes for the change in slopes of subsequent piece-wise linear functions. These regularizers can also be applied to the output\r\ncalibration layer and the lattice layer, but we do not opt for that because the number of changing points in the output calibration layer\r\nand the lattice size in lattice layer are not that high compared to the changing points in the input calibration layer.\r\nThe monotonicity constraints are imposed by inequality constraints related to the matrix A, B, and C in Eq. (14). Only matrix A,\r\nthat imposes monotonicity constraints on the lattice layer with parameter \u03b8LN\r\nLat, is explained in detail by Eq. (15) because the calibration\r\nlayers with parameter \u03b8LN\r\nINCal or \u03b8LN\r\nOUTCal are equivalent to the one-dimensional lattice.\r\nFor example, in Fig. 4, to guarantee the monotonic increase in L( \u22c5 ) relative to XIN\r\non[1] and XIN\r\non[2], the following seven inequalities\r\nshould be satisfied (see Gupta et al. 2016):\r\n\u2022 \u03b8LN\r\nLat,1,1 \u2264 \u03b8LN\r\nLat,2,1, \u03b8LN\r\nLat,2,1 \u2264 \u03b8LN\r\nLat,3,1, \u03b8LN\r\nLat,1,2 \u2264 \u03b8LN\r\nLat,2,2, \u03b8LN\r\nLat,2,2 \u2264 \u03b8LN\r\nLat,3,2 for XIN\r\non[1]-axis (4 inequalities)\r\n\u2022 \u03b8LN\r\nLat,1,1 \u2264 \u03b8LN\r\nLat,1,2, \u03b8LN\r\nLat,2,1 \u2264 \u03b8LN\r\nLat,2,2, \u03b8LN\r\nLat,3,1 \u2264 \u03b8LN\r\nLat,3,2 for XIN\r\non[2]-axis (3 inequalities)\r\nSeven rows in Eq. (15) correspond to these seven inequalities. The number of inequalities is the number of edges related to the\r\nconstrained attributes in the hypercube (e.g., 2-dimensional hypercube in Fig. 4). If we want to impose the monotonicity constraint\r\nonly on XIN\r\non[1]-axis (i.e., partial monotonicity), we can ignore the inequalities related to the XIN\r\non[2]-axis.\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n14\r\nA\u03b8LN\r\nLat =\r\n\u239b\r\n\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d\r\n1 \u2212 1 0 0 0 0\r\n0 0 1 \u2212 1 0 0\r\n0 0 0 0 1 \u2212 1\r\n1 0 \u2212 1 0 0 0\r\n0 0 1 0 \u2212 1 0\r\n0 1 0 \u2212 1 0 0\r\n0 0 0 1 0 \u2212 1\r\n\u239e\r\n\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0\r\n\u239b\r\n\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d\r\n\u03b8LN\r\nLat,1,1\r\n\u03b8LN\r\nLat,1,2\r\n\u03b8LN\r\nLat,2,1\r\n\u03b8LN\r\nLat,2,2\r\n\u03b8LN\r\nLat,3,1\r\n\u03b8LN\r\nLat,3,2\r\n\u239e\r\n\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0\r\n(15)\r\nWith the similar concept, we impose the monotonicity constraints on input and output calibration layers for all attributes. For\r\nexample, in Fig. 3a, to ensure that the calibrated travel cost increases related to travel cost, we impose the following inequality\r\nconditions on the parameters for the input calibrator: \u03b8LN\r\nINCal,2 \u2264 \u03b8LN\r\nINCal,4, \u03b8LN\r\nINCal,4 \u2264 \u03b8LN\r\nINCal,6, \u2026, \u03b8LN\r\nINCal,8 \u2264 \u03b8LN\r\nINCal,10. The output calibration\r\nlayer imposes monotonicity constraints using the same process. Since we consistently impose the monotonicity constraints on a specific\r\nattribute from input calibration layer, lattice layer, to output calibration layer, the final systematic utility function is monotonic\r\nrelative to the attribute. If we do not want to impose monotonicity on any attribute, we exclude the inequalities related to that attribute\r\nin all layers.\r\nWe adopt efficient optimization strategies (Gupta et al., 2016), such as parallel computation, suboptimal projections of inequality\r\nconstraints, and alternate updating of the parameters of lattice and calibration layers with stochastic sub-gradient. The DCM-LN is\r\ntrained based on Eq. (14), which indicates a structural risk minimization with inequality conditions to incorporate partial monotonicity constraints. The batched stochastic gradients and ADAM optimizer are used to update the model parameters. In each\r\nmini-batch, sub-gradients are derived to update the parameter. When these sub-gradients update parameters, we apply projection on\r\nthe parameters to satisfy the monotonicity constraints. For the input and output calibrator, isotonic regression with total ordering is\r\nused for projection, which satisfy the B\u03b8LN\r\nINCal \u2264 0 and C\u03b8LN\r\nOUTCal \u2264 0. The isotonic regression fits input values to the values that are\r\nmonotonic and as close to the original values as possible. We solve it using a pool-adjacent-violator algorithm (Barlow et al., 1974)\r\nwhose complexity is linear in the number of changing points. For the lattice layer, isotonic regression with partial ordering is used for\r\nprojection, which requires additional linear inequality constraints as many as the number of edges of the hypercube (A\u03b8LN\r\nLat \u2264 0). These\r\nisotonic regression problems are jointly solved for the input calibration layer, lattice layer, and output calibration layer using Eq. (14).\r\nThe consensus-based optimization and alternating direction method of multipliers are used for solving these projection problems with\r\nparallel computation (You et al., 2017).\r\n3.4. Bayesian optimization for hyperparameter calibration\r\nThe proposed DCM-LN requires tuning numerous hyperparameters. We need to determine the number of change points in the input\r\ncalibration layer for each alternative-specific attributes across alternatives and individual-specific attributes, the number of change\r\npoints in the output calibration layer for each alternative, weights for regularizers, and lattice size for each attribute. The simplest way\r\nfor hyperparameter tuning is a grid search technique, i.e., considering all combinations of hyperparameters; however, it is infeasible to\r\nenumerate all hyperparameters due to the curse-of-dimensionality. To efficiently tune these high-dimensional hyperparameters, this\r\nstudy applies Bayesian optimization (BO). We provide the details on BO applied in this study in Appendix A.\r\n3.5. Partial dependence (PD) and individual conditional expectation (ICE)\r\nThe DCM-DNN tends to fit the data with an overly complex model considering excessive interactions between attributes. The\r\nmonotonicity constraints rather act as regularization to control such excessive interactions of data-driven DCM, leading to improvements in both interpretability and recovery of the underlying true utility function (i.e., identification of attribute-specific effect\r\non the utility). To evaluate these aspects, we need to estimate the individual-level utility function of each attribute.\r\nVarious post hoc analysis tools have emerged to explain the attribute-specific effect (i.e., utility function) of machine learning\r\nmodels, such as partial dependence (PD) (Friedman, 2001), individual conditional expectation (ICE) (Goldstein et al., 2015), local\r\ninterpretable model-agnostic explanations (LIME) (Ribeiro et al., 2016), and Shapley additive explanations (SHAP) (Lundberg et al.,\r\n2017). We adopt PD and ICE to estimate the attribute-specific utility function at population level and individual level, respectively.\r\nSpecifically, we estimate the PD of dth attribute on the utility of alternative o, PDo[d] and the ICE of dth attribute of individual n on the\r\nutility of alternative o, ICEon[d]. The PDo[d] is computed as follows:\r\nPDo[d] = 1\r\nN\r\n\u2211N\r\nn=1\r\nFo(Xon[d],Xon[\u2212 d]), (16)\r\nwhere Xon[d] is the target attribute to estimate the PD and Xon[ \u2212 d] are the remaining attributes in the utility function Fo. The effect of\r\nXon[d] on the Fo is calculated by marginalizing the Fo over the distribution of the other remaining attributes Xon[ \u2212 d] using Monte\r\nCarlo method. The ICE is individual-level PD, which is ICEon[d] = Fo(Xon[d],Xon[ \u2212 d]). In other words, the aggregated ICE indicates the\r\nPD (Goldstein et al., 2015). In the following section, we verify that the PD and ICE well estimate the true utility function of the\r\nsimulation data.\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n15\r\n4. Monte Carlo study\r\nWe conduct a Monte Carlo study to evaluate the interpretability and predictability of DCM-LN. Regarding interpretability, we\r\ncompare the DCM-LN\u2019s willingness-to-pay (WTP) estimates for alternative-specific attributes at a disaggregated level with those of the\r\ntrue data-generating process (DGP). We use the in-sample and out-of-sample choice prediction accuracy and Brier score as the predictability metrics. The DGP of the utility function in the Monte Carlo study is complex enough to include earlier-discussed commonly\r\nencountered behavioral mechanisms: (i) interactions between alternative-specific and individual-specific attributes (i.e., systematic\r\nindividual taste heterogeneity); (ii) interactions between multiple alternative-specific attributes; (iii) interactions between individualspecific attributes; and (iv) inherent non-linear effect of alternative-specific attributes. More details of the considered DGPs, evaluation\r\nmetrics, benchmarked models, hyperparameter tuning, and the results of the simulation study are provided in the upcoming sections.\r\n4.1. Data generating process\r\n4.1.1. DGP-New: interactions and non-linearity of alternative-specific attributes\r\nThe DGP-Base follows Han et al. (2022) to represent the systematic taste heterogeneity in the synthetic binary mode choice between train and metro. In the DGP-Base, the three individual attributes \u2013 income (IN), full-time job (FUL), and flexible commuting\r\n(FLX) create systematic taste heterogeneity for the effects of two alternative-specific attributes \u2013 travel time (TT) and waiting time\r\n(WT). The IN is a numeric variable, while the FUL and FLX are dummy variables. Since we are interested in the WTP estimates for\r\nalternative-specific attributes (i.e., the value-of-travel time (VOT) and the value-of-walking time (VOWT)), we set the parameter of\r\ntravel cost (TC) as \u20131. We assume that the train and metro share the parameters for TT and WT. Eq. (17) denotes the systematic utility\r\n(Von) of individual n and alternative o (train or metro) in the DGP-Base.\r\nFig. 5. Attribute-specific true utility functions of DGP represented by PD and ICE.\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n16\r\nVon = \u2212 0.1 \u2212 TCon+\r\n( \u2212 0.1 \u2212 0.5 \u00d7 INn \u2212 0.1 \u00d7 FULn + 0.05 \u00d7 FLXn\r\n\u2212 0.2 \u00d7 INn \u00d7 FULn + 0.05 \u00d7 INn \u00d7 FLXn + 0.1 \u00d7 FULn \u00d7 FLXn\r\n)\r\n\u00d7 TTon+\r\n( \u2212 0.2 \u2212 0.8 \u00d7 INn \u2212 0.3 \u00d7 FULn + 0.1 \u00d7 FLXn\r\n\u2212 0.3 \u00d7 INn \u00d7 FULn + 0.08 \u00d7 INn \u00d7 FLXn + 0.3 \u00d7 FULn \u00d7 FLXn\r\n)\r\n\u00d7 WTon\r\n(17)\r\nThe interaction terms in the parenthesis represent the systematic taste heterogeneity caused by the combined effects of individual\r\nattributes.\r\nThe interaction and inherent non-linearity of alternative-specific attributes are well-known behavioral mechanisms that are\r\nignored by DCM-Linear and the previous DCM-DNN relying on MNL (Han et al., 2022; Sifringer et al., 2020). To evaluate these aspects,\r\nwe define a DGP-New that modifies the DGP-Base as follows: (i) the crowding (CR) and its interaction with TT are added to the\r\ndeterministic utility; (ii) inherent non-linear utility (i.e., diminishing marginal utility) of TC replaces the linear utility of TC. Eq. (18)\r\ndenotes the systematic utility of DGP-New.\r\nVon = \u2212 0.1 \u2212 8 \u22c5 \u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305 TCon \u221a \u2212 2.0 \u00d7 CRon+\r\n( \u2212 0.02 \u00d7 CRon \u2212 0.1 \u2212 0.5 \u00d7 INn \u2212 0.1 \u00d7 FULn + 0.05 \u00d7 FLXn\r\n\u2212 0.2 \u00d7 INn \u00d7 FULn + 0.05 \u00d7 INn \u00d7 FLXn + 0.1 \u00d7 FULn \u00d7 FLXn\r\n)\r\n\u00d7 TTon+\r\n( \u2212 0.2 \u2212 0.8 \u00d7 INn \u2212 0.3 \u00d7 FULn + 0.1 \u00d7 FLXn\r\n\u2212 0.3 \u00d7 INn \u00d7 FULn + 0.08 \u00d7 INn \u00d7 FLXn + 0.3 \u00d7 FULn \u00d7 FLXn\r\n)\r\n\u00d7 WTon\r\n(18)\r\n4.1.2. Sampling from the DGP\r\nWe describe the sampling distribution of each attribute in DGP-Base and DGP-New in Appendix B. Based on the assumed distribution, we calculate the utility derived by an individual for each alternative using the DGP and assign the alternative with the highest\r\nutility as the chosen alternative by the individual. For each trial, 10,000 individuals are generated from DGP. 70 % of the generated\r\ndata are used for training, 15 % of the data for validation, and 15 % of the data for testing. Since the DGP generates slightly different\r\ndatasets due to random error term and uncertainty in attributes, we conduct analysis for 50 different synthetic datasets. Fig. 5 illustrates the utility functions of alternative-specific attributes in the DGP-New. The PD and ICE, by definition, represent the\r\npopulation-level and individual-level utility functions, respectively. Variations in individual-level utility (i.e., ICE \u2013 thin grey lines) of\r\nTT and WT indicate the individual taste heterogeneity explained by combined effects of individual-level attributes. Also, it represents\r\nthe inherent non-linear utility function of TC. The two-way PD at the bottom right of Fig. 5 shows the interactions among the TT and\r\nCR.\r\n4.2. Benchmarked models and evaluation metrics\r\nDCM-DNN and DCM-Linear models are considered as benchmark models while evaluating the recovery of WTP and predictive\r\nperformance. The DCM-Linear in this section considers the true first-order interactions between the alternative-specific and individualspecific attributes (e.g., FLXn \u00d7 TTon, FULn \u00d7 TTon) while ignoring second-order interactions (e.g., FULn \u00d7 FLXn \u00d7 TTon) because\r\nhandcrafted utility functions rarely consider such high-order interactions. Also, the DCM-Linear does not consider interactions between alternative-specific attributes (e.g., TTon \u00d7 CRon) and inherent non-linearity (e.g., 8 \u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305 TCon \u221a ). For the DCM-DNN and DCM-LN\r\nestimation, we do not provide any information related to the true DGP and input all the individual- and alternative-specific attributes. The DCM-LN only uses the prior knowledge in which the utility values at individual and population levels monotonically\r\ndecrease with the increase in TT, WT, CR, and TC.\r\nTo evaluate the interpretability, we compare the taste heterogeneity in WTP for alternative-specific attributes obtained from 50\r\nsynthetic datasets, which provide the most critical behavioral interpretation in transportation planning. Specifically, we define 40\r\n(=10 \u00d7 2 \u00d7 2) demographic groups based on three individual-specific attributes: IN (10 levels), FUL (2 levels), and FLX (2 levels), and\r\ncompare the true and estimated VOT and VOWT for these groups. VOT and VOWT could be easily computed through the ratio of the\r\nestimated coefficients in MNL. In DCM-DNN and DCM-LN, VOT and VOWT are calculated by aggregating ICE based on attribute levels\r\ncorresponding to each group, where the median value is used for aggregation. After examining the distribution of estimated VOT and\r\nVOWT at five quantile values: 1 %, 25 %, 50 % (median), 75 %, and 99 %, the accuracy of the estimated VOT and VOWT for 40\r\ndemographic groups is evaluated by comparing the root mean squared error (RMSE) and mean absolute percentage error (MAPE). To\r\nevaluate the predictability, we compare choice prediction accuracy and Brier score. Appendix C provides the formula and interpretation for these evaluation metrics.\r\n4.3. Hyperparameter tuning\r\nHyperparameter tuning is closely related to the model performance since it affects the quality of the training process. It is\r\nparticularly critical for the proposed DCM-LN because the attribute-specific hyperparameters in DCM-LN are generally more than the\r\nhyperparameters in DCM-DNN. As discussed earlier, we apply the Bayesian optimization approach that estimates the relationship\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n17\r\nbetween the set of hyperparameters and model performance to reduce the search space.\r\nAppendix D summarizes search space of hyperparameters for DCM-LN and DCM-DNN, training conditions (e.g., learning rate, the\r\nnumber of epochs, stopping criteria) and the tuned hyperparameters. We tune four hyperparameters of DCM-DNN: the number of\r\nlayers (i.e., layer depth), the number of neurons in each layer (i.e., layer width), the learning rate, and the dropout rate after each layer.\r\nFor DCM-LN, we tune the number of change points for each attribute in the input calibration layer, lattice size for all attributes, and the\r\nnumber of change points for the output calibration layer.\r\n4.4. Evaluation results for DGP-New\r\nWe evaluate the proposed DCM-LN for DGP-Base and DGP-New. An in-depth analysis of evaluation results for DGP-Base is provided\r\nin Appendix E, which discusses several insightful patterns and implications. In summary, DCM-LN outperforms the DCM-Linear\r\nregarding both interpretability and predictability. DCM-LN also outperforms DCM-DNN in terms of interpretability, but DCM-DNN\r\nhas a slight edge over DCM-LN in terms of predictive accuracy. Table 3 summarizes the evaluation results for interpretability and\r\npredictability for DGP-New. The DGP-New is more complex than DGP-Base because it additionally includes the inherent non-linear\r\neffect of TC and the interaction effect between two alternative-specific attributes (TT and CR). The nonlinearity in the marginal\r\nutility of TC also increases the difficulty in recovering the true VOT and VOWT distributions.\r\nThe evaluation results for DGP-New in Table 3 provide four noteworthy findings. First, the predictive performance of DCM-Linear\r\nis much lower than that of DCM-DNN and DCM-LN. The interpretability of DCM-linear also reduces significantly, indicating how the\r\nmisspecification of the utility function is more likely to occur in the case of complex DGPs and how it dramatically reduces the predictive performance and the trustworthiness of traditional DCMs with hand-crafted utility specifications. Second, DCM-DNN shows\r\nthe best predictability and the worst interpretability, indicating that the trade-off relationship still holds for more complex functions.\r\nThe interpretability of DCM-DNN remains much worse than that of DCM-Linear. These results imply that the overly complex function\r\nfitted by DCM-DNN cannot provide trustworthy behavioral information about the true DGP regardless of its complexity. Third, DCMLN highly outperforms DCM-Linear and DCM-DNN in terms of interpretability, i.e., it shows the best performance for all quantiles and\r\ndemographic group-level WTP estimates. The capability to approximate the non-linear function of TC could be a reason behind the\r\ndistinct interpretability of DCM-LN. In terms of predictive performance, the DCM-LN also highly outperforms the DCM-Linear while\r\nslightly underperforming the DCM-DNN. Considering the balanced performance based on interpretability and predictability, the DCMLN is the best model for DGP-New. Forth, unlike DCM-LN, DCM-Linear and DCM-DNN estimate the negative VOT and VOWT for some\r\nindividuals (see 1 % quantiles). This result suggests that the monotonicity constraints successfully regularize the DCM-LN toward the\r\ntrue DGP and make its WTP estimates trustworthy.\r\nWhile Figs. 6 and 7 support the findings derived from Table 3 and are consistent with those obtained for DGP-Base, they also reveal\r\nadditional insightful patterns. First, Fig. 7 shows that DCM-LN approximates the non-linear effect of TC much better than DCM-DNN at\r\nboth population and individual levels, resulting in much better recovery of WTP distribution. The estimated utility of TC by DCM-DNN\r\nhas a serious concern as it provides positive marginal utility at some levels of TC, leading to negative WTP estimates. In contrast, DCMLN prevents such misidentifications through theory-driven monotonicity constraints. Second, even though the DCM-Linear\u2019s estimated utility functions are far from the true DGP (as shown in Fig. 7), the estimated distribution patterns of VOT and VOWT are fairly\r\nclose to those of DCM-LN, as shown in Fig. 6. This result supports our old belief in the interpretability of DCM-Linear.\r\nTable 3\r\nInterpretability and predictability evaluation in DGP-New.\r\nParameter True DCM-Linear\r\n(50 trials)\r\nDCM-DNN\r\n(50 trials)\r\nDCM-LN\r\n(50 trials)\r\nMean Std. Mean Std. Mean Std. Mean Std.\r\nInterpretability: recovery of distribution VOT (Median) 0.284 0.014 0.126 0.019 0.075 0.105 0.188 0.080\r\nVOT (1 %) 0.142 0.010 \u2212 0.026 0.029 \u2212 0.012 0.281 0.093 0.063\r\nVOT (25 %) 0.216 0.013 0.066 0.021 0.040 0.085 0.135 0.072\r\nVOT (75 %) 0.404 0.024 0.200 0.013 0.123 0.172 0.257 0.089\r\nVOT (99 %) 0.675 0.027 0.372 0.024 0.222 0.214 0.456 0.105\r\nVOWT (Median) 0.480 0.019 0.258 0.148 0.146 0.210 0.322 0.134\r\nVOWT (1 %) 0.252 0.011 \u2212 0.068 0.124 \u2212 0.114 0.797 0.153 0.082\r\nVOWT (25 %) 0.372 0.017 0.118 0.141 0.086 0.159 0.244 0.127\r\nVOWT (75 %) 0.779 0.033 0.414 0.175 0.233 0.273 0.472 0.181\r\nVOWT (99 %) 1.236 0.049 0.719 0.288 0.402 0.560 0.892 0.263\r\nInterpretability: recovery of demographic groups\u2019 value VOT (MAPE) 0.630 0.044 0.802 0.329 0.351 0.103\r\nVOT (RMSE) 0.193 0.012 0.272 0.102 0.129 0.030\r\nVOWT (MAPE) 0.598 0.195 0.846 0.459 0.359 0.112\r\nVOWT (RMSE) 0.348 0.092 0.546 0.259 0.243 0.063\r\nPredictability: chosen alternative and probabilities Training accuracy 0.552 0.006 0.775 0.010 0.741 0.018\r\nTest accuracy 0.546 0.013 0.716 0.014 0.697 0.016\r\nTraining Brier score 0.491 0.002 0.311 0.012 0.349 0.020\r\nTest Brier score 0.493 0.003 0.371 0.014 0.395 0.015\r\nNote: Std. indicates the standard deviation from different datasets generated; The bold indicates the best performance among the benchmark models.\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n18\r\n5. Empirical application\r\nWe use the Swissmetro dataset (Bierlaire, 2018), an open source stated preference data that collect the preferences of 1192 respondents for a train (TR), Swiss metro (SM), and car (CAR) in 9 choice situations. The attribute levels of the choice experiment are\r\nsummarized in Table 4 (Bierlaire et al., 2001). We exclude individuals with unknown age, income, or purpose. The final sample includes 939 respondents who face 8451 choice situations. The travel cost for TR and SM modes is set to zero for respondents with an\r\nannual season pass. We split the data into 70 % training, 15 % validation, and 15 % test samples.\r\n5.1. Specifications of DCM-Linear, DCM-DNN, and DCM-LN models\r\nWe estimate three DCM-Linear models \u2013 DCM-Linear-A, DCM-Linear-B, and DCM-Linear-C. The DCM-Linear-A only includes\r\nalternative-specific attributes, while the DCM-Linear-B additionally considers first-order interactions between travel time and individual attributes. Referring to the model specification of Bierlaire et al. (2001) and Han et al. (2022), we find the best specification for\r\nDCM-Linear-A and Linear-B, which has a good predictive performance and statistically significant interaction effects at the 10 %\r\nsignificance level at least for one alternative. The DCM-Linear-C represents the utility specification optimized by an automatic (or\r\nassisted) specification in Ortelli et al. (2021). It considers first-order interaction between travel time, travel cost, and individual attributes. Also, non-linear effects of travel time and headway are considered by log and square root transformation, respectively. The\r\noptimized utility function was selected among 4.6 \u00d7 108 distinct specifications based on the out-of-sample log-likelihood.The\r\nparameter estimates of DCM-Linear-A, DCM-Linear-B, and DCM-Linear-C are presented in Tables A.5\u2013A.7 in Appendix F. We design\r\nthe architecture of DCM-LN (Fig. 2a) and DCM-DNN (Fig. 2b) based on the alternative-specific utility specification. Table A.8 in\r\nAppendix G summarizes the search space for hyperparameters, training conditions, and the tuned hyperparameters for DCM-LN and\r\nDCM-DNN for the Swissmetro dataset. Since the DCM-LN requires only one-twentieth of the model parameters of DCM-DNN, the\r\nregularization effects would be much more pronounced in the Swissmetro dataset than those in the DGP-Base and DGP-New.\r\n5.2. Evaluation results for the Swissmetro dataset\r\nWe use similar measures of predictability and interpretability as those of the Monte Carlo study. To evaluate taste heterogeneity in\r\nWTP estimates, we define 60 demographic groups based on three individual-specific attributes \u2013 five categories for age, three categories for income, and four categories for purpose. We also compare five quantile values of the WTP distribution.\r\nUnlike the Monte Carlo study, the true values of parameters (i.e., true VOT) remain unknown in the empirical data. We will use the\r\nFig. 6. VOT and VOWT estimates for 40 demographic groups in DGP-New.\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n19\r\nfollowing facts based on theory-driven intuition and results of the previous empirical studies in the Swiss context for model benchmarking (Axhausen et al., 2007, 2004): (i) the VOT in the car is larger than VOT in the railway; (ii) VOT cannot be negative for any\r\nsocio-demographic group. In light of these facts, we discuss the causes and implications of the discrepancy between VOT estimates of\r\ndifferent DCM-Linear specifications, DCM-DNN, and DCM-LN.\r\nTable 5 summarizes the estimated quantiles of the VOT distribution for all models. We observe two insightful trends. First, DCMLinear-A, DCM-Linear-B, DCM-Linear-C and DCM-LN estimate similar levels of VOT, while the estimated VOTs of DCM-DNN are much\r\nlower than those of DCM-LN and DCM-Linear across all quantiles and alternatives. In contrast, the DCM-DNN shows the best predictability for all metrics. These results further verify the insights of the simulation study that the DCM-DNN does not ensure interpretability while optimizing the loss function to maximize the predictability. Second, DCM-Linear-B\u2019s VOT estimates have two biases:\r\n(i) negative VOT for some demographic groups (see 1 % quantile) and (ii) higher VOT in the train than in the car. DCM-Linear-C\r\ncorrects both biases of DCM-Linear-B but exacerbates another bias of VOT overestimation for a large proportion of the population.\r\nThe one-percentile of the estimated VOT distribution (i.e., 1.759) by DCM-Linear-C is higher than the median VOT (i.e., 1.435) estimate of DCM-Linear-B for Swissmetro. This result implies that the estimated VOT is sensitive to the model specification of DCMLinear, and the optimal model with the best model fit may not guarantee the best estimation of the true VOT. DCM-LN corrects all\r\nbiases of DCM-Linear-B and DCM-Linear-C, highlighting the strength of DCM-LN to enhance the trustworthiness of behavioral\r\nFig. 7. Attribute-specific utility functions estimated by (a) DCM-Linear, (b) DCM-DNN, (c) DCM-LN in DGP-New at the individual and population\r\nlevel, and d) overlapping population-level utility functions.\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n20\r\ninterpretations. This result underlies our argument that the DCM-LN better approximates true VOT than the DCM-Linear by capturing\r\nthe complex high-order interaction and non-linear effects, which are not captured by DCM-Linear. Importantly, such improved\r\ninterpretability comes with DCM-LN\u2019s improved predictability over the DCM-Linear, thanks to the flexible utility specification with\r\nregularization generated by monotonicity constraints.\r\nWe delve deeper into insights from Table 5 by plotting VOT estimates for 60 demographic groups (in ascending order of DCMLinear-B\u2019s estimates \u2013 black solid line) in Fig. 8 and plotting PD and ICE estimates in Fig. 9. Among the three DCM-Linear, we adopt the\r\nDCM-Linear-B for the comparison since it shows the highest predictability and a reasonable range of VOT estimates.\r\nTwo meaningful patterns from Figs. 8 and 9 are summarized as follows. First, DCM-DNN in Fig. 8 consistently underestimates the\r\nVOT for all three alternatives, and their trend seems to be incoherent with the trend of DCM-Linear. This pattern reconfirms our\r\nhypothesis that DNN\u2019s capability to maximize the data-fitting does not relate to the behavioral relationships between individual- and\r\nalternative-specific attributes. Fig. 9 provides clear evidence for this pattern in which several individuals have a positive marginal\r\nutility of travel time and cost for each alternative. These negative values of VOTs cause the underestimation of group specific VOTs.\r\nDespite the highly heterogenous individual-level utility, the PD of DCM-DNN (i.e., population-level utility) is not very different from\r\nthose of DCM-Linear and DCM-LN. This result suggests that even if DCM-DNN can provide some useful economic information at the\r\npopulation level, it still cannot be applied to individual-level behavioral modeling. Second, Fig. 9 shows that DCM-LN could successfully impose monotonicity constraints on the utility of travel time and travel cost. It is promising that the DCM-LN could capture\r\nthe diminishing marginal utility of travel cost, a well-known behavioral pattern (Daly et al., 2017; Rich and Mabit, 2016), while the\r\nutility functions estimated by DCM-DNN do not adhere to economic theory. These results imply that the monotonicity constraints in\r\nthe DCM-LN help the model approximate the true utility function, without just focusing on enhancing the predictive performance.\r\nTable 4\r\nDescriptive statistics of the Swiss metro datasets.\r\nAlternative Alternative attributes\r\nTrain (TR) Intercept; Travel time; Travel cost; Headway;\r\nSwiss Metro (SM) Intercept; Travel time; Travel cost; Headway; Airline seats\r\nCar (CAR) (reference) Travel time; Travel cost\r\nIndividual\r\nattributes\r\nAttribute levels\r\nAnnual pass 0: No, 1: Yes\r\nAge 1: age \u2264 24 (reference), 2: 24 < age \u2264 39, 3: 39 < age \u2264 54, 4: 54 < age \u2264 65, 5: 65 < age\r\nIncome 1: under 50 (reference), 2: between 50 and 100, 3: over 100\r\n* Traveler\u2019s income per year [thousand CHF]\r\nPurpose 1: Commuter or return from work (reference), 2: Shopping or return from shopping,\r\n3: Business or return from business, 4: Leisure or return from leisure\r\nNote: Airline seats indicate seat configuration in the Swissmetro (dummy attributes).\r\nTable 5\r\nInterpretability and predictability evaluation for Swissmetro dataset.\r\nParameter DCM-LinearA\r\nDCM-LinearB\r\nDCM-LinearC\r\nDCM-DNN (50\r\ntrials)\r\nDCM-LN (50\r\ntrials)\r\nMean Mean Mean Mean Std. Mean Std.\r\nInterpretability: estimated distribution Train VOT\r\n(Median)\r\n2.423 1.931 2.249 0.585 0.192 1.321 0.492\r\nVOT (1 %) \u2212 0.079 1.537 0.187 0.093 0.214 0.197\r\nVOT (25 %) 1.041 1.852 0.475 0.102 0.892 0.274\r\nVOT (75 %) 2.565 2.921 0.696 0.233 2.066 0.686\r\nVOT (99 %) 3.340 5.696 0.931 0.272 5.703 2.643\r\nSwiss VOT\r\n(Median)\r\n1.951 1.435 3.054 0.507 0.101 1.039 0.385\r\nmetro VOT (1 %) \u2212 1.088 1.759 \u2212 0.189 0.51 0.251 0.22\r\nVOT (25 %) 0.730 2.506 0.312 0.162 0.706 0.377\r\nVOT (75 %) 2.135 4.467 0.664 0.141 1.778 0.661\r\nVOT (99 %) 2.838 9.249 1.252 0.341 4.76 1.653\r\nCar VOT\r\n(Median)\r\n1.529 1.512 2.790 0.33 0.295 1.450 0.29\r\nVOT (1 %) \u2212 0.165 1.160 \u2212 0.335 0.232 0.318 0.255\r\nVOT (25 %) 0.960 2.221 0.11 0.106 0.931 0.29\r\nVOT (75 %) 2.050 4.259 0.607 0.33 2.191 0.463\r\nVOT (99 %) 3.053 9.175 1.046 0.417 5.918 3.126\r\nPredictability: chosen alternative and\r\nprobabilities\r\nTraining accuracy 0.675 0.689 0.673 0.752 0.008 0.716 0.005\r\nTest accuracy 0.657 0.688 0.651 0.719 0.005 0.705 0.01\r\nTraining Brier score 0.453 0.432 0.448 0.344 0.011 0.391 0.005\r\nTest Brier score 0.464 0.447 0.459 0.387 0.003 0.415 0.007\r\nNote: Std. indicates the standard deviation from different datasets generated; The bold indicates the best performance among the benchmark models.\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n21\r\nAppendix H compares the computational efficiency of DCM-DNN and DCM-LN to evaluate the scalability. The main result indicates\r\nthat although the DCM-LN is slightly computationally expensive than DCM-DNN as the lattice size increases, its computational expense\r\nis still reasonable. For instance, the DCM-LN only takes one minute to train the model for the Swissmetro dataset, which has the same\r\norder of observations and attributes as of generally encountered SP choice datasets.\r\n6. Conclusion\r\nWhile marrying emerging data-driven learning approaches with traditional econometric models is a need of this hour, flexibilitydriven predictability gains should not come at the expense of violating theory-driven constraints and losing domain-specific interpretability. To this end, this study proposes a flexible and partially monotonic discrete choice model (DCM), which ensures the\r\nmonotonicity of the utility relative to a subset of alternative-specific attributes while learning non-linear and interaction effects of\r\nattributes in a data-driven manner. The proposed DCM specifies the systematic utility using the lattice networks (LNs) (i.e., DCM-LN,\r\nhenceforth). DCM-LN estimates the attribute-specific non-linear effects as piecewise linear functions and considers their interactions\r\nusing multilinear interpolation. Relatively fewer inequality constraints to ensure partial monotonicity and an automated process to\r\nwrite these constraints in DCM-LN make it scalable and translatable to practice. The proposed LN-based approach has broad appeal to\r\napplied economics because monotonicity is the most intuitive and incontrovertible constraint in economic-related domains (e.g.,\r\nmonotonically decreasing utility relative to cost).\r\nWe benchmark the DCM-LN against DCM-DNN (i.e., DCM with deep neural network-based systematic utility) and a DCM-Linear (i.\r\nFig. 8. VOT estimates for three alternatives for 60 demographic groups in the Swissmetro dataset.\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n22\r\nFig. 9. Attribute-specific utility functions estimated by DCM-Linear, DCM-DNN, and DCM-LN in Swissmetro dataset at the individual and population level: (a) travel time for the train, (b) travel cost for the train, (c) travel time for the Swiss metro, (d) travel cost for the Swiss metro, (e) travel\r\ntime for the car, (f) travel cost for the car.\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n23\r\ne., multinomial logit model with linear additive utility specification considering the first-order interactions) in a Monte Carlo study.\r\nThe results show that DCM-LN highly outperforms both models in terms of interpretability, i.e., recovering willingness to pay at individual and population levels. The visualization of the estimated and true utility functions shows that the DCM-LN outperforms\r\nconsidered models in capturing the underlying non-linear and interaction effects (i.e., taste heterogeneity) of attributes at individual\r\nand population levels while circumventing the main shortcoming of DCM-DNN \u2013 unreasonable or abrupt changes in the utility\r\nfunction, thus avoiding unexpected signs of marginal utility. DCM-LN is slightly outperformed by DCM-DNN in predictability, which is\r\nnot surprising because DCM-LN reduces the parameter search space by imposing monotonicity constraints. This balanced performance\r\nof DCM-LN is quite promising because it approximates the true utility function during the training rather than learning arbitrary utility\r\nfunctions to maximize the predictability as DCM-DNN does. We further validate the proposed DCM-LN in an empirical study. The\r\nresults suggest that DCM-LN provides trustworthy behavioral interpretations compared to DCM-DNN and DCM-Linear. For instance,\r\nwhile DCM-linear and DCM-DNN estimate negative willingness to pay to save travel time in travel mode choice for some demographic\r\ngroups, such unreasonable results are not produced by DCM-LN. In summary, if predictability is the only important factor, then DCMDNN should be preferred. All other policy-relevant scenarios, where recovery of underlying elasticity and willingness to pay distributions is critical, should choose DCM-LN over DCM-DNN and DCM-Linear.\r\nWhile the interest in data-driven DCMs has been exponentially growing with the hope of improving predictability and avoiding the\r\nmisspecification arising from hand-crafting the utility function in theory-driven DCMs, the literature also agrees that existing datadriven DCMs improve predictability but do not provide trustworthy behavioral interpretations. There have been discussions about\r\nachieving interpretability through domain-knowledge-based regularizations in the flexible models (Han et al., 2022; Kim and Bansal,\r\n2023; Sifringer et al., 2020; Wang et al., 2020a), but this is the first study to demonstrate how such regularizations can be applied\r\nthrough constraints in the standard risk minimization framework without losing much of flexibility. This lattice-network-based\r\nframework opens three avenues for future research. First, in addition to hard monotonicity constraints, soft constraints of other\r\nbehavioral theories could be incorporated into the DCM-LN through penalties in the training procedure. Specifically, since DCM-LN\r\ncaptures the attribute-specific non-linear effect as segmented/piece-wise effects, behavioral mechanisms, such as\r\nsemi-compensatory decision rule (e.g., attribute cut-off) (Swait, 2001) and asymmetric marginal utility (e.g., prospect theory) (Van de\r\nKaa, 2010) could be modeled using DCM-LN. Second, the interpretable DCM-LN architecture could be extended to more complex\r\nmodeling objectives. For example, the DCM-LN can be formulated to jointly estimate hierarchical multiple-choice dimensions such as\r\nactivity destination, type, duration, and travel mode in activity-based travel demand models (Castiglione et al., 2014; Kim et al., 2022;\r\nMiller, 2023). The domain knowledge for behavioral mechanisms within/across choice dimensions can reveal the underlying complex\r\ndecision-making process of activity scheduling and inform the necessary constraints. Such models could improve the feasibility and\r\ngeneralizability of generated activity chains by maintaining theory-driven constraints. Third, the individual-level monotonicity is the\r\nmost intuitive and uncontroversial behavioral constraint that is likely to be valid for all decision-makers, and enforcing such constraints in a data-driven manner can help develop a model that approximates the underlying causal relationships, ensuring interpretability rather than fitting the training data to maximize the predictability. However, validating a structural causal model is\r\ninfeasible due to the inability to handle unobserved confounders (i.e., the attribute is correlated with an error term representing the\r\nunobserved data) (Brathwaite and Walker, 2018). Combining DCM-LN with the control function approach could be a potential way\r\nforward to develop a causal DCM.\r\nCRediT authorship contribution statement\r\nEui-Jin Kim: Conceptualization, Data curation, Formal analysis, Software, Writing \u2013 original draft, Methodology. Prateek Bansal:\r\nConceptualization, Funding acquisition, Methodology, Writing \u2013 original draft.\r\nData availability\r\nData will be made available on request.\r\nAcknowledgements\r\nPrateek Bansal acknowledges support from the Presidential Young Professorship Grant, National University of Singapore. This\r\nresearch was conducted at the Future Cities Lab Global at Singapore-ETH Centre. Future Cities Lab Global is supported and funded by\r\nthe National Research Foundation, Prime Minister\u2019s Office, Singapore under its Campus for Research Excellence and Technological\r\nEnterprise (CREATE) programme and ETH Zurich (ETHZ), with additional contributions from the National University of Singapore\r\n(NUS), Nanyang Technological University (NTU), Singapore and the Singapore University of Technology and Design (SUTD). Eui-Jin\r\nKim was supported by a National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (No.RS-2023-\r\n00246523) and by a Korea Agency for Infrastructure Technology Advancement (KAIA) grant funded by the Korean government\r\n(MOLIT) (No.RS-2022-001560).\r\nAppendix A. Bayesian Optimization (BO) for Hyperparameter Tuning of DCM-LN\r\nThe BO for hyperparameter tuning is implemented in four steps: (a) DCM-LN is initialized with a set of hyperparameters; (b) DCMLN is trained for these hyperparameters, and its validation accuracy is saved; (c) a probabilistic surrogate model (Gaussian process (GP)\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n24\r\nin this study) is fitted in which the independent variables are combinations of hyperparameters so far, and the dependent variable is a\r\ncorresponding validation accuracy for each combination of hyperparameters; (d) an acquisition function is maximized to determine\r\nthe next set of hyperparameters which could potentially improve the validation accuracy.\r\nThe GP estimates the relationship between a combination of hyperparameters and the validation accuracy, as denoted by Equation\r\n(A.1), where f is the validation accuracy that we target, C = (C1,\u2026, Ct) is the observed sets of hyperparameters in previous t iterations,\r\n\u03bc and K are mean and covariance of the GP, respectively.\r\nP(f |C) = GP(\r\nf ; \u03bcf |C, Kf |C\r\n)\r\n(A.1)\r\nGiven this GP, the acquisition function evaluates an expected accuracy at the next set of hyperparameters Ct + 1. This study uses the\r\nupper confidence bound acquisition function (Srinivas et al., 2010). The upper confidence bound is a weighted sum of the expected\r\naccuracy captured by the GP\u2019s mean \u03bc(C) and uncertainty captured by the GP\u2019s covariance K(C, C), as shown in Equation (A.2). We\r\nuse the generally recommended \u03b2 = 2.6 (O\u2019Malley et al., 2019). Readers are referred to (Shahriari et al., 2016) for more details on BO.\r\naUCB(C; \u03b2) = \u03bc(C) \u2212 \u03b2 \u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305 trace(K(C, C)) \u221a (A.2)\r\nAppendix B. Sampling Distribution of DGP-Base and DGP-New\r\nTable A.1 describes the sampling distribution of each attribute in DGP-Base and DGP-New. The FUL and FLX are sampled from the\r\nBernoulli distribution with p = 0.5, and the IN is sampled conditional on the FUL from a log-normal distribution. The TC, TT, and WT\r\nhave ranges of (0.2,100), (5100), and (5,30), respectively, and are sampled from the uniform distribution. We assume that the train is\r\nnot affected by crowding due to the reserved seat system; thus, only the CR of the metro is sampled from the uniform distribution,\r\nranging from \u2212 5 to 5.\r\nTable A.1\r\nDescription of input attributes in DGP-Base and DGP-New.\r\nDGP-Base DGP-New Meaning\r\nIndividual\r\nattributes\r\nIN LogNormal(log(0.5),0.25) for full-time;\r\nLogNormal(log(0.25),0.2) for not full-time\r\nLogNormal(log(0.5),0.25) for full-time;\r\nLogNormal(log(0.25),0.2) for not full-time\r\nHousehold income ($ per\r\nmin)\r\nFUL Bernoulli(0.5) Bernoulli(0.5) Full time worker (1=yes,\r\n0=no)\r\nFLX Bernoulli(0.5) Bernoulli(0.5) Flexible working hours\r\n(1=yes, 0=no)\r\nAlternative\r\nattributes\r\nTC Uniform(0.2,100) for train and metro Uniform(0.2,100) for train and metro Travel cost ($)\r\nTT Uniform(5,100) for train and metro Uniform(5,100) for train and metro Travel time (min)\r\nWT Uniform(5,30) for train and metro Uniform(5,30) for train and metro Waiting time (min)\r\nCR \u2013 Uniform(\u2212 5,5) for metro; 0 for train Degree of crowding\r\nNote: DGP-Base is the same as the one considered by Han et al. (2022).\r\nAppendix C. Formula and Interpretation for the Evaluation Metrics\r\nThe interpretability is evaluated by computing RMSE and MAPE, denoted by Eqs. (A.3) and (A.4), where the Vg and V\r\n\u0302g are the true\r\nand estimated VOT or VOWT for the demographic group g.\r\nRMSE =\r\n\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\r\n1\r\n40\r\n\u221140\r\ng=1\r\n(\r\nVg \u2212 \u0302Vg\r\n)2\r\n\u221a\u221a\u221a\u221a (A.3)\r\nMAPE = 100%\r\n40\r\n\u221140\r\ng=1\r\n\u20d2\r\n\u20d2\r\n\u20d2\r\n\u20d2\r\nVg \u2212 \u0302Vg\r\nVg\r\n\u20d2\r\n\u20d2\r\n\u20d2\r\n\u20d2\r\n(A.4)\r\nThe predictability is evaluated by accuracy and Brier score, denoted by Eqs. (A.5) and (A.6).\r\nAccuracy = 1\r\nN\r\n\u2211N\r\nn=1\r\n1Y\u02c6n=Yn (A.5)\r\nBrier Score = 1\r\nN\r\n\u2211N\r\nn=1\r\n\u2211\r\no\r\n(Pon \u2212 Yon)\r\n2 (A.6)\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n25\r\nwhere the 1( \u22c5 ) is an indicator function, which takes value 1 if the observed choice (Yn) is the same as the predicted choice (Y\r\n\u0302n ) for an\r\nindividual n; Pon is the estimated choice probability of the target alternative o for the nthindividual, and Yon is 1 if o is the chosen\r\nalternative and is 0 otherwise. While the accuracy measures how close the estimated choice is to the true one, the Brier score measures\r\nthe accuracy of the estimated choice probability vector. The accurate estimation of the choice probability vector is necessary for\r\ntransportation planning because it is applied in many practical applications, such as the prediction of modal share.\r\nAppendix D. Hyperparameter Tuning for DCM-DNN and DCM-LN in DGP-Base and DGP-New\r\nTable A.2 shows the search space for optimal hyperparameter tuning of DCM-LN and DCM-DNN for DGP-Base and DGP-New. The\r\nlarger lattice size is searched for the DGP-New due to its higher complexity than the DGP-Base. We consider the batch size and the\r\nnumber of epochs to be 128 and 200, respectively. We use the l2 norm for wrinkles and Hessian regularizers with weights of 0.5 and\r\n0.0001, respectively. To reduce the overfitting, we set the callback for early stopping, in which the training stops if the validation\r\naccuracy does not increase during 20 epochs.\r\nTable A.2\r\nSearch space for tuning optimal hyperparameters of DCM-LN and DCM-DNN for DGP-Base and DGP-New.\r\nDGP-Base DGP-New\r\nHyperparameter Search space Hyperparameter Search space\r\nDCM-DNN DCM-DNN\r\nLayer depth [1,2,3,4,5] Layer depth [1,2,3,4,5]\r\nLayer width [50,100,150 200,250,300] Layer width [50,100,150 200,250,300]\r\nDropout rate [0.005,0.05,0.1,0.2,0.3] Dropout rate [0.005,0.05,0.1,0.2,0.3]\r\nLearning rate [0.0005, 0.001, 0.005, 0.01] Learning rate [0.0005, 0.001, 0.005, 0.01]\r\nNumber of model parameters 11,001 Number of model parameters 93,301\r\nDCM-LN DCM-LN\r\nNumber of changing points Number of changing points\r\nTC [10,20,30,40] TC [10,20,30,40]\r\nTT [10,20,30,40] TT [0,20,30,40]\r\nWT [10,20,30,40] WT [10,20,30,40]\r\nCR \u2013 CR [10,20,30,40]\r\nINC [10,20,30,40] INC [10,20,30,40]\r\nOutput calibration [2,3,4,5] Output calibration [2,3,4,5]\r\nLattice size Lattice size\r\nASC [2,3,4,5] ASC [2,3,4,5]\r\nTC [2,3,4,5] TC [2,3,4,5,10,20]\r\nTT [2,3,4,5] TT [2,3,4,5,10,20]\r\nWT [2,3,4,5] WT [2,3,4,5,10,20]\r\nCR \u2013 CR [2,3,4,5,10,20]\r\nINC [2,3,4,5] INC [2,3,4,5,10,20]\r\nFLX [2,3,4,5] FLX [2,3,4,5,10,20]\r\nFUL [2,3,4,5] FUL [2,3,4,5,10,20]\r\nLearning rate [0.0005, 0.001, 0.005, 0.01] Learning rate [0.0005, 0.001, 0.005, 0.01]\r\nNumber of model parameters 4049 Number of model parameters 50,119\r\nNote: Bold indicates the optimal hyperparameters calibrated by Bayesian optimization.\r\nAppendix E. Evaluations Result for DGP-Base\r\nTable A.3 summarizes the evaluation results of the recovery of VOT and VOWT for DGP-Base, where values in bold indicate the best\r\nperformance among the benchmarked models. Table A.4 shows the parameter estimates of DCM-Linear, highlighting the need for\r\nDCMs with a flexible utility specification.\r\nThree main implications are derived from the evaluation results for DGP-Base. First, DCM-LN exhibits the best performance in\r\nrecovering VOT and VOWT distributions and the demographic group-level estimates. Considering a good recovery of model parameters by DCM-Linear (as summarized in Table A.4), the performance of DCM-LN in recovering WTP is impressive. This result implies\r\nthat DCM-LN not only identifies the main and first-order interactions but also successfully uncovers the second-order interactions that\r\nDCM-Linear ignores. Second, DCM-LN only slightly underperforms DCM-DNN and outperforms DCM-Linear in predictability. This\r\nresult is aligned with our hypothesis that DCM-LN can provide trustworthy behavioral interpretations at the expense of a slight\r\ndecrease in predictive performance by approximating the true attribute-specific utility function rather than just fitting it to the data.\r\nThe first two insights also imply a trade-off between interpretability and predictability. If predictability is the only objective, the DCMDNN should be the preferred model due to its superior prediction accuracy even for less complex DGPs like DGP-Base. Third, the\r\nstandard deviation (across 50 synthetic datasets) of VOT and VOWT estimates of DCM-LN is much lower than those estimated by DCMDNN. For example, the standard deviations of quantiles estimated by DCM-LN range from 5 % to 11 % of the mean quantiles, which\r\nrange from 25 % to 32 % for DCM-DNN. This result reflects reasonably stable training of DCM-LN due to constrained parameter space.\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n26\r\nTable A.3\r\nInterpretability and predictability evaluation for DGP-Base.\r\nParameter True DCM-Linear\r\n(50 trials)\r\nDCM-DNN\r\n(50 trials)\r\nDCM-LN\r\n(50 trials)\r\nMean Std. Mean Std. Mean Std. Mean Std.\r\nInterpretability: recovery of distribution VOT (Median) 0.276 0.010 0.306 0.019 0.393 0.104 0.276 0.019\r\nVOT (1 %) 0.142 0.000 0.103 0.004 0.276 0.069 0.156 0.008\r\nVOT (25 %) 0.213 0.002 0.236 0.007 0.356 0.092 0.218 0.012\r\nVOT (75 %) 0.392 0.006 0.403 0.005 0.433 0.115 0.373 0.039\r\nVOT (99 %) 0.656 0.004 0.620 0.006 0.552 0.161 0.550 0.045\r\nVOWT (Median) 0.470 0.020 0.419 0.024 0.836 0.266 0.514 0.045\r\nVOWT (1 %) 0.247 0.000 0.019 0.021 0.502 0.162 0.291 0.033\r\nVOWT (25 %) 0.363 0.009 0.286 0.015 0.702 0.208 0.408 0.044\r\nVOWT (75 %) 0.768 0.012 0.605 0.018 0.988 0.251 0.775 0.088\r\nVOWT (99 %) 1.216 0.006 0.988 0.032 1.280 0.394 1.135 0.102\r\nInterpretability: recovery of demographic groups\u2019 value VOT (MAPE) 0.102 0.002 0.558 0.274 0.091 0.022\r\nVOT (RMSE) 0.029 0.001 0.156 0.067 0.045 0.012\r\nVOWT (MAPE) 0.269 0.030 0.780 0.365 0.128 0.036\r\nVOWT (RMSE) 0.151 0.012 0.383 0.175 0.085 0.026\r\nPredictability: chosen alternative and probabilities Training accuracy 0.716 0.006 0.777 0.007 0.765 0.008\r\nTest accuracy 0.717 0.011 0.748 0.012 0.737 0.013\r\nTraining Brier score 0.372 0.005 0.305 0.008 0.347 0.010\r\nTest Brier score 0.371 0.008 0.339 0.012 0.369 0.010\r\nNote: Std. indicates the standard deviation across synthetic datasets; The bold indicates the best performance among the benchmark models.\r\nWe conduct an in-depth analysis of the results in Table A.3 with Figs. A.1 and A.2. Fig. A.1 shows the demographic group\u2019s VOT and\r\nVOWT estimates for DGP-Base in ascending order of the demographic groups\u2019 true VOT and VOWT. Fig. A.2 shows the utility functions\r\nof TT, WT, and TC. The thick lines in the figure are the population-level utility function (indicated by PD), and the multiple thin lines\r\nare the heterogeneous individual-level utility functions (indicated by ICE). The ICE spread helps in visualizing the extent of interaction\r\neffects or taste heterogeneity. A comparison of PD for all models is provided in Fig. A.2d.\r\nWe observe three insightful patterns in these figures. First, DCM-DNN provides much higher VOT and VOWT estimates for all\r\ndemographic groups in Fig. A.1. This erroneous trend most likely occurs because the estimated utility function of TC (the third plot in\r\nFig. A.2b) is partially flat, leading to the low marginal utility of TC and the overestimation of VOT and VOWT. Second, Fig. A.1 shows\r\nthe highly jagged pattern for DCM-DNN\u2019s estimates of VOT and VOWT (which is expected to be increasing as true values are arranged\r\nin that order), indicating that DCM-DNN cannot provide meaningful behavioral implications for WTP of demographic groups (i.e.,\r\ntaste heterogeneity). This pattern might be a consequence of accounting for non-existing interaction effects of TC by DNN, as indicated\r\nby the DCM-DNN\u2019s non-linear utility function of TC \u2013 leading to erroneous variation in the marginal utility of TC across its domain and\r\ncorresponding WTP estimates. Although such erroneous variations in the marginal utility of TC also exist in DCM-LN, the extent of\r\nvariations is much smaller than those of DCM-DNN. Third, DCM-LN and DCM-Linear well approximate the distribution of VOT and\r\nVOWT for demographic groups. However, better recovery of WTP in DCM-LN compared to that in DCM-Linear could be attributed to\r\nthe former\u2019s capability to capture the second-order interactions ignored by the latter.\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n27\r\nFig. A.1. VOT and VOWT estimates for 40 demographic groups in DGP-Base.\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n28\r\nFig. A.2. Attribute-specific utility functions estimated by (a) DCM-Linear, (b) DCM-DNN, (c) DCM-LN in DGP-Base at the individual and population\r\nlevel, and (d) overlapping population-level utility functions.\r\nTable A.4 reports the mean and standard deviation (across 50 synthetic datasets or trials) of the estimated coefficients and t-statistics of DCM-Linear for DGP-Base and DGP-New. To control the magnitude of the estimated coefficients, we fix the TC\u2019s coefficient of\r\nDCM-Linear as \u20131. While the DGP-Base can describe the true coefficients for each attribute as a single WTP value, the DGP-New cannot\r\ndo so due to the non-linear utility of TC.2 Therefore, we compare the true coefficients with the estimated ones only in DGP-Base.\r\nThe estimation results in DGP-Base show that the DCM-Linear generally provides good estimates for the true coefficients, and tstatistics indicate statistically significant effects for all parameters. However, overlooking the second-order interactions causes biased\r\nestimates for TT \u00d7 FLX and WT \u00d7 FLX, which have second-order interactions in the true DGP-Base. In the DGP-New, the t-statistics of\r\n2 Since the true coefficient of TC (8 \u22c5 \u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305 TCnj \u221a ) in DGP-New is set to have a similar magnitude of utility as in DGP-Base, the estimated coefficients in\r\nDGP-New would have a similar scale to those in DGP-Base.\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n29\r\nall estimated coefficients drastically decrease. In particular, the coefficients for TT and WT are no longer statistically significant, and\r\ntheir t-statistics show large variations across synthetic datasets. Even among other statistically significant coefficients, t-statistics vary\r\nsignificantly across trials. For example, the estimated coefficients for TT \u00d7 FUL and TT \u00d7 CR would be interpreted as significant in\r\nsome trials but not in others. These results verify that the misspecification of utility functions causes unstable and biased estimates for\r\nDGP-New. Such unreliable estimation also causes a sharp decline in model fit measured by Pseudo R2 (0.200 for DGP-Base vs. 0.012 for\r\nDGP-New). These results suggest the need for more flexible models, such as DCM-DNN and DCM-LN, to deal with the complex choice\r\nmechanisms.\r\nTable A.4\r\nEstimation results of DCM-Linear for DGP-Base and DGP-New.\r\nDGP-Base DGP-New\r\nAttribute True DCM-Linear (50 trials) DCM-Linear (50 trials)\r\nCoef. Std (Coef.) t-stat Std (t-stat.) Coef. Std (Coef.) t-stat Std (t-stat.)\r\nConstant \u2212 0.1 \u00a10.158 \u2212 0.051 \u00a13.144 1.000 \u00a11.183 \u2212 0.385 \u00a12.927 0.950\r\nTC \u2212 1 (fixed) \u00a11.000 \u2212 0.027 \u00a136.696 0.404 \u00a11.000 \u2212 0.132 \u00a19.105 0.920\r\nTT \u2212 0.1 \u00a10.089 \u2212 0.005 \u00a118.357 0.917 0.034 \u2212 0.039 1.019 1.185\r\nWT \u2212 0.2 \u00a10.225 \u2212 0.016 \u00a113.394 0.949 0.034 \u2212 0.122 0.269 0.980\r\nCR \u2212 0.2 \u00a10.166 \u2212 0.282 \u00a10.546 0.929\r\nTT \u00d7 IN \u2212 0.5 \u00a10.651 \u2212 0.024 \u00a129.009 0.721 \u00a10.530 \u2212 0.144 \u00a14.347 1.176\r\nTT \u00d7 FUL \u2212 0.1 \u00a10.107 \u2212 0.005 \u00a120.108 0.766 \u00a10.055 \u2212 0.035 \u00a11.551 0.993\r\nTT \u00d7 FLX 0.05 0.120 \u2212 0.004 28.14 0.603 0.095 \u2212 0.020 4.116 0.867\r\nTT \u00d7 CR \u2212 0.02 \u00a10.011 \u2212 0.005 \u00a12.104 1.036\r\nWT \u00d7 INC \u2212 0.8 \u00a11.018 \u2212 0.051 \u00a117.46 0.777 \u00a10.898 \u2212 0.395 \u00a12.156 0.937\r\nWT \u00d7 FUL \u2212 0.3 \u00a10.236 \u2212 0.016 \u00a113.297 0.880 \u00a10.125 \u2212 0.108 \u00a10.949 0.816\r\nWT \u00d7 FLX 0.1 0.280 \u2212 0.015 22.087 0.811 0.207 \u2212 0.088 2.517 1.077\r\nPseudo R2 0.200 (0.003) 0.012 (0.0004)\r\nAppendix F. Estimation Results of DCM-Linear-A, DCM-Linear-B, and DCM-Linear-C\r\nTables A.5\u2013A.7 show the estimation results of DCM-Linear-A, DCM-Linear-B, and DCM-Linear-C, respectively. In DCM-Linear-A, all\r\ncoefficients for alternative attributes are statistically significant at the 5 % level. In DCM-Linear-B, considerable and significant\r\ninteraction effects indicate the presence of systematic taste heterogeneity among different demographic groups. The improved predictability of DCM-Linear-B over DCM-Linear-A suggests the importance of including interactions between alternative- and individualspecific attributes (i.e., taste heterogeneity). DCM-Linear-C shows comparable predictability to DCM-Linear-A but underperforms\r\nDCM-Linear-B, implying that considering the non-linear effect and interaction does not always enhance the predictability. These\r\nresults show that data-driven DCMs could further enhance predictability by capturing the other underlying interactions (e.g., the\r\ninteractions between alternative attributes) and the non-linearity of attribute-specific effects that are ignored by DCM-Linear-B.\r\nTable A.5\r\nEstimated parameters of DCM-Linear-A.\r\nAttributes Train Swiss metro Car (reference)\r\nCoef. t-stat Coef. t-stat Coef. t-stat\r\nConstant 0.216 1.199 0.394 3.598\r\nTravel cost \u00a10.755 \u2212 15.977 \u00a10.755 \u2212 15.977 \u00a10.755 \u2212 15.977\r\nHeadway \u00a10.814 \u2212 5.658 \u2212 0.493 \u2212 1.422\r\nAirline seat 0.759 6.545\r\nAnnual ticket 2.664 10.979 1.429 6.264\r\nTravel time \u00a11.831 \u2212 17.963 \u00a11.474 \u2212 18.65 \u00a11.155 \u2212 19.292\r\nTraining accuracy Test accuracy Training Brier Test Brier\r\nPredictability 0.675 0.657 0.453 0.464\r\nNote: Bold indicates the coefficient is statistically significant at the 10% level.\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n30\r\nAppendix G. Hyperparameter Tuning for DCM-DNN and DCM-LN in Swissmetro Dataset\r\nTable A.8 summarizes the search space for hyperparameters and optimal values for DCM-LN and DCM-DNN for the Swissmetro\r\ndataset. To prevent excessive model complexity and memory requirement, we limit the lattice size for each attribute to below five and\r\nthe number of change points below thirty. The different optimal values of hyperparameters for different alternatives indicate that the\r\nsame attribute affects the utility of different alternatives at different complexity levels. Comparison of the hyperparameters of the\r\nempirical study with those of DGP-Base and DGP-New in the simulation study indicates that the complexity of the Swissmeto dataset\u2019s\r\nDGP could be between those of DGP-Base and DGP-New. The batch size, the number of epochs, and l2 norm weights for wrinkles and\r\nHessian regularizers are set to 128, 200, 0.5, and 0.00001, respectively.\r\nTable A.6\r\nEstimated parameters of DCM-Linear-B.\r\nAttributes Train Swiss metro Car (reference)\r\nCoef. t-stat Coef. t-stat Coef. t-stat\r\nConstant 0.129 0.701 0.299 2.596\r\nTravel cost \u00a10.863 \u2212 17.168 \u00a10.863 \u2212 17.168 \u00a10.863 \u2212 17.168\r\nHeadway \u00a10.844 \u2212 5.773 \u00a10.601 \u2212 1.679\r\nAirline seat 0.717 5.966\r\nAnnual ticket 2.370 9.565 1.091 4.701\r\nTravel time (TT) 0.048 0.196 0.693 1.955 \u2212 0.096 \u2212 0.731\r\nTT \u00d7 Age: 2 \u00a11.957 \u2212 7.032 \u00a11.937 \u2212 4.349 \u00a11.609 \u2212 6.07\r\nTT \u00d7 Age: 3 \u00a11.735 \u2212 6.398 \u00a11.825 \u2212 4.199 \u00a11.394 \u2212 5.521\r\nTT \u00d7 Age: 4 \u00a11.012 \u2212 3.792 \u00a11.157 \u2212 2.728 \u00a10.901 \u2212 3.609\r\nTT \u00d7 Age: 5 \u2212 0.491 \u2212 1.612 \u2212 0.710 \u2212 1.45 \u2212 0.434 \u2212 1.464\r\nTT \u00d7 Income: 2 0.042 0.247 0.501 1.914 0.428 2.384\r\nTT \u00d7 Income: 3 \u00a10.601 \u2212 3.369 \u00a10.100 \u2212 0.367 \u2212 0.147 \u2212 0.797\r\nTT \u00d7 Purpose: 2 \u2212 0.382 \u2212 1.38 \u00a11.095 \u2212 2.613 \u00a10.857 \u2212 2.748\r\nTT \u00d7 Purpose: 3 \u00a10.341 \u2212 2.236 \u00a11.113 \u2212 5.558 \u00a10.269 \u2212 1.784\r\nTT \u00d7 Purpose: 4 \u00a10.363 \u2212 2.268 \u00a11.053 \u2212 5.522 0.056 0.37\r\nTraining accuracy Test accuracy Training Brier Test Brier\r\nPredictability 0.689 0.688 0.432 0.447\r\nNote: Bold indicates the coefficient is statistically significant at the 10 % level.\r\nTable A.7\r\nEstimated parameters of DCM-Linear-C.\r\nAttributes Train Swiss metro Car (reference)\r\nCoef. t-stat Coef. t-stat Coef. t-stat\r\nConstant 0.368 1.224 \u2212 0.001 \u2212 0.003\r\nConstant \u00d7 Male \u00a10.806 \u2212 6.094 \u00a10.217 \u2212 2.590\r\nTravel cost \u00a10.918 \u2212 5.329 \u00a10.749 \u2212 8.510 \u00a10.442 \u2212 3.535 \u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305 Headway \u221a 0.741 1.371 1.168 1.349\r\nLog (Travel time) \u00a13.237 \u2212 22.291 \u00a11.769 \u2212 21.773 \u00a11.768 \u2212 16.877\r\nTT \u00d7 Annual ticket 1.623 5.045 1.358 2.372 \u2212 0.113 \u2212 0.245\r\nTT \u00d7 First class \u2212 0.120 \u2212 0.710 \u00a10.196 \u2212 1.939 \u00a10.573 \u2212 4.615\r\nTT \u00d7 Age: 2 \u00a11.893 \u2212 5.260 \u2212 1.202 \u2212 1.213\r\nTT \u00d7 Age: 3 \u00a11.499 \u2212 4.385 \u2212 1.519 \u2212 1.542\r\nTT \u00d7 Age: 4 \u00a11.101 \u2212 3.115 \u00a12.425 \u2212 2.412\r\nTT \u00d7 Age: 5 \u2212 0.552 \u2212 1.475 \u00a14.456 \u2212 4.016\r\nTraining accuracy Test accuracy Training Brier Test Brier\r\nPredictability 0.673 0.650 0.448 0.459\r\nNote: Bold indicates the coefficient is statistically significant at the 10 % level. While the original model specification for travel cost was piecewise\r\nlinear in Ortelli et al. (2021), the details were missing in that study; thus, we consider the effect of travel cost as linear.\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n31\r\nTable A.8\r\nSearch space for tuning optimal hyperparameters of DCM-LN and DCM-DNN\r\nfor the Swissmetro dataset.\r\nSwissmetro dataset\r\nHyperparameter Search space\r\nDCM-DNN\r\nLayer depth [1,2,3,4,5]\r\nLayer width (N) [50,100,150, 200,250,300]\r\nDropout rate [0.005,0.05,0.1,0.2,0.3]\r\nLearning rate [0.0005, 0.001, 0.005, 0.01]\r\nNumber of model parameters 143,803\r\nDCM-LN\r\nNumber of changing points\r\nTravel time (Train) [10,20,30]\r\nTravel time (Swissmetro) [10,20,30]\r\nTravel time (Car) [10,20,30]\r\nTravel cost (Train) [10,20,30]\r\nTravel cost (Swissmetro) [10,20,30]\r\nTravel cost (Car) [10,20,30]\r\nHeadway (Train) [10,20,30]\r\nHeadway (Swissmetro) [10,20,30]\r\nOutput calibration (Train) [2,3,4]\r\nOutput calibration (Swissmetro) [2,3,4]\r\nOutput calibration (Car) [2,3,4]\r\nLattice size\r\nTravel time (Train) [2,3,4]\r\nTravel time (Swissmetro) [2,3,4]\r\nTravel time (Car) [2,3,4]\r\nTravel cost (Train) [2,3,4]\r\nTravel cost (Swissmetro) [2,3,4]\r\nTravel cost (Car) [2,3,4]\r\nHeadway (Train) [2,3,4]\r\nHeadway (Swissmetro) [2,3,4]\r\nAirline seat (Swissmetro) [2,3,4]\r\nAge (All) [2,3,4]\r\nIncome (All) [2,3,4]\r\nPurpose (All) [2,3,4]\r\nLearning rate [0.0005, 0.001, 0.005, 0.01]\r\nNumber of model parameters 6296\r\nNote: Bold indicates the optimal hyperparameters calibrated by Bayesian\r\noptimization.\r\nAppendix H. Computational expense\r\nThe number of parameters of the model is a well-known measure of model complexity; however, the constrained DCM-LN have\r\ndifferent convergence characteristics to the conventional unconstrained DCM-DNN because the inequality constraints can affect model\r\nconvergence. Therefore, we evaluate the computational expense by comparing the actual training time until the convergence of DCMDNN and DCM-LN. Also, we examine the changes in computational time according to the lattice size, which is the most critical hyperparameter affecting computational complexity.\r\nTable A.9 shows the actual training time of DCM-DNN and DCM-LN until the convergence, which is obtained on a CPU of 12th Gen\r\nIntel Core i9\u201312900HK 2.50 GHz with 32 GB of RAM. For the tuned models, the DCM-LN takes two times more than the DCM-DNN for\r\ntraining until convergence, and the standard deviation of training time is also larger in DCM-LN. Despite the learning rate of DCM-LN is\r\nten times higher than DCM-DNN, the number of epochs until convergence is comparable, and its standard deviation is much higher in\r\nDCM-LN. These results indicate that the computational expense of DCM-LN is generally higher than that of DCM-DNN due to the\r\nstructure of the lattice. Also, the high volatility of the computational expense in DCM-LN may be attributed to sub-optimal gradient\r\nupdate due to the projection for monotonicity constraints (See Section 3.3). Nevertheless, the DCM-LN only takes one minute to train\r\nthe model for the Swissmetro dataset, which has the same order of observations and attributes as of generally encountered SP choice\r\ndatasets. This result indicates the DCM-LN is likely to be scalable for most SP choice datasets, even with a general personal computer.\r\nThe change in computational time according to the lattice size is a good measure of the scalability of the DCM-LN. We arbitrarily\r\nincrease the lattice size for 8 alternative-specific attributes (i.e., travel time, travel cost, and headway for each alternative) from the\r\ntuned DCM-LN ([4,4,4,4,2,4,2,2]) and measure the training time and number of epochs. For example, the DCM-LN (+1) has a lattice\r\nsize of [5,5,5,5,3,5,3,3]. As expected, the training time until convergence increases substantially with the increase in Lattice size, but\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n32\r\nthe number of epochs until convergence is maintained. This result indicates that the model can be stably converged despite the\r\nincreasing computational expenses to estimate the complex utility function. In other words, the DCM-LN is scalable for the choice\r\ndataset that requires inferring more complex utility functions than the Swissmetro dataset.\r\nTable A.9\r\nActual training time until the convergence with 50 trials.\r\nMetric DCM-DNN (Tuned) DCM-LN (Tuned) DCM-LN (+1) DCM-LN (+2)\r\nMean Std Mean Std Mean Std Mean Std\r\nTraining time until the convergence (sec) 27.88 6.50 61.36 23.33 123.03 31.51 148.61 34.55\r\nNumber of epochs until convergence (times) 43.66 5.62 50.76 16.08 51.03 13.45 49.76 10.37\r\nNote: The callback for early stopping is set, in which the training stops if the validation accuracy does not increase during 20 epochs, and the early\r\nstopping point indicate the convergence. The learning rate of DCM-DNN and DCM-LN are 0.0005 and 0.005, respectively.\r\nReferences\r\nAlwosheel, A., van Cranenburgh, S., Chorus, C.G., 2021. Why did you predict that? Towards explainable artificial neural networks for travel demand analysis. Transp.\r\nRes. Part C. Emerg. Technol. 128 https://doi.org/10.1016/j.trc.2021.103143.\r\nArcher, N.P., Wang, S., 1993. Application of the back propagation neural network algorithm with monotonicity constraints for two-group classification problems.\r\nDecis. Sci. 24, 60\u201375. https://doi.org/10.1111/j.1540-5915.1993.tb00462.x.\r\nArkoudi, I., Krueger, R., Azevedo, C.L., Pereira, F.C., 2023. Combining discrete choice models and neural networks through embeddings: formulation, interpretability\r\nand performance. Transp. Res. Part B Methodol. 175, 102783 https://doi.org/10.1016/j.trb.2023.102783.\r\nAxhausen, K.W., Konig, \u00a8 A., Abay, G., Bates, J.J., Bierlaire, M., 2004. Swiss Value of Travel Time Savings. In: Proceedings of the European Transport Conference 2004\r\n(ETC 2004). doi:10.3929/ethz-b-000023559.\r\nAxhausen, K.W., Hess, S., Konig, \u00a8 A., Abay, G., Bates, J., Bierlaire, M., Axhausen, K.W., Hess, S., Konig, \u00a8 A., Abay, G., Bates, J.J., 2007. State of the art estimates of the\r\nSwiss value of travel time savings. In: Proceedings of the 86th Annual Meeting of the Transportation Research Board. https://doi.org/10.3929/ethz-a-005240029.\r\nBansal, P., Daziano, R.A., Sunder, N., 2019. Arriving at a decision: a semi-parametric approach to institutional birth choice in India. J. Choice Model. 31, 86\u2013103.\r\nhttps://doi.org/10.1016/j.jocm.2019.04.001.\r\nBansal, P., Kumar, R.R., Raj, A., Dubey, S., Graham, D.J., 2021. Willingness to pay and attitudinal preferences of Indian consumers for electric vehicles. Energy Econ.\r\n100 https://doi.org/10.1016/j.eneco.2021.105340.\r\nBansal, P., Horcher, \u00a8 D., Graham, D.J., 2022. A dynamic choice model to estimate the user cost of crowding with large-scale transit data. J. R. Stat. Soc. Ser. A (Stat.\r\nSoc.) 1\u201325. https://doi.org/10.1111/rssa.12804.\r\nBarlow, R.E., Bartholomew, D.J., Bremner, J.M., Brunk, H.D., 1974. Statistical inference under order restrictions. The theory and application of isotonic regression.\r\nJ. R. Stat. Soc. Ser. A 137, 92. https://doi.org/10.2307/2345150.\r\nBarredo Arrieta, A., D\u00edaz-Rodr\u00edguez, N., del Ser, J., Bennetot, A., Tabik, S., Barbado, A., Garcia, S., Gil-Lopez, S., Molina, D., Benjamins, R., Chatila, R., Herrera, F.,\r\n2020. Explainable Artificial Intelligence (XAI): concepts, taxonomies, opportunities and challenges toward responsible AI. Inf. Fusion 58, 82\u2013115. https://doi.\r\norg/10.1016/j.inffus.2019.12.012.\r\nBatarce, M., Munoz, \u02dc J.C., de Ort\u00fazar, J.D., 2016. Valuing crowding in public transport: implications for cost-benefit analysis. Transp. Res. Part A Policy Pract. 91,\r\n358\u2013378. https://doi.org/10.1016/j.tra.2016.06.025.\r\nBierlaire, M., Axhausen, K., Abay, G., 2001. The acceptance of modal innovation: the case of Swissmetro. In: Proceedings of the 1st Swiss Transport Research\r\nConference (STRC 2001). doi:10.3929/ethz-a-004238511.\r\nBierlaire, M., 2018. Swissmetro [WWW Document]. URL https://transp-or.epfl.ch/documents/technicalReports/CS_SwissmetroDescription.pdf (accessed 10.14.22).\r\nBrathwaite, T., Walker, J.L., 2018. Causal inference in travel demand modeling (and the lack thereof). J. Choice Model. 26, 1\u201318. https://doi.org/10.1016/j.\r\njocm.2017.12.001.\r\nBrownstone, D., Ghosh, A., Golob, T.F., Kazimi, C., van Amelsfort, D., 2003. Drivers\u2019 willingness-to-pay to reduce travel time: evidence from the San Diego I-15\r\ncongestion pricing project. Transp. Res. Part A Policy Pract. 37, 373\u2013387. https://doi.org/10.1016/S0965-8564(02)00021-6.\r\nCastiglione, J., Bradley, M., Gliebe, J., 2014. Activity-Based Travel Demand Models: A Primer, Activity-Based Travel Demand Models: A Primer. Transportation\r\nResearch Board, Washington, D.C.. https://doi.org/10.17226/22357\r\nCybenkot, G., 1989. Mathematics of control, signals, and systems approximation by superpositions of a sigmoidal function. Math. Control Signal. 2, 303\u2013314.\r\nDaly, A., Sanko, N., Wardman, M., 2017. Cost and time damping: evidence from aggregate rail direct demand models. Transportation 44, 1499\u20131517. https://doi.org/\r\n10.1007/s11116-016-9711-9 (Amst).\r\nDaniels, H., Velikova, M., 2010. Monotone and partially monotone neural networks. IEEE Trans. Neural Netw. 21, 906\u2013917. https://doi.org/10.1109/\r\nTNN.2010.2044803.\r\nDubey, S., Cats, O., Hoogendoorn, S., Bansal, P., 2022. A multinomial probit model with Choquet integral and attribute cut-offs. Transp. Res. Part B Methodol. 158,\r\n140\u2013163. https://doi.org/10.1016/j.trb.2022.02.007.\r\nFriedman, J.H., 2001. Greedy function approximation: a gradient boosting machine. Ann. Stat. 29, 225\u2013244. https://doi.org/10.1214/aos/1013203451.\r\nFukuda, D., Yai, T., 2010. Semiparametric specification of the utility function in a travel mode choice model. Transportation 37, 221\u2013238. https://doi.org/10.1007/\r\ns11116-009-9253-5 (Amst).\r\nGoldstein, A., Kapelner, A., Bleich, J., Pitkin, E., 2015. Peeking inside the black box: visualizing statistical learning with plots of individual conditional expectation.\r\nJ. Comput. Graph. Stat. 24, 44\u201365. https://doi.org/10.1080/10618600.2014.907095.\r\nGunning, D., Stefik, M., Choi, J., Miller, T., Stumpf, S., Yang, G.Z., 2019. XAI-Explainable artificial intelligence. Sci. Robot. 4 https://doi.org/10.1126/scirobotics.\r\naay7120.\r\nGupta, M., Cotter, A., Pfeifer, J., Voevodski, K., Canini, K., Mangylov, A., Moczydlowski, W., Van Esbroeck, A., 2016. Monotonic calibrated interpolated look-up\r\ntables. J. Mach. Learn. Res. 17, 1\u201347. https://doi.org/10.5555/2946645.3007062.\r\nHan, Y., Pereira, F.C., Ben-Akiva, M., Zegras, C., 2022. A neural-embedded discrete choice model: learning taste representation with strengthened interpretability.\r\nTransp. Res. Part B Methodol. 163, 166\u2013186. https://doi.org/10.1016/j.trb.2022.07.001.\r\nHastie, T., Tibshirani, R., 1987. Generalized additive models: some applications. J. Am. Stat. Assoc. 82, 371\u2013386. https://doi.org/10.1080/\r\n01621459.1987.10478440.\r\nHernandez, J.I., van Cranenburgh, S., Chorus, C., Mouter, N., 2023. Data-driven assisted model specification for complex choice experiments data: association rules\r\nlearning and random forests for participatory value evaluation experiments. J. Choice Model. 46 https://doi.org/10.1016/j.jocm.2022.100397.\r\nE.-J. Kim and P. Bansal\r\nTransportation Research Part B 183 (2024) 102947\r\n33\r\nHo, C.Q., Mulley, C., Hensher, D.A., 2020. Public preferences for mobility as a service: insights from stated preference surveys. Transp. Res. Part A Policy Pract. 131,\r\n70\u201390. https://doi.org/10.1016/j.tra.2019.09.031.\r\nJi, S., Wang, X., Lyu, T., Liu, X., Wang, Y., Heinen, E., Sun, Z., 2022. Understanding cycling distance according to the prediction of the XGBoost and the interpretation\r\nof SHAP: a non-linear and interaction effect analysis. J. Transp. Geogr. 103 https://doi.org/10.1016/j.jtrangeo.2022.103414.\r\nKim, E.J., Bansal, P., 2023. A deep generative model for feasible and diverse population synthesis. Transp. Res. Part C Emerg. Technol. 148 https://doi.org/10.1016/j.\r\ntrc.2023.104053.\r\nKim, E.J., Kim, Y., Kim, D.K., 2020. Interpretable machine learning models for estimating trip purpose in smart card data. Proc. Inst. Civ. Eng. Munic. Eng. 1\u201322.\r\nhttps://doi.org/10.1680/jmuen.20.00003.\r\nKim, E., Kim, Y., Jang, S., Kim, D., 2021a. Tourists\u2019 preference on the combination of travel modes under Mobility-as-a-Service environment. Transp. Res. Part A 150,\r\n236\u2013255. https://doi.org/10.1016/j.tra.2021.06.016.\r\nKim, Y., Kim, E., Jang, S., Kim, D., 2021b. A comparative analysis of the users of private cars and public transportation for intermodal options under Mobility-as-aService in Seoul. Travel. Behav. Soc. 24, 68\u201380. https://doi.org/10.1016/j.tbs.2021.03.001.\r\nKim, E.J., Kim, D.K., Sohn, K., 2022. Imputing qualitative attributes for trip chains extracted from smart card data using a conditional generative adversarial network.\r\nTransp. Res. Part C Emerg. Technol. 137 https://doi.org/10.1016/j.trc.2022.103616.\r\nKim, E.J., 2021. Analysis of travel mode choice in seoul using an interpretable machine learning approach. J. Adv. Transp. 2021, 1\u201313. https://doi.org/10.1155/\r\n2021/6685004.\r\nLipton, Z.C., 2018. The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery. Queue 16, 31\u201357.\r\nhttps://doi.org/10.1145/3236386.3241340.\r\nLiu, X., Han, X., Zhang, N., Liu, Q., 2020. Certified Monotonic Neural Networks. In: Proceedings of the 34th Conference on Neural Information Processing Systems.\r\nLundberg, S.M., Allen, P.G., Lee, S.I., 2017. A unified approach to interpreting model predictions. In: Proceedings of the 31st Conference on Neural Information\r\nProcessing Systems.\r\nMcFadden, D., 1973. Conditional logit analysis of qualitative choice behavior. In: Zaremmbka, P. (Ed.), Frontiers in Econometrics. Academic Press, New York.\r\nMiller, T., 2019. Explanation in artificial intelligence: insights from the social sciences. Artif. Intell. https://doi.org/10.1016/j.artint.2018.07.007.\r\nMiller, E., 2023. The current state of activity-based travel demand modelling and some possible next steps. Transp. Rev. 43, 565\u2013570. https://doi.org/10.1080/\r\n01441647.2023.2198458.\r\nO\u2019Malley, T., Bursztein, E., Long, J., Chollet, F., Jin, H., Invernizzi, L., 2019. Keras Tuner [WWW Document]. https://github.com/keras-team/keras-tuner.\r\nOrtelli, N., Hillel, T., Pereira, F.C., de Lapparent, M., Bierlaire, M., 2021. Assisted specification of discrete choice models. J. Choice Model. 39 https://doi.org/\r\n10.1016/j.jocm.2021.100285.\r\nQu, Y.J., Hu, B.G., 2011. Generalized constraint neural network regression model subject to linear priors. IEEE Trans. Neural Netw. 22, 2447\u20132459. https://doi.org/\r\n10.1109/TNN.2011.2167348.\r\nRibeiro, M.T., Singh, S., Guestrin, C., 2016. Why should i trust you?\u201d Explaining the predictions of any classifier. In: Proceedings of the ACM SIGKDD International\r\nConference on Knowledge Discovery and Data Mining. Association for Computing Machinery, pp. 1135\u20131144. https://doi.org/10.1145/2939672.2939778.\r\nRich, J., Mabit, S.L., 2016. Cost damping and functional form in transport models. Transportation 43, 889\u2013912. https://doi.org/10.1007/s11116-015-9628-8 (Amst).\r\nRodrigues, F., Ortelli, N., Bierlaire, M., Pereira, F.C., 2022. Bayesian automatic relevance determination for utility function specification in discrete choice models.\r\nIEEE Trans. Intell. Transp. Syst. 23, 3126\u20133136. https://doi.org/10.1109/TITS.2020.3031965.\r\nShahriari, B., Swersky, K., Wang, Z., Adams, R.P., de Freitas, N., 2016. Taking the human out of the loop: a review of Bayesian optimization. Proc. IEEE. https://doi.\r\norg/10.1109/JPROC.2015.2494218.\r\nSifringer, B., Lurkin, V., Alahi, A., 2020. Enhancing discrete choice models with representation learning. Transp. Res. Part B Methodol. 140, 236\u2013261. https://doi.org/\r\n10.1016/j.trb.2020.08.006.\r\nSill, J., 1997. Monotonic networks. In: Proceedings of the 11th Conference on Neural Information Processing Systems, pp. 661\u2013667.\r\nSrinivas, N., Krause, A., Kakade, S.M., Seeger, M., 2010. Gaussian process optimization in the bandit setting: no regret and experimental design. In: Proceedings of the\r\n27th International Conference on Machine Learning, pp. 1015\u20131022.\r\nSwait, J., 2001. A non-compensatory choice model incorporating attribute cutoffs. Transp. Res. Part B Methodol. 35, 903\u2013928. https://doi.org/10.1016/S0191-2615\r\n(00)00030-8.\r\nTrain, K.E., 2009. Discrete Choice Methods with Simulation. Cambridge University Press. https://doi.org/10.1017/CBO9780511805271.\r\nvan Cranenburgh, S., Wang, S., Vij, A., Pereira, F., Walker, J., 2022. Choice modelling in the age of machine learning\u2013discussion paper. J. Choice Model. 42, 100340\r\nhttps://doi.org/10.1016/j.jocm.2021.100340.\r\nVan de Kaa, E.J., 2010. Applicability of an extended prospect theory to travel behaviour research: a meta-analysis. Transp. Rev. 30, 771\u2013804. https://doi.org/\r\n10.1080/01441647.2010.486907.\r\nWang, S., Mo, B., Zhao, J., 2020a. Deep neural networks for choice analysis: architecture design with alternative-specific utility functions. Transp. Res. Part C Emerg.\r\nTechnol. 112, 234\u2013251. https://doi.org/10.1016/j.trc.2020.01.012.\r\nWang, S., Wang, Q., Zhao, J., 2020b. Deep neural networks for choice analysis: extracting complete economic information for interpretation. Transp. Res. Part C\r\nEmerg. Technol. 118, 102701 https://doi.org/10.1016/j.trc.2020.102701.\r\nWang, S., Mo, B., Zhao, J., 2021. Theory-based residual neural networks: a synergy of discrete choice models and deep neural networks. Transp. Res. Part B Methodol.\r\n146, 333\u2013358. https://doi.org/10.1016/j.trb.2021.03.002.\r\nWong, M., Farooq, B., 2021. ResLogit: a residual neural network logit model for data-driven choice modelling. Transp. Res. Part C Emerg. Technol. 126, 103050\r\nhttps://doi.org/10.1016/j.trc.2021.103050.\r\nYou, S., Ding, D., Canini, K., Pfeifer, J., Gupta, M., 2017. Deep Lattice Networks and Partial Monotonic Functions. In: Proceedings of the 31st Conference on Neural\r\nInformation Processing Systems.\r\nZhao, X., Yan, X., Yu, A., van Hentenryck, P., 2020. Prediction and behavioral analysis of travel mode choice: a comparison of machine learning and logit models.\r\nTravel Behav. Soc. 20, 22\u201335. https://doi.org/10.1016/j.tbs.2020.02.003.\r\nE.-J. Kim and P. Bansal\nCode:\n#!/usr/bin/env python\r\n# coding: utf-8\r\n\r\n# In[ ]:\r\n\r\n\r\n## Python basics\r\nimport os\r\nimport numpy as np\r\nimport scipy\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport random\r\nfrom IPython.core.pylabtools import figsize\r\nfrom time import time\r\nfrom scipy.stats import norm, uniform\r\nimport math\r\nimport collections\r\nimport collections.abc\r\nfrom collections import OrderedDict    # For recording the model specification \r\ncollections.Iterable = collections.abc.Iterable\r\nimport pylogit as pl  \r\nimport lxml\r\n\r\n## DNN\r\nimport tensorflow as tf\r\nimport tensorflow_lattice as tfl\r\nfrom scikeras.wrappers import KerasRegressor, KerasClassifier\r\nfrom tensorflow.keras.layers import Activation,Input, Dense, Reshape, Concatenate, Layer, Dropout\r\nfrom tensorflow.keras.layers import BatchNormalization, Activation, Embedding, Flatten,LeakyReLU,ReLU\r\nfrom tensorflow.keras.models import Sequential, Model\r\nfrom tensorflow.keras.optimizers import RMSprop, Adam\r\n\r\nfrom sklearn.metrics import accuracy_score,f1_score\r\nfrom statsmodels.formula.api import logit\r\nimport statsmodels.api as sm\r\nfrom sklearn.base import BaseEstimator, ClassifierMixin\r\nfrom sklearn.utils.multiclass import unique_labels\r\nimport torch.nn.functional as F\r\nimport torch\r\nfrom torch import tensor\r\nimport torch.distributions.log_normal as log_normal\r\nimport torch.distributions.bernoulli as bernoulli \r\n\r\n\r\n## PDP\r\nfrom sklearn.inspection import PartialDependenceDisplay, partial_dependence\r\nfrom sklearn.linear_model import LogisticRegression\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.metrics import mean_absolute_error,mean_absolute_percentage_error,accuracy_score,mean_squared_error\r\nfrom tensorflow.keras import regularizers\r\nfrom sklearn.metrics import brier_score_loss\r\n\r\n\r\n# Get the prediction metrics\r\ndef brier_multi(targets, probs):\r\n     return np.mean(np.sum((probs - targets)**2, axis=1))\r\ndef save_clipboard(X):\r\n    pd.DataFrame(X).to_clipboard(index=False,header=False)\r\nnp.set_printoptions(suppress=True)\r\n\r\npd.options.mode.chained_assignment = None  # default='warn'\r\n\r\ndef compute_VOT_POP(X,Y):\r\n    VOT_temp = []\r\n    for i in range(X.shape[0]):\r\n        for j in range(Y.shape[0]):\r\n            VOT_temp.append(X[i]/Y[j])\r\n    return np.array(VOT_temp)\r\n\r\ndef compute_VOT_IND(X,Y):\r\n    VOT_temp = []\r\n    for i in range(X.shape[0]):\r\n        #VOT_temp.append(compute_VOT_POP(X[i,:],Y[i,:]).mean())\r\n         VOT_temp.append(np.median(compute_VOT_POP(X[i,:],Y[i,:])))\r\n    return np.array(VOT_temp)\r\n\r\nimport warnings\r\n\r\n#suppress warnings\r\nwarnings.filterwarnings('ignore')\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"    \r\nimport tensorflow as tf\r\n\r\n# gpu = tf.config.experimental.get_visible_devices('GPU')[0]\r\n# tf.config.experimental.set_memory_growth(device = gpu, enable = True)\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\ndf_wide = pd.read_csv(\"data/data_METRO.csv\")\r\ndf_wide = df_wide[(df_wide.AGE != 6) & (df_wide.CHOICE != 0) & (df_wide.INCOME != 4) & (df_wide.PURPOSE != 9) & (df_wide.CAR_AV != 0)]\r\n\r\n# Create AGE dummy\r\ndf_wide[\"AGES\"] = 0\r\ndf_wide.loc[df_wide[\"AGE\"] == 1,\"AGES\"] = 1\r\ndf_wide.loc[df_wide[\"AGE\"] == 2,\"AGES\"] = 2\r\ndf_wide.loc[df_wide[\"AGE\"] == 3,\"AGES\"] = 3\r\ndf_wide.loc[df_wide[\"AGE\"] == 4,\"AGES\"] = 4\r\ndf_wide.loc[df_wide[\"AGE\"] == 5,\"AGES\"] = 5\r\n\r\n# Create INCOME dummy\r\ndf_wide[\"INCOMES\"] = 0\r\ndf_wide.loc[df_wide[\"INCOME\"].isin([0,1]),\"INCOMES\"] = 1\r\ndf_wide.loc[df_wide[\"INCOME\"] == 2,\"INCOMES\"] = 2\r\ndf_wide.loc[df_wide[\"INCOME\"] == 3,\"INCOMES\"] = 3\r\ndf_wide.loc[df_wide[\"INCOME\"] == 4,\"INCOMES\"] = 4\r\n\r\n\r\n# Create PURPOSE dummy\r\ndf_wide[\"PURPOSES\"] = 0\r\ndf_wide.loc[df_wide[\"PURPOSE\"].isin([1,5]),\"PURPOSES\"] = 1\r\ndf_wide.loc[df_wide[\"PURPOSE\"].isin([2,6]),\"PURPOSES\"] = 2\r\ndf_wide.loc[df_wide[\"PURPOSE\"].isin([3,7]),\"PURPOSES\"] = 3\r\ndf_wide.loc[df_wide[\"PURPOSE\"].isin([4,8]),\"PURPOSES\"] = 4\r\n\r\n\r\n\r\n# Scale the travel time column by 60 to convert raw units (minutes) to hours\r\ndf_wide[\"TRAIN_TT\"] = df_wide[\"TRAIN_TT\"] / 100.0\r\ndf_wide[\"SM_TT\"] = df_wide[\"SM_TT\"] / 100.0\r\ndf_wide[\"CAR_TT\"] = df_wide[\"CAR_TT\"] / 100.0\r\n\r\n# Scale the headway column by 60 to convert raw units (minutes) to hours\r\ndf_wide[\"TRAIN_HE\"] = df_wide[\"TRAIN_HE\"] / 100.0\r\ndf_wide[\"SM_HE\"] = df_wide[\"SM_HE\"] / 100.0\r\n\r\ndf_wide[\"TRAIN_TC\"] = df_wide[\"TRAIN_CO\"] / 100.0\r\ndf_wide[\"SM_TC\"] = df_wide[\"SM_CO\"] / 100.0\r\ndf_wide[\"CAR_TC\"] = df_wide[\"CAR_CO\"] / 100.0\r\n\r\ndf_wide.loc[(df_wide[\"GA\"] == 1),\"TRAIN_TC\"] = 0\r\ndf_wide.loc[(df_wide[\"GA\"] == 1),\"SM_TC\"] = 0\r\n\r\n\r\n##########\r\n# Create various dummy variables to describe the choice context of a given\r\n# invidual for each choice task.\r\n##########\r\n\r\n# Create AGE dummy\r\n#df_wide[\"AGE_1\"] = (df_wide[\"AGE\"] == 1).astype(int)\r\ndf_wide[\"AGE_2\"] = (df_wide[\"AGE\"] == 2).astype(int)\r\ndf_wide[\"AGE_3\"] = (df_wide[\"AGE\"] == 3).astype(int)\r\ndf_wide[\"AGE_4\"] = (df_wide[\"AGE\"] == 4).astype(int)\r\ndf_wide[\"AGE_5\"] = (df_wide[\"AGE\"] == 5).astype(int)\r\n\r\n# Create INCOME dummy\r\n#df_wide[\"INCOME_01\"] = (df_wide[\"INCOME\"].isin([0,1])).astype(int)\r\ndf_wide[\"INCOME_2\"] = (df_wide[\"INCOME\"] == 2).astype(int)\r\ndf_wide[\"INCOME_3\"] = (df_wide[\"INCOME\"] == 3).astype(int)\r\ndf_wide[\"INCOME_4\"] = (df_wide[\"INCOME\"] == 4).astype(int)\r\n\r\n# Create PURPOSE dummy\r\n#df_wide[\"PURPOSE_1\"] = (df_wide[\"PURPOSE\"].isin([1,5])).astype(int)\r\ndf_wide[\"PURPOSE_2\"] = (df_wide[\"PURPOSE\"].isin([2,6])).astype(int)\r\ndf_wide[\"PURPOSE_3\"] = (df_wide[\"PURPOSE\"].isin([3,7])).astype(int)\r\ndf_wide[\"PURPOSE_4\"] = (df_wide[\"PURPOSE\"].isin([4,8])).astype(int)\r\n\r\n# Create LUGGAGE dummy\r\n#df_wide[\"LUGGAGE_0\"] = (df_wide[\"LUGGAGE\"] == 0).astype(int)\r\ndf_wide[\"LUGGAGE_1\"] = (df_wide[\"LUGGAGE\"] == 1).astype(int)\r\ndf_wide[\"LUGGAGE_3\"] = (df_wide[\"LUGGAGE\"] == 3).astype(int)\r\n\r\n# Create a dummy variable indicating that a person is NOT first class\r\ndf_wide[\"TRAIN_CLASS\"] = 1 - df_wide[\"FIRST\"]\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\ndf_wide['TRAIN_TT']\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n# Create the list of individual specific variables\r\nind_variables = ['ID','GA','AGE_2', 'AGE_3', 'AGE_4',\r\n       'AGE_5', 'INCOME_2', 'INCOME_3', 'INCOME_4', 'PURPOSE_2', 'PURPOSE_3',\r\n       'PURPOSE_4','AGES', 'INCOMES','PURPOSES']\r\n\r\n\r\n# Specify the variables that vary across individuals and some or all alternatives\r\n# The keys are the column names that will be used in the long format dataframe.\r\n# The values are dictionaries whose key-value pairs are the alternative id and\r\n# the column name of the corresponding column that encodes that variable for\r\n# the given alternative. Examples below.\r\nalt_varying_variables = {u'TT': dict([(1, 'TRAIN_TT'),\r\n                                               (2, 'SM_TT'),\r\n                                               (3, 'CAR_TT')]),\r\n                          u'TC': dict([(1, 'TRAIN_TC'),\r\n                                                (2, 'SM_TC'),\r\n                                                (3, 'CAR_TC')]),\r\n                          u'HE': dict([(1, 'TRAIN_HE'),\r\n                                            (2, 'SM_HE')]),\r\n                          u'SEAT': dict([(2, \"SM_SEATS\")])}\r\n\r\n# Specify the availability variables\r\n# Note that the keys of the dictionary are the alternative id's.\r\n# The values are the columns denoting the availability for the\r\n# given mode in the dataset.\r\navailability_variables = {1: 'TRAIN_AV',\r\n                          2: 'SM_AV', \r\n                          3: 'CAR_AV'}\r\n\r\n##########\r\n# Determine the columns for: alternative ids, the observation ids and the choice\r\n##########\r\n# The 'custom_alt_id' is the name of a column to be created in the long-format data\r\n# It will identify the alternative associated with each row.\r\ncustom_alt_id = \"mode_id\"\r\n\r\n# Create a custom id column that ignores the fact that this is a \r\n# panel/repeated-observations dataset. Note the +1 ensures the id's start at one.\r\nobs_id_column = \"custom_id\"\r\ndf_wide[obs_id_column] = np.arange(df_wide.shape[0], dtype=int) + 1\r\n\r\n# Create a variable recording the choice column\r\nchoice_column = \"CHOICE\"\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n# Perform the conversion to long-format\r\ndf_long = pl.convert_wide_to_long(df_wide, \r\n                                           ind_variables, \r\n                                           alt_varying_variables, \r\n                                           availability_variables, \r\n                                           obs_id_column, \r\n                                           choice_column,\r\n                                           new_alt_id_name=custom_alt_id)\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\ndf_long[\"TTxAGE_2\"] = df_long[\"TT\"]*df_long[\"AGE_2\"]\r\ndf_long[\"TTxAGE_3\"] = df_long[\"TT\"]*df_long[\"AGE_3\"]\r\ndf_long[\"TTxAGE_4\"] = df_long[\"TT\"]*df_long[\"AGE_4\"]\r\ndf_long[\"TTxAGE_5\"] = df_long[\"TT\"]*df_long[\"AGE_5\"]\r\n\r\ndf_long[\"TTxINCOME_2\"] = df_long[\"TT\"]*df_long[\"INCOME_2\"]\r\ndf_long[\"TTxINCOME_3\"] = df_long[\"TT\"]*df_long[\"INCOME_3\"]\r\n\r\ndf_long[\"TTxPURPOSE_2\"] = df_long[\"TT\"]*df_long[\"PURPOSE_2\"]\r\ndf_long[\"TTxPURPOSE_3\"] = df_long[\"TT\"]*df_long[\"PURPOSE_3\"]\r\ndf_long[\"TTxPURPOSE_4\"] = df_long[\"TT\"]*df_long[\"PURPOSE_4\"]\r\n\r\n\r\n# ## 1. MNL-A\r\n\r\n# In[ ]:\r\n\r\n\r\n# Alternative-specific Utility Specification\r\n\r\nbasic_specification = OrderedDict()\r\nbasic_names = OrderedDict()\r\n\r\nbasic_specification[\"intercept\"] = [1, 2]\r\nbasic_names[\"intercept\"] = ['ASC Train',\r\n                            'ASC Swissmetro']\r\n\r\nbasic_specification[\"TT\"] = [1, 2, 3]\r\nbasic_names[\"TT\"] = ['Travel Time(Train)',\r\n                     'Travel Time(Swissmetro)',\r\n                     'Travel Time(Car)']\r\n\r\nbasic_specification[\"TC\"] = [[1, 2, 3]]\r\nbasic_names[\"TC\"] = ['Travel Cost(All)']\r\n\r\nbasic_specification[\"HE\"] = [1, 2]\r\nbasic_names[\"HE\"] = [\"Headway(Train)\",\r\n                     \"Headway(Swissmetro)\"]\r\n\r\nbasic_specification[\"SEAT\"] = [2]\r\nbasic_names[\"SEAT\"] = ['Airline Seat(Swissmetro)']\r\n\r\nbasic_specification[\"GA\"] = [1,2]\r\nbasic_names[\"GA\"] = ['GA(Train)',\r\n                     'GA(Swissmetro)']\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nfrom sklearn.model_selection import GroupShuffleSplit\r\nfrom sklearn.model_selection import train_test_split\r\n\r\n#Original: 1005\r\n\r\nSS = GroupShuffleSplit(n_splits=2, train_size=0.7, random_state=1234)\r\ntrain_idx, rem_idx = next(SS.split(df_wide,groups = df_wide['custom_id']))\r\n\r\ndf_wide_train = df_wide.iloc[train_idx]\r\ndf_wide_rem = df_wide.iloc[rem_idx]\r\n\r\nSS2 = GroupShuffleSplit(n_splits=2, train_size=0.5, random_state=1234)\r\ntest_idx, valid_idx = next(SS2.split(df_wide_rem,groups = df_wide_rem['custom_id']))\r\ndf_wide_test = df_wide_rem.iloc[test_idx]\r\ndf_wide_valid = df_wide_rem.iloc[valid_idx]\r\n\r\ndf_train = df_long[df_long.custom_id.isin(df_wide_train['custom_id'].unique())]\r\ndf_test = df_long[df_long.custom_id.isin(df_wide_test['custom_id'].unique())]\r\n\r\n\r\n\r\n# Estimate the multinomial logit model (MNL)\r\nMNL_A = pl.create_choice_model(data=df_train,\r\n                                alt_id_col=custom_alt_id,\r\n                                obs_id_col=obs_id_column,\r\n                                choice_col=choice_column,\r\n                                specification=basic_specification,\r\n                                model_type=\"MNL\",\r\n                                names=basic_names)\r\n\r\n# Specify the initial values and method for the optimization.\r\nMNL_A.fit_mle(np.zeros(11),maxiter=3000)\r\n\r\n\r\n# Get the prediction metrics\r\npredict_train = np.argmax(MNL_A.predict(df_train).reshape(-1,3),axis=1)\r\npredict_test =  np.argmax(MNL_A.predict(df_test).reshape(-1,3),axis=1)\r\n\r\ny_train = np.array(df_train.CHOICE).reshape(-1,3)\r\ny_test = np.array(df_test.CHOICE).reshape(-1,3)\r\ny_train_cat = np.argmax(np.array(df_train.CHOICE).reshape(-1,3),axis=1)\r\ny_test_cat = np.argmax(np.array(df_test.CHOICE).reshape(-1,3),axis=1)\r\n\r\n\r\ntrain_p = MNL_A.predict(df_train).reshape(-1,3)\r\ntest_p = MNL_A.predict(df_test).reshape(-1,3)\r\n\r\ntest_NLL = round((F.cross_entropy(input=torch.log(tensor(test_p)),target=torch.Tensor(y_test)).numpy()).item(),3)\r\ntrain_NLL =  round((F.cross_entropy(input=torch.log(tensor(train_p)),target=torch.Tensor(y_train)).numpy()).item(),3)\r\n\r\n\r\ntrain_acc = round(accuracy_score(y_train_cat,predict_train),3)\r\ntest_acc = round(accuracy_score(y_test_cat,predict_test),3)\r\n\r\ntrain_brier = round(brier_multi(y_train,train_p),3)\r\ntest_brier = round(brier_multi(y_test,test_p),3)\r\n\r\nprint([train_acc,test_acc,train_NLL,test_NLL,train_brier,test_brier])\r\npd.DataFrame(np.array([train_acc,test_acc,train_NLL,test_NLL,train_brier,test_brier])).transpose().to_clipboard(index=False,header=False)\r\n# Look at the estimation results\r\nMNL_A.get_statsmodels_summary()\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n# Get the utility\r\nparam_table = MNL_A.get_statsmodels_summary().tables[1].as_html()\r\nparam_table = pd.read_html(param_table, header=0, index_col=0)[0]\r\nparam_coef = param_table['coef']\r\ndf_TR = df_long.loc[df_long['mode_id'] == 1,['TT','TC','HE','GA']]\r\ndf_SM = df_long.loc[df_long['mode_id'] == 2,['TT','TC','HE','GA','SEAT']]\r\ndf_CAR = df_long.loc[df_long['mode_id'] == 3,['TT','TC']]\r\n\r\nutil_TR = param_table['coef']['ASC Train'] + np.dot(np.array(df_TR),np.array(param_table['coef'][['Travel Time(Train)','Travel Cost(All)','Headway(Train)','GA(Train)']]))\r\nutil_SM = param_table['coef']['ASC Swissmetro'] + np.dot(np.array(df_SM),np.array(param_table['coef'][['Travel Time(Swissmetro)','Travel Cost(All)','Headway(Swissmetro)','GA(Swissmetro)','Airline Seat(Swissmetro)']]))\r\nutil_CAR = np.dot(np.array(df_CAR),np.array(param_table['coef'][['Travel Time(Car)','Travel Cost(All)']]))\r\n\r\nprint(util_TR.min(),util_TR.max(),util_SM.min(),util_SM.max(),util_CAR.min(),util_CAR.max())\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n# Get the individual-level VOT \r\ndef first(x):\r\n    return(x.iloc[1,:])\r\n\r\nVOT_TR_A =  param_coef['Travel Time(Train)']/param_coef['Travel Cost(All)']\r\nVOT_SM_A =  param_coef['Travel Time(Swissmetro)']/param_coef['Travel Cost(All)']\r\nVOT_CAR_A =  param_coef['Travel Time(Car)']/param_coef['Travel Cost(All)']\r\n\r\nprint(np.array([VOT_TR_A,VOT_SM_A,VOT_CAR_A,]).round(3))\r\n\r\n\r\n# ## 2. MNL-B\r\n\r\n# In[ ]:\r\n\r\n\r\n# Alternative-specific Utility Specification\r\n\r\nbasic_specification = OrderedDict()\r\nbasic_names = OrderedDict()\r\n\r\nbasic_specification[\"intercept\"] = [1, 2]\r\nbasic_names[\"intercept\"] = ['ASC Train',\r\n                            'ASC Swissmetro']\r\n\r\nbasic_specification[\"TT\"] = [1, 2, 3]\r\nbasic_names[\"TT\"] = ['Travel Time(Train)',\r\n                     'Travel Time(Swissmetro)',\r\n                     'Travel Time(Car)']\r\n\r\nbasic_specification[\"TC\"] = [[1, 2, 3]]\r\nbasic_names[\"TC\"] = ['Travel Cost(All)']\r\n                     \r\n\r\nbasic_specification[\"HE\"] = [1, 2]\r\nbasic_names[\"HE\"] = [\"Headway(Train)\",\r\n                     \"Headway(Swissmetro)\"]\r\n\r\nbasic_specification[\"SEAT\"] = [2]\r\nbasic_names[\"SEAT\"] = ['Airline Seat(Swissmetro)']\r\n\r\nbasic_specification[\"GA\"] = [1,2]\r\nbasic_names[\"GA\"] = ['GA(Train)',\r\n                     'GA(Swissmetro)']\r\n\r\n## First-order Interactions\r\nbasic_specification[\"TTxAGE_2\"] = [1, 2, 3]\r\nbasic_names[\"TTxAGE_2\"] = [\"TTxAGE_2(Train)\",\r\n                           \"TTxAGE_2(Swissmetro)\",\r\n                           \"TTxAGE_2(Car)\"]\r\n\r\nbasic_specification[\"TTxAGE_3\"] = [1, 2, 3]\r\nbasic_names[\"TTxAGE_3\"] = [\"TTxAGE_3(Train)\",\r\n                           \"TTxAGE_3(Swissmetro)\",\r\n                           \"TTxAGE_3(Car)\"]\r\n\r\nbasic_specification[\"TTxAGE_4\"] = [1, 2, 3]\r\nbasic_names[\"TTxAGE_4\"] = [\"TTxAGE_4(Train)\",\r\n                           \"TTxAGE_4(Swissmetro)\",\r\n                           \"TTxAGE_4(Car)\"]\r\n\r\nbasic_specification[\"TTxAGE_5\"] = [1, 2, 3]\r\nbasic_names[\"TTxAGE_5\"] = [\"TTxAGE_5(Train)\",\r\n                           \"TTxAGE_5(Swissmetro)\",\r\n                           \"TTxAGE_5(Car)\"]\r\n\r\nbasic_specification[\"TTxINCOME_2\"] = [1, 2, 3]\r\nbasic_names[\"TTxINCOME_2\"] = [\"TTxINCOME_2(Train)\",\r\n                              \"TTxINCOME_2(Swissmetro)\",\r\n                              \"TTxINCOME_2(Car)\"]\r\n\r\nbasic_specification[\"TTxINCOME_3\"] = [1, 2, 3]\r\nbasic_names[\"TTxINCOME_3\"] = [\"TTxINCOME_3(Train)\",\r\n                              \"TTxINCOME_3(Swissmetro)\",\r\n                              \"TTxINCOME_3(Car)\"]\r\n\r\nbasic_specification[\"TTxPURPOSE_2\"] = [1, 2, 3]\r\nbasic_names[\"TTxPURPOSE_2\"] = [\"TTxPURPOSE_2(Train)\",\r\n                              \"TTxPURPOSE_2(Swissmetro)\",\r\n                              \"TTxPURPOSE_2(Car)\"]\r\n\r\nbasic_specification[\"TTxPURPOSE_3\"] = [1, 2, 3]\r\nbasic_names[\"TTxPURPOSE_3\"] = [\"TTxPURPOSE_3(Train)\",\r\n                              \"TTxPURPOSE_3(Swissmetro)\",\r\n                              \"TTxPURPOSE_3(Car)\"]\r\n\r\nbasic_specification[\"TTxPURPOSE_4\"] = [1, 2, 3]\r\nbasic_names[\"TTxPURPOSE_4\"] = [\"TTxPURPOSE_4(Train)\",\r\n                              \"TTxPURPOSE_4(Swissmetro)\",\r\n                              \"TTxPURPOSE_4(Car)\"]\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nfrom sklearn.model_selection import GroupShuffleSplit\r\n\r\n\r\n# Estimate the multinomial logit model (MNL)\r\nMNL_B = pl.create_choice_model(data=df_train,\r\n                                alt_id_col=custom_alt_id,\r\n                                obs_id_col=obs_id_column,\r\n                                choice_col=choice_column,\r\n                                specification=basic_specification,\r\n                                model_type=\"MNL\",\r\n                                names=basic_names)\r\n\r\n# Specify the initial values and method for the optimization.\r\nMNL_B.fit_mle(np.zeros(38))\r\n\r\n# Look at the estimation results\r\nMNL_B.get_statsmodels_summary()\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n# Get the prediction metrics\r\npredict_train = np.argmax(MNL_B.predict(df_train).reshape(-1,3),axis=1)\r\npredict_test =  np.argmax(MNL_B.predict(df_test).reshape(-1,3),axis=1)\r\n\r\ny_train = np.array(df_train.CHOICE).reshape(-1,3)\r\ny_test = np.array(df_test.CHOICE).reshape(-1,3)\r\ny_train_cat = np.argmax(np.array(df_train.CHOICE).reshape(-1,3),axis=1)\r\ny_test_cat = np.argmax(np.array(df_test.CHOICE).reshape(-1,3),axis=1)\r\n\r\n\r\ntrain_p = MNL_B.predict(df_train).reshape(-1,3)\r\ntest_p = MNL_B.predict(df_test).reshape(-1,3)\r\n\r\ntest_NLL = round((F.cross_entropy(input=torch.log(tensor(test_p)),target=torch.Tensor(y_test)).numpy()).item(),3)\r\ntrain_NLL =  round((F.cross_entropy(input=torch.log(tensor(train_p)),target=torch.Tensor(y_train)).numpy()).item(),3)\r\n\r\ntrain_acc = round(accuracy_score(y_train_cat,predict_train),3)\r\ntest_acc = round(accuracy_score(y_test_cat,predict_test),3)\r\n\r\ntrain_brier = round(brier_multi(y_train,train_p),3)\r\ntest_brier = round(brier_multi(y_test,test_p),3)\r\n\r\nprint([train_acc,test_acc,train_NLL,test_NLL,train_brier,test_brier])\r\npd.DataFrame(np.array([train_acc,test_acc,train_NLL,test_NLL,train_brier,test_brier])).transpose().to_clipboard(index=False)\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n# Get the individual-level VOT \r\ndef first(x):\r\n    return(x.iloc[1,:])\r\n\r\nparam_table = MNL_B.get_statsmodels_summary().tables[1].as_html()\r\nparam_table = pd.read_html(param_table, header=0, index_col=0)[0]\r\nparam_coef = param_table['coef']\r\ndf_VOT_B = df_test[['ID','custom_id','AGE_2', 'AGE_3','AGE_4', 'AGE_5',\r\n                  'INCOME_2', 'INCOME_3', 'PURPOSE_2','PURPOSE_3', 'PURPOSE_4','AGES','INCOMES','PURPOSES']].groupby('custom_id').apply(first)\r\ndf_VOT_B['intercept'] = np.ones(df_VOT_B.shape[0])\r\nVOT_TR_B = np.dot(\r\n        np.array(df_VOT_B[['intercept','AGE_2', 'AGE_3','AGE_4', 'AGE_5',\r\n                'INCOME_2', 'INCOME_3', 'PURPOSE_2','PURPOSE_3', 'PURPOSE_4']]),\r\n        np.array(param_coef[['Travel Time(Train)',\r\n                'TTxAGE_2(Train)','TTxAGE_3(Train)','TTxAGE_4(Train)','TTxAGE_5(Train)','TTxINCOME_2(Train)','TTxINCOME_3(Train)',\r\n                'TTxPURPOSE_2(Train)','TTxPURPOSE_3(Train)','TTxPURPOSE_4(Train)']]))\r\n\r\nVOT_SM_B = np.dot(\r\n        np.array(df_VOT_B[['intercept','AGE_2', 'AGE_3','AGE_4','AGE_5',\r\n                'INCOME_2', 'INCOME_3', 'PURPOSE_2','PURPOSE_3','PURPOSE_4']]),\r\n        np.array(param_coef[['Travel Time(Swissmetro)',\r\n                'TTxAGE_2(Swissmetro)','TTxAGE_3(Swissmetro)','TTxAGE_4(Swissmetro)','TTxAGE_5(Swissmetro)','TTxINCOME_2(Swissmetro)','TTxINCOME_3(Swissmetro)',\r\n                'TTxPURPOSE_2(Swissmetro)','TTxPURPOSE_3(Swissmetro)','TTxPURPOSE_4(Swissmetro)']]))\r\n\r\nVOT_CAR_B = np.dot(\r\n        np.array(df_VOT_B[['intercept','AGE_2', 'AGE_3','AGE_4', 'AGE_5',\r\n                'INCOME_2', 'INCOME_3', 'PURPOSE_2','PURPOSE_3', 'PURPOSE_4']]),\r\n        np.array(param_coef[['Travel Time(Car)',\r\n                'TTxAGE_2(Car)','TTxAGE_3(Car)','TTxAGE_4(Car)','TTxAGE_5(Car)','TTxINCOME_2(Car)','TTxINCOME_3(Car)',\r\n                'TTxPURPOSE_2(Car)','TTxPURPOSE_3(Car)','TTxPURPOSE_4(Car)']]))\r\n\r\ndf_VOT_B['VOT_TR'] = VOT_TR_B / param_coef['Travel Cost(All)']\r\ndf_VOT_B['VOT_SM'] = VOT_SM_B / param_coef['Travel Cost(All)']\r\ndf_VOT_B['VOT_CAR'] = VOT_CAR_B / param_coef['Travel Cost(All)']\r\n\r\ndf_VOT_B = df_VOT_B[['ID','VOT_TR','VOT_SM','VOT_CAR',\r\n                     'AGES','INCOMES','PURPOSES']].groupby(['AGES','INCOMES','PURPOSES']).mean()\r\n\r\nprint(np.quantile(df_VOT_B['VOT_TR'],(0.5,0.01,0.25,0.75,0.99)).round(3))\r\nprint(np.quantile(df_VOT_B['VOT_SM'],(0.5,0.01,0.25,0.75,0.99)).round(3))\r\nprint(np.quantile(df_VOT_B['VOT_CAR'],(0.5,0.01,0.25,0.75,0.99)).round(3))\r\n\r\n\r\n# ## 3. DNN\r\n\r\n# In[ ]:\r\n\r\n\r\n## 1. DNN\r\n\r\nx_train = df_wide_train[[\r\n    'TRAIN_TT','TRAIN_TC','TRAIN_HE',\r\n    'SM_TT','SM_TC','SM_HE','SM_SEATS',\r\n    'CAR_TT', 'CAR_TC','GA','AGES','INCOMES','PURPOSES']]\r\n\r\ny_train = np.array(pd.get_dummies(df_wide_train['CHOICE']))\r\n\r\n\r\nx_test = df_wide_test[[\r\n    'TRAIN_TT','TRAIN_TC','TRAIN_HE',\r\n    'SM_TT','SM_TC','SM_HE','SM_SEATS',\r\n    'CAR_TT', 'CAR_TC','GA','AGES','INCOMES','PURPOSES']]\r\n\r\ny_test = np.array(pd.get_dummies(df_wide_test['CHOICE']))\r\n\r\nx_valid = df_wide_valid[[\r\n    'TRAIN_TT','TRAIN_TC','TRAIN_HE',\r\n    'SM_TT','SM_TC','SM_HE','SM_SEATS',\r\n    'CAR_TT', 'CAR_TC','GA','AGES','INCOMES','PURPOSES']]\r\n\r\ny_valid = np.array(pd.get_dummies(df_wide_valid['CHOICE']))\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n## 2. Define the Original DNN\r\n\r\ndef build_DNN(num_layers,num_neurons,drop_rate,learning_rate):\r\n\r\n    img = Input(shape=x_train.shape[1],name=\"main_input\")   \r\n    h = Dense(num_neurons,activation='relu')(img)\r\n    h = Dropout(drop_rate)(h)\r\n    for num_layer in range(num_layers-1):\r\n        h = Dense(num_neurons,activation='relu')(h)\r\n        h = Dropout(drop_rate)(h)\r\n    util = Dense(1)(h)\r\n    \r\n    ## Alternative-specific utility\r\n    util_ALT1 = Dense(int(num_neurons*0.5))(h)\r\n    util_ALT1 = Dropout(drop_rate)(util_ALT1)\r\n    util_ALT1 = Dense(1,name='output_TR')(util_ALT1)\r\n    \r\n    util_ALT2 = Dense(int(num_neurons*0.5))(h)\r\n    util_ALT2 = Dropout(drop_rate)(util_ALT2)\r\n    util_ALT2 = Dense(1,name='output_SM')(util_ALT2)\r\n       \r\n    util_ALT3 = Dense(int(num_neurons*0.5))(h)\r\n    util_ALT3 = Dropout(drop_rate)(util_ALT3)\r\n    util_ALT3 = Dense(1,name='output_CAR')(util_ALT3)\r\n\r\n    out_prob =  tf.keras.layers.Softmax(name='out_prob')(Concatenate()([util_ALT1,util_ALT2,util_ALT3]))\r\n    \r\n    model = Model(img,[out_prob,util_ALT1,util_ALT2,util_ALT3])  \r\n    \r\n    optimizer = Adam(learning_rate=learning_rate)\r\n    model.compile(optimizer=optimizer, loss=['categorical_crossentropy',None,None,None],\r\n                  metrics=['accuracy',None,None,None])\r\n\r\n    return model\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nfor i in range(0,1):\r\n    \r\n    num_layers = 3\r\n    num_neurons = 200\r\n    drop_rate = 0.005\r\n    learning_rate = 0.0005\r\n\r\n    DNN = build_DNN(num_layers,num_neurons,drop_rate,learning_rate)\r\n\r\n    batch_size = 128\r\n    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', restore_best_weights=True, verbose=1, patience=20)\r\n    history = DNN.fit(\r\n        x=x_train,\r\n        y=y_train,\r\n        shuffle=True,\r\n        epochs = 200,\r\n        batch_size = batch_size,\r\n        validation_data=[x_valid,y_valid],\r\n        callbacks = [es],\r\n        verbose=0)\r\n\r\n    DNN.save_weights('C:/Users/euijin/Documents/DNN_Models_Revision/DNN_FUL_'+str(i))\r\n    #DNN.load_weights('C:/Users/euijin/Documents/DNN_Models/DNN_FUL_'+str(i))\r\n\r\n\r\n\r\n    # Get the prediction metrics\r\n    test_p = DNN.predict(x_test,verbose=0)\r\n    train_p = DNN.predict(x_train,verbose=0)\r\n\r\n    predict_train = np.argmax(train_p[0],axis=1)\r\n    predict_test =  np.argmax(test_p[0],axis=1)\r\n\r\n    y_train = np.array(df_train.CHOICE).reshape(-1,3)\r\n    y_test = np.array(df_test.CHOICE).reshape(-1,3)\r\n    y_train_cat = np.argmax(np.array(df_train.CHOICE).reshape(-1,3),axis=1)\r\n    y_test_cat = np.argmax(np.array(df_test.CHOICE).reshape(-1,3),axis=1)\r\n\r\n    train_acc = round(accuracy_score(y_train_cat,predict_train),3)\r\n    test_acc = round(accuracy_score(y_test_cat,predict_test),3)\r\n\r\n    train_brier = round(brier_multi(y_train,train_p[0]),3)\r\n    test_brier = round(brier_multi(y_test,test_p[0]),3)\r\n\r\n    print([train_acc,test_acc,train_brier,test_brier])\r\n\r\n\r\n    ## Plot the PD and ICE\r\n    from sklearn.inspection import partial_dependence\r\n    from sklearn.inspection import PartialDependenceDisplay\r\n    import matplotlib.pyplot as plt\r\n\r\n    class DNNFunction(BaseEstimator, ClassifierMixin):\r\n        def fit(self, X, y):\r\n            self.classes_ = unique_labels(y)\r\n            return self\r\n\r\n        def predict_proba(self, X):\r\n            self.proba_,_,_,_ = DNN.predict(X,verbose=0)\r\n            return np.array(self.proba_)\r\n\r\n        def decision_function(self, X):\r\n            self.utility_ = np.array(DNN.predict(X)[1:4]).transpose().reshape(-1,3)\r\n            return self.utility_\r\n\r\n    DNN_ALL = DNNFunction()\r\n    DNN_ALL.fit(x_train,y_train)\r\n\r\n    common_params = {\r\n        \"subsample\": 0.99999,\r\n        \"n_jobs\": 1,\r\n        \"grid_resolution\": 20,\r\n        \"centered\": True,\r\n        \"random_state\": 1,\r\n        \"response_method\": 'decision_function',\r\n        \"percentiles\": (0.01, 0.99),\r\n\r\n    }\r\n\r\n    fig,ax = plt.subplots(nrows=1,ncols=2, figsize=(12,6),sharex=False,sharey=True)\r\n\r\n    display_0 = PartialDependenceDisplay.from_estimator(\r\n        DNN_ALL,\r\n        x_train,\r\n        features=['TRAIN_TT','TRAIN_TC'], # TC,TT\r\n        kind=[\"both\",\"both\"],\r\n        ice_lines_kw={\"color\": \"gray\",'alpha':0.3},pd_line_kw={\"color\": \"black\",\"lw\":3,'linestyle':'solid','label':'True'},\r\n        ax=ax,\r\n        target = 0,\r\n        **common_params,\r\n    )\r\n\r\n    fig,ax = plt.subplots(nrows=1,ncols=2, figsize=(12,6),sharex=False,sharey=True)\r\n\r\n    display_1 = PartialDependenceDisplay.from_estimator(\r\n        DNN_ALL,\r\n        x_train,\r\n        features=['SM_TT','SM_TC'], # TC,TT\r\n        kind=[\"both\",\"both\"],\r\n        ice_lines_kw={\"color\": \"gray\",'alpha':0.3},pd_line_kw={\"color\": \"black\",\"lw\":3,'linestyle':'solid','label':'True'},\r\n        ax=ax,\r\n        target = 1,\r\n        **common_params,\r\n    )\r\n\r\n    fig,ax = plt.subplots(nrows=1,ncols=2, figsize=(12,6),sharex=False,sharey=True)\r\n\r\n    display_2 = PartialDependenceDisplay.from_estimator(\r\n        DNN_ALL,\r\n        x_train,\r\n        features=['CAR_TT','CAR_TC'], # TC,TT\r\n        kind=[\"both\",\"both\"],\r\n        ice_lines_kw={\"color\": \"gray\",'alpha':0.3},pd_line_kw={\"color\": \"black\",\"lw\":3,'linestyle':'solid','label':'True'},\r\n        ax=ax,\r\n        target = 2,\r\n        **common_params,\r\n    )\r\n\r\n\r\n\r\n    ## Population-level Parameters\r\n\r\n    PD_DNN_TT_TR = display_0.pd_results[0]['average'][0]\r\n    PD_DNN_TC_TR = display_0.pd_results[1]['average'][0] \r\n\r\n    PD_DNN_TT_SM = display_1.pd_results[0]['average'][1]\r\n    PD_DNN_TC_SM = display_1.pd_results[1]['average'][1] \r\n\r\n    PD_DNN_TT_CAR = display_2.pd_results[0]['average'][2]\r\n    PD_DNN_TC_CAR = display_2.pd_results[1]['average'][2] \r\n\r\n    PD_values_TT_TR = display_0.pd_results[0]['values'][0]\r\n    PD_values_TC_TR = display_0.pd_results[1]['values'][0]\r\n\r\n    PD_values_TT_SM = display_1.pd_results[0]['values'][0]\r\n    PD_values_TC_SM = display_1.pd_results[1]['values'][0]\r\n\r\n    PD_values_TT_CAR = display_2.pd_results[0]['values'][0]\r\n    PD_values_TC_CAR = display_2.pd_results[1]['values'][0]\r\n\r\n    beta_DNN_TT_TR = (np.diff(PD_DNN_TT_TR)/np.diff(PD_values_TT_TR))\r\n    beta_DNN_TC_TR = (np.diff(PD_DNN_TC_TR)/np.diff(PD_values_TC_TR))\r\n\r\n    beta_DNN_TT_SM = (np.diff(PD_DNN_TT_SM)/np.diff(PD_values_TT_SM))\r\n    beta_DNN_TC_SM = (np.diff(PD_DNN_TC_SM)/np.diff(PD_values_TC_SM))\r\n\r\n    beta_DNN_TT_CAR = (np.diff(PD_DNN_TT_CAR)/np.diff(PD_values_TT_CAR))\r\n    beta_DNN_TC_CAR = (np.diff(PD_DNN_TC_CAR)/np.diff(PD_values_TC_CAR))\r\n\r\n\r\n    TC_TR_IDX =(beta_DNN_TC_TR != 0)\r\n    TC_SM_IDX =(beta_DNN_TC_SM != 0)\r\n    TC_CAR_IDX =(beta_DNN_TC_CAR != 0)\r\n\r\n    beta_DNN_TC_TR = beta_DNN_TC_TR[TC_TR_IDX]\r\n    beta_DNN_TC_SM = beta_DNN_TC_SM[TC_SM_IDX]\r\n    beta_DNN_TC_CAR = beta_DNN_TC_CAR[TC_CAR_IDX]\r\n\r\n    beta_DNN_TT_TR = beta_DNN_TT_TR[TC_TR_IDX]\r\n    beta_DNN_TT_SM = beta_DNN_TT_SM[TC_SM_IDX]\r\n    beta_DNN_TT_CAR = beta_DNN_TT_CAR[TC_CAR_IDX]\r\n\r\n\r\n    beta_DNN_VOT_POP_TR = np.median(compute_VOT_POP(beta_DNN_TT_TR,beta_DNN_TC_TR))\r\n    beta_DNN_VOT_POP_SM = np.median(compute_VOT_POP(beta_DNN_TT_SM,beta_DNN_TC_SM))\r\n    beta_DNN_VOT_POP_CAR = np.median(compute_VOT_POP(beta_DNN_TT_CAR,beta_DNN_TC_CAR))\r\n\r\n\r\n    ## Individual-level Parameters\r\n\r\n    ICE_DNN_TT_TR = display_0.pd_results[0]['individual'][0]\r\n    ICE_DNN_TC_TR = display_0.pd_results[1]['individual'][0]\r\n\r\n    ICE_DNN_TT_SM = display_1.pd_results[0]['individual'][1]\r\n    ICE_DNN_TC_SM = display_1.pd_results[1]['individual'][1]\r\n\r\n    ICE_DNN_TT_CAR = display_2.pd_results[0]['individual'][2]\r\n    ICE_DNN_TC_CAR = display_2.pd_results[1]['individual'][2]\r\n\r\n    beta_DNN_TT_TR_ICE = (np.diff(ICE_DNN_TT_TR)/np.diff(PD_values_TT_TR))\r\n    beta_DNN_TC_TR_ICE = (np.diff(ICE_DNN_TC_TR)/np.diff(PD_values_TC_TR))\r\n    beta_DNN_VOT_TR_ICE = compute_VOT_IND(beta_DNN_TT_TR_ICE[:,TC_TR_IDX],beta_DNN_TC_TR_ICE[:,TC_TR_IDX])\r\n\r\n    beta_DNN_TT_SM_ICE = (np.diff(ICE_DNN_TT_SM)/np.diff(PD_values_TT_SM))\r\n    beta_DNN_TC_SM_ICE = (np.diff(ICE_DNN_TC_SM)/np.diff(PD_values_TC_SM))\r\n    beta_DNN_VOT_SM_ICE = compute_VOT_IND(beta_DNN_TT_SM_ICE[:,TC_SM_IDX],beta_DNN_TC_SM_ICE[:,TC_SM_IDX])\r\n\r\n\r\n    beta_DNN_TT_CAR_ICE = (np.diff(ICE_DNN_TT_CAR)/np.diff(PD_values_TT_CAR))\r\n    beta_DNN_TC_CAR_ICE = (np.diff(ICE_DNN_TC_CAR)/np.diff(PD_values_TC_CAR))\r\n    beta_DNN_VOT_CAR_ICE = compute_VOT_IND(beta_DNN_TT_CAR_ICE[:,TC_CAR_IDX],beta_DNN_TC_CAR_ICE[:,TC_CAR_IDX])\r\n\r\n\r\n    # Estimation # If we use the median value, we don't need to care about the outlier or somethings.\r\n\r\n    x_test_E = x_train[['AGES','INCOMES','PURPOSES']].reset_index(drop=True)\r\n    x_test_E['VOT_TR'] = beta_DNN_VOT_TR_ICE\r\n    x_test_E['VOT_SM'] = beta_DNN_VOT_SM_ICE\r\n    x_test_E['VOT_CAR'] = beta_DNN_VOT_CAR_ICE\r\n\r\n    VOT_DNN_IND = x_test_E[['AGES','INCOMES','PURPOSES','VOT_TR','VOT_SM','VOT_CAR']].groupby(['AGES','INCOMES','PURPOSES']).median()\r\n\r\n\r\n    print(np.quantile(VOT_DNN_IND['VOT_TR'],(0.5,0.01,0.25,0.75,0.99)).round(3))\r\n    print(np.quantile(VOT_DNN_IND['VOT_SM'],(0.5,0.01,0.25,0.75,0.99)).round(3))\r\n    print(np.quantile(VOT_DNN_IND['VOT_CAR'],(0.5,0.01,0.25,0.75,0.99)).round(3))\r\n\r\n\r\n    # Estimation results\r\n    PRED_PERF = np.array([train_acc,test_acc,train_brier,test_brier]).transpose()\r\n    EST_RESULTS = [PRED_PERF,VOT_DNN_IND]\r\n    with open(\"C:/Users/euijin/Documents/DNN_Results_Revision/FUL_RESULTS_FF_\"+str(i), \"wb\") as f:\r\n        pickle.dump(EST_RESULTS, f)\r\n    #save_clipboard(np.concatenate([PRED_PERF,VOT_DNN_IND.median(axis=0).round(3)])) \r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nimport scipy.stats as stats\r\n## Results analysis\r\nPRED_PERF_S = []\r\nVOT_DNN_IND_S = []\r\nVOT_DNN_index = []\r\n\r\nfor i in range(1):\r\n    with open(\"C:/Users/euijin/Documents/DNN_Results_Revision/FUL_RESULTS_FF_\"+str(i),'rb') as f:\r\n        temp = pickle.load(f)\r\n    PRED_PERF_S.append(temp[0])\r\n    VOT_DNN_IND_S.append(temp[1])\r\n    VOT_DNN_index.append(temp[1].index.values)\r\n    \r\n\r\n## Predictability    \r\nPRED_PERF_S = np.array(PRED_PERF_S)\r\nA = np.concatenate([PRED_PERF_S.mean(axis=0).round(3).reshape(-1,1),PRED_PERF_S.std(axis=0).round(3).reshape(-1,1)],axis=1)\r\n\r\n\r\n## Interpretability\r\nIND_stats = []\r\nVOT_DNN_IND_S_TR = []\r\nVOT_DNN_IND_S_SM = []\r\nVOT_DNN_IND_S_CAR = []\r\n\r\nfor i in range(len(VOT_DNN_IND_S)):\r\n    \r\n    inter_IDX_IND = VOT_DNN_IND_S[i].index.isin(np.intersect1d(VOT_DNN_IND_S[i].index,df_VOT_B.index))\r\n        \r\n    TRIND = np.quantile(VOT_DNN_IND_S[i].iloc[inter_IDX_IND,0],(0.5,0.01,0.25,0.75,0.99)).round(3)\r\n    SMIND = np.quantile(VOT_DNN_IND_S[i].iloc[inter_IDX_IND,1],(0.5,0.01,0.25,0.75,0.99)).round(3)\r\n    CARIND = np.quantile(VOT_DNN_IND_S[i].iloc[inter_IDX_IND,2],(0.5,0.01,0.25,0.75,0.99)).round(3)   \r\n    ALLIND = np.concatenate([TRIND,SMIND,CARIND])   \r\n    IND_stats.append(ALLIND)\r\n    \r\n    VOT_DNN_IND_S_TR.append(VOT_DNN_IND_S[i].iloc[inter_IDX_IND,0])\r\n    VOT_DNN_IND_S_SM.append(VOT_DNN_IND_S[i].iloc[inter_IDX_IND,1])\r\n    VOT_DNN_IND_S_CAR.append(VOT_DNN_IND_S[i].iloc[inter_IDX_IND,2])\r\n      \r\nB = np.array(IND_stats)\r\nB = np.concatenate([np.median(B,axis=0).round(3).reshape(-1,1),\r\n                    stats.iqr(B,axis=0).round(3).reshape(-1,1)],axis=1)\r\n\r\n\r\nVOT_DNN_IND_S_TR = np.concatenate(VOT_DNN_IND_S_TR)\r\nVOT_DNN_IND_S_SM = np.concatenate(VOT_DNN_IND_S_SM)\r\nVOT_DNN_IND_S_CAR = np.concatenate(VOT_DNN_IND_S_CAR)\r\n\r\nEST_RESULTS_DNN = np.vstack([B,A])\r\nsave_clipboard(EST_RESULTS_DNN)\r\n\r\n\r\n# ## 4. TCNN\r\n\r\n# In[ ]:\r\n\r\n\r\n## 2. BCDNN\r\n\r\nx_train = df_wide_train[[\r\n    'TRAIN_TT','TRAIN_TC','TRAIN_HE',\r\n    'SM_TT','SM_TC','SM_HE','SM_SEATS',\r\n    'CAR_TT', 'CAR_TC','GA','AGES','INCOMES','PURPOSES']]\r\n\r\ny_train = np.array(pd.get_dummies(df_wide_train['CHOICE']))\r\n\r\n\r\nx_test = df_wide_test[[\r\n    'TRAIN_TT','TRAIN_TC','TRAIN_HE',\r\n    'SM_TT','SM_TC','SM_HE','SM_SEATS',\r\n    'CAR_TT', 'CAR_TC','GA','AGES','INCOMES','PURPOSES']]\r\n\r\ny_test = np.array(pd.get_dummies(df_wide_test['CHOICE']))\r\n\r\nx_valid = df_wide_valid[[\r\n    'TRAIN_TT','TRAIN_TC','TRAIN_HE',\r\n    'SM_TT','SM_TC','SM_HE','SM_SEATS',\r\n    'CAR_TT', 'CAR_TC','GA','AGES','INCOMES','PURPOSES']]\r\n\r\ny_valid = np.array(pd.get_dummies(df_wide_valid['CHOICE']))\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n## Hyperparameters of Lattice Networks\r\n\r\ndef build_LatDNN(TRAIN_TT_KP,TRAIN_TT_LS,TRAIN_TC_KP,TRAIN_TC_LS,TRAIN_HE_KP,TRAIN_HE_LS,SM_TT_KP,SM_TT_LS,SM_TC_KP,SM_TC_LS,SM_HE_KP,SM_HE_LS,\r\n                 CAR_TT_KP,CAR_TT_LS,CAR_TC_KP,CAR_TC_LS,SM_SEATS_LS,GA_LS,AGE_LS,INCOME_LS,PURPOSE_LS,ACT_TR,ACT_SM,ACT_CAR):\r\n    \r\n    non_split_input = Input(shape=x_train.shape[1], name='non_split_input',dtype='float32')\r\n    \r\n    # Alternative-specific Lattice inputs\r\n    lattice_inputs_TR = []\r\n    lattice_inputs_SM = []\r\n    lattice_inputs_CAR = []\r\n    \r\n    # TRAIN_TT\r\n    TRAIN_TT_input = tf.reshape(non_split_input[:,0],(-1,1))\r\n    \r\n    ## TRAIN_TT to TR\r\n    TRAIN_TT_TR_calibrator = tfl.layers.PWLCalibration(\r\n    input_keypoints=np.linspace(\r\n        x_train['TRAIN_TT'].min(), x_train['TRAIN_TT'].max(), num=TRAIN_TT_KP),\r\n    dtype=tf.float32,\r\n    output_min=0.0,\r\n    output_max=TRAIN_TT_LS - 1.0,\r\n    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\r\n    monotonicity='decreasing', ## Sign constraints \r\n    name='TRAIN_TT_TR_Calib',\r\n    )(TRAIN_TT_input)\r\n\r\n    ## TRAIN_TT to Others\r\n    TRAIN_TT_OTH_calibrator = tfl.layers.PWLCalibration(\r\n    input_keypoints=np.linspace(\r\n        x_train['TRAIN_TT'].min(), x_train['TRAIN_TT'].max(), num=TRAIN_TT_KP),\r\n    dtype=tf.float32,\r\n    output_min=0.0,\r\n    output_max=TRAIN_TT_LS - 1.0,\r\n    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\r\n    monotonicity='increasing',\r\n    name='TRAIN_TT_OTH_Calib',\r\n    )(TRAIN_TT_input)\r\n   \r\n\r\n    lattice_inputs_TR.append(TRAIN_TT_TR_calibrator)\r\n    # lattice_inputs_SM.append(TRAIN_TT_OTH_calibrator)\r\n    # lattice_inputs_CAR.append(TRAIN_TT_OTH_calibrator)      \r\n    \r\n    \r\n    # TRAIN_TC\r\n    TRAIN_TC_input = tf.reshape(non_split_input[:,1],(-1,1))\r\n    \r\n    ## TRAIN_TC to TR\r\n    TRAIN_TC_TR_calibrator = tfl.layers.PWLCalibration(\r\n    input_keypoints=np.linspace(\r\n        x_train['TRAIN_TC'].min(), x_train['TRAIN_TC'].max(), num=TRAIN_TC_KP),\r\n    dtype=tf.float32,\r\n    output_min=0.0,\r\n    output_max=TRAIN_TC_LS - 1.0,\r\n    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\r\n    monotonicity='decreasing', ## Sign constraints \r\n    name='TRAIN_TC_TR_Calib',\r\n    )(TRAIN_TC_input)\r\n\r\n\r\n    ## TRAIN_TC to Others\r\n    TRAIN_TC_OTH_calibrator = tfl.layers.PWLCalibration(\r\n    input_keypoints=np.linspace(\r\n        x_train['TRAIN_TC'].min(), x_train['TRAIN_TC'].max(), num=TRAIN_TC_KP),\r\n    dtype=tf.float32,\r\n    output_min=0.0,\r\n    output_max=TRAIN_TC_LS - 1.0,\r\n    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\r\n    monotonicity='increasing',\r\n    name='TRAIN_TC_OTH_Calib',\r\n    )(TRAIN_TC_input)\r\n    \r\n    lattice_inputs_TR.append(TRAIN_TC_TR_calibrator)\r\n    # lattice_inputs_SM.append(TRAIN_TC_OTH_calibrator)\r\n    # lattice_inputs_CAR.append(TRAIN_TC_OTH_calibrator)\r\n    \r\n    \r\n        \r\n    # TRAIN_HE\r\n    TRAIN_HE_input = tf.reshape(non_split_input[:,2],(-1,1))\r\n    \r\n    ## TRAIN_HE to TR\r\n    TRAIN_HE_TR_calibrator = tfl.layers.PWLCalibration(\r\n    input_keypoints=np.linspace(\r\n        x_train['TRAIN_HE'].min(), x_train['TRAIN_HE'].max(), num=TRAIN_HE_KP),\r\n    dtype=tf.float32,\r\n    output_min=0.0,\r\n    output_max=TRAIN_HE_LS - 1.0,\r\n    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\r\n    monotonicity='decreasing', ## Sign constraints \r\n    name='TRAIN_HE_TR_Calib',\r\n    )(TRAIN_HE_input)\r\n\r\n\r\n    ## TRAIN_HE to Others\r\n    TRAIN_HE_OTH_calibrator = tfl.layers.PWLCalibration(\r\n    input_keypoints=np.linspace(\r\n        x_train['TRAIN_HE'].min(), x_train['TRAIN_HE'].max(), num=TRAIN_HE_KP),\r\n    dtype=tf.float32,\r\n    output_min=0.0,\r\n    output_max=TRAIN_HE_LS - 1.0,\r\n    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\r\n    monotonicity='increasing', ## Sign constraints \r\n    name='TRAIN_HE_OTH_Calib',\r\n    )(TRAIN_HE_input)\r\n   \r\n    lattice_inputs_TR.append(TRAIN_HE_TR_calibrator)\r\n    # lattice_inputs_SM.append(TRAIN_HE_OTH_calibrator)\r\n    # lattice_inputs_CAR.append(TRAIN_HE_OTH_calibrator)\r\n    \r\n    \r\n    \r\n    \r\n    # Swissmetro_TT\r\n    SM_TT_input = tf.reshape(non_split_input[:,3],(-1,1))\r\n    \r\n    ## SM_TT to SM\r\n    SM_TT_SM_calibrator = tfl.layers.PWLCalibration(\r\n    input_keypoints=np.linspace(\r\n        x_train['SM_TT'].min(), x_train['SM_TT'].max(), num=SM_TT_KP),\r\n    dtype=tf.float32,\r\n    output_min=0.0,\r\n    output_max=SM_TT_LS - 1.0,\r\n    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\r\n    monotonicity='decreasing', ## Sign constraints \r\n    name='SM_TT_SM_Calib',\r\n    )(SM_TT_input)\r\n\r\n    ## SM_TT to Others\r\n    SM_TT_OTH_calibrator = tfl.layers.PWLCalibration(\r\n    input_keypoints=np.linspace(\r\n        x_train['SM_TT'].min(), x_train['SM_TT'].max(), num=SM_TT_KP),\r\n    dtype=tf.float32,\r\n    output_min=0.0,\r\n    output_max=SM_TT_LS - 1.0,\r\n    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\r\n    monotonicity='increasing', ## Sign constraints \r\n    name='SM_TT_OTH_Calib',\r\n    )(SM_TT_input)\r\n    lattice_inputs_SM.append(SM_TT_SM_calibrator)\r\n    # lattice_inputs_TR.append(SM_TT_OTH_calibrator)\r\n    # lattice_inputs_CAR.append(SM_TT_OTH_calibrator)\r\n    \r\n    \r\n    # Swissmetro_TC\r\n    SM_TC_input = tf.reshape(non_split_input[:,4],(-1,1))\r\n    \r\n    ## SM_TC to SM\r\n    SM_TC_SM_calibrator = tfl.layers.PWLCalibration(\r\n    input_keypoints=np.linspace(\r\n        x_train['SM_TC'].min(), x_train['SM_TC'].max(), num=SM_TC_KP),\r\n    dtype=tf.float32,\r\n    output_min=0.0,\r\n    output_max=SM_TC_LS - 1.0,\r\n    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\r\n    monotonicity='decreasing', ## Sign constraints \r\n    name='SM_TC_SM_Calib',\r\n    )(SM_TC_input)\r\n\r\n    ## SM_TC to Others\r\n    SM_TC_OTH_calibrator = tfl.layers.PWLCalibration(\r\n    input_keypoints=np.linspace(\r\n        x_train['SM_TC'].min(), x_train['SM_TC'].max(), num=SM_TC_KP),\r\n    dtype=tf.float32,\r\n    output_min=0.0,\r\n    output_max=SM_TC_LS - 1.0,\r\n    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\r\n    monotonicity='increasing', ## Sign constraints \r\n    name='SM_TC_OTH_Calib',\r\n    )(SM_TC_input)\r\n\r\n    lattice_inputs_SM.append(SM_TC_SM_calibrator)\r\n    # lattice_inputs_TR.append(SM_TC_OTH_calibrator)\r\n    # lattice_inputs_CAR.append(SM_TC_OTH_calibrator)\r\n    \r\n  \r\n\r\n    # Swissmetro_HE\r\n    SM_HE_input = tf.reshape(non_split_input[:,5],(-1,1))\r\n    \r\n    ## SM_HE to SM\r\n    SM_HE_SM_calibrator = tfl.layers.PWLCalibration(\r\n    input_keypoints=np.linspace(\r\n        x_train['SM_HE'].min(), x_train['SM_HE'].max(), num=SM_HE_KP),\r\n    dtype=tf.float32,\r\n    output_min=0.0,\r\n    output_max=SM_HE_LS - 1.0,\r\n    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\r\n    monotonicity='decreasing', ## Sign constraints \r\n    name='SM_HE_SM_Calib',\r\n    )(SM_HE_input)\r\n\r\n    ## SM_HE to Others\r\n    SM_HE_OTH_calibrator = tfl.layers.PWLCalibration(\r\n    input_keypoints=np.linspace(\r\n        x_train['SM_HE'].min(), x_train['SM_HE'].max(), num=SM_HE_KP),\r\n    dtype=tf.float32,\r\n    output_min=0.0,\r\n    output_max=SM_HE_LS - 1.0,\r\n    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\r\n    monotonicity='increasing', ## Sign constraints \r\n    name='SM_HE_OTH_Calib',\r\n    )(SM_HE_input)\r\n\r\n    lattice_inputs_SM.append(SM_HE_SM_calibrator)\r\n    # lattice_inputs_TR.append(SM_HE_OTH_calibrator)\r\n    # lattice_inputs_CAR.append(SM_HE_OTH_calibrator)\r\n    \r\n    \r\n    \r\n    \r\n    # CAR_TT\r\n    CAR_TT_input = tf.reshape(non_split_input[:,7],(-1,1))\r\n    \r\n    ## CAR_TT to CAR\r\n    CAR_TT_CAR_calibrator = tfl.layers.PWLCalibration(\r\n    input_keypoints=np.linspace(\r\n        x_train['CAR_TT'].min(), x_train['CAR_TT'].max(), num=CAR_TT_KP),\r\n    dtype=tf.float32,\r\n    output_min=0.0,\r\n    output_max=CAR_TT_LS - 1.0,\r\n    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\r\n    monotonicity='decreasing', ## Sign constraints \r\n    name='CAR_TT_CAR_Calib',\r\n    )(CAR_TT_input)\r\n\r\n    ## CAR_TT to Others\r\n    CAR_TT_OTH_calibrator = tfl.layers.PWLCalibration(\r\n    input_keypoints=np.linspace(\r\n        x_train['CAR_TT'].min(), x_train['CAR_TT'].max(), num=CAR_TT_KP),\r\n    dtype=tf.float32,\r\n    output_min=0.0,\r\n    output_max=CAR_TT_LS - 1.0,\r\n    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\r\n    monotonicity='increasing', ## Sign constraints \r\n    name='CAR_TT_OTH_Calib',\r\n    )(CAR_TT_input)\r\n    lattice_inputs_CAR.append(CAR_TT_CAR_calibrator)\r\n    # lattice_inputs_TR.append(CAR_TT_OTH_calibrator)\r\n    # lattice_inputs_SM.append(CAR_TT_OTH_calibrator)\r\n    \r\n    \r\n    # CAR_TC\r\n    CAR_TC_input = tf.reshape(non_split_input[:,8],(-1,1))\r\n    \r\n    ## CAR_TC to CAR\r\n    CAR_TC_CAR_calibrator = tfl.layers.PWLCalibration(\r\n    input_keypoints=np.linspace(\r\n        x_train['CAR_TC'].min(), x_train['CAR_TC'].max(), num=CAR_TC_KP),\r\n    dtype=tf.float32,\r\n    output_min=0.0,\r\n    output_max=CAR_TC_LS - 1.0,\r\n    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\r\n    monotonicity='decreasing', ## Sign constraints \r\n    name='CAR_TC_CAR_Calib',\r\n    )(CAR_TC_input)\r\n\r\n    ## CAR_TC to Others\r\n    CAR_TC_OTH_calibrator = tfl.layers.PWLCalibration(\r\n    input_keypoints=np.linspace(\r\n        x_train['CAR_TC'].min(), x_train['CAR_TC'].max(), num=CAR_TC_KP),\r\n    dtype=tf.float32,\r\n    output_min=0.0,\r\n    output_max=CAR_TC_LS - 1.0,\r\n    kernel_regularizer=[(\"wrinkle\", 0.0, 0.5),('hessian', 0.0, 0.0001)],\r\n    monotonicity='increasing', ## Sign constraints \r\n    name='CAR_TC_OTH_Calib',\r\n    )(CAR_TC_input)\r\n    lattice_inputs_CAR.append(CAR_TC_CAR_calibrator)\r\n    # lattice_inputs_TR.append(CAR_TC_OTH_calibrator)\r\n    # lattice_inputs_SM.append(CAR_TC_OTH_calibrator)\r\n       \r\n       \r\n    \r\n    \r\n    ## Non-monotonic attributes\r\n    \r\n    ## SM_SEATS\r\n    SM_SEATS_input = tf.reshape(non_split_input[:,6],(-1,1))\r\n    SM_SEATS_calibrator = tfl.layers.CategoricalCalibration(\r\n        num_buckets=2,\r\n        output_min=0.0,\r\n        output_max=SM_SEATS_LS - 1.0,\r\n        name='SM_SEATS_calib',\r\n    )(SM_SEATS_input)\r\n    lattice_inputs_CAR.append(SM_SEATS_calibrator)\r\n    lattice_inputs_TR.append(SM_SEATS_calibrator)\r\n    lattice_inputs_SM.append(SM_SEATS_calibrator)\r\n\r\n    ## GA\r\n    GA_input = tf.reshape(non_split_input[:,9],(-1,1))\r\n    GA_calibrator = tfl.layers.CategoricalCalibration(\r\n        num_buckets=2,\r\n        output_min=0.0,\r\n        output_max=GA_LS - 1.0,\r\n        name='GA_SM_calib',\r\n    )(GA_input)\r\n    lattice_inputs_CAR.append(GA_calibrator)\r\n    lattice_inputs_TR.append(GA_calibrator)\r\n    lattice_inputs_SM.append(GA_calibrator)\r\n    \r\n    ## AGE\r\n    AGE_input = tf.reshape(non_split_input[:,10],(-1,1))\r\n    AGE_calibrator = tfl.layers.CategoricalCalibration(\r\n        num_buckets=5,\r\n        output_min=0.0,\r\n        output_max=AGE_LS - 1.0,\r\n        name='AGE_SM_calib',\r\n    )(AGE_input)\r\n    lattice_inputs_CAR.append(AGE_calibrator)\r\n    lattice_inputs_TR.append(AGE_calibrator)\r\n    lattice_inputs_SM.append(AGE_calibrator)\r\n\r\n    ## INCOME\r\n    INCOME_input = tf.reshape(non_split_input[:,11],(-1,1))\r\n    INCOME_calibrator = tfl.layers.CategoricalCalibration(\r\n        num_buckets=3,\r\n        output_min=0.0,\r\n        output_max=INCOME_LS - 1.0,\r\n        name='INCOME_calib',\r\n    )(INCOME_input)\r\n    lattice_inputs_CAR.append(INCOME_calibrator)\r\n    lattice_inputs_TR.append(INCOME_calibrator)\r\n    lattice_inputs_SM.append(INCOME_calibrator)\r\n    \r\n    \r\n    ## PURPOSE\r\n    PURPOSE_input = tf.reshape(non_split_input[:,12],(-1,1))\r\n    PURPOSE_calibrator = tfl.layers.CategoricalCalibration(\r\n        num_buckets=4,\r\n        output_min=0.0,\r\n        output_max=PURPOSE_LS - 1.0,\r\n        name='PURPOSE_calib',\r\n    )(PURPOSE_input)\r\n    lattice_inputs_CAR.append(PURPOSE_calibrator)\r\n    lattice_inputs_TR.append(PURPOSE_calibrator)\r\n    lattice_inputs_SM.append(PURPOSE_calibrator)   \r\n    \r\n\r\n    ### Non-lienarly fuse the outputs of calibrator\r\n    util_ALT1 = tfl.layers.Lattice(   \r\n    lattice_sizes=[TRAIN_TT_LS,TRAIN_TC_LS,TRAIN_HE_LS,\r\n                   SM_SEATS_LS,GA_LS,AGE_LS,INCOME_LS,PURPOSE_LS],\r\n    monotonicities=[\r\n        'increasing', 'increasing','increasing',\r\n        'none','none','none','none','none'],\r\n\r\n    output_min=0,\r\n    output_max=1,\r\n    # output_min=-100,\r\n    # output_max=100,\r\n    name='lattice_TR',\r\n    )(lattice_inputs_TR)\r\n    \r\n    \r\n    ### Non-lienarly fuse the outputs of calibrator\r\n    util_ALT2 = tfl.layers.Lattice(   \r\n    lattice_sizes=[SM_TT_LS,SM_TC_LS,SM_HE_LS,\r\n                   SM_SEATS_LS,GA_LS,AGE_LS,INCOME_LS,PURPOSE_LS],\r\n    monotonicities=[\r\n        'increasing', 'increasing','increasing',\r\n        'none','none','none','none','none'],\r\n\r\n    output_min=0,\r\n    output_max=1,\r\n    # output_min=-100,\r\n    # output_max=100,\r\n    name='lattice_SM',\r\n    )(lattice_inputs_SM)\r\n    \r\n    ## Non-lienarly fuse the outputs of calibrator\r\n    util_ALT3= tfl.layers.Lattice(   \r\n    lattice_sizes=[CAR_TT_LS,CAR_TC_LS,\r\n                   SM_SEATS_LS,GA_LS,AGE_LS,INCOME_LS,PURPOSE_LS],\r\n    monotonicities=[\r\n        'increasing', 'increasing',\r\n        'none','none','none','none','none'],\r\n\r\n    output_min=0,\r\n    output_max=1,\r\n    # output_min=-100,\r\n    # output_max=100,\r\n    name='lattice_CAR',\r\n    )(lattice_inputs_CAR)\r\n    \r\n    ## Output PWLCalibrator is the key of improving predictability\r\n    util_ALT1 = tfl.layers.PWLCalibration(\r\n    input_keypoints=np.linspace(0.0, 1.0, ACT_TR),\r\n    output_min=-100,\r\n    output_max=100,\r\n    name='output_TR')(util_ALT1)\r\n  \r\n    util_ALT2 = tfl.layers.PWLCalibration(\r\n    input_keypoints=np.linspace(0.0, 1.0, ACT_SM),\r\n    output_min=-100,\r\n    output_max=100,\r\n    name='output_SM')(util_ALT2)\r\n    \r\n    \r\n    util_ALT3 = tfl.layers.PWLCalibration(\r\n    input_keypoints=np.linspace(0.0, 1.0, ACT_CAR),\r\n    output_min=-100,\r\n    output_max=100,\r\n    name='output_CAR')(util_ALT3)\r\n\r\n    out_prob =  tf.keras.layers.Softmax(name='out_prob')(Concatenate()([util_ALT1,util_ALT2,util_ALT3]))\r\n    \r\n    model = Model(non_split_input,[out_prob,util_ALT1,util_ALT2,util_ALT3])   \r\n\r\n    optimizer = Adam(learning_rate=learning_rate)\r\n           \r\n    model.compile(optimizer=optimizer, loss=['categorical_crossentropy',None,None,None],\r\n                  metrics=['accuracy',None,None,None])\r\n    \r\n    return model\r\n\r\nfrom sklearn.metrics import mean_absolute_error,mean_absolute_percentage_error\r\n\r\n\r\n## Repeated Experiments\r\nfor i in range(1):\r\n\r\n    TRAIN_TT_KP = 10\r\n    SM_TT_KP = 30\r\n    CAR_TT_KP = 10\r\n    \r\n    TRAIN_TC_KP = 30\r\n    SM_TC_KP = 30\r\n    CAR_TC_KP = 10\r\n    TRAIN_HE_KP = 10\r\n    SM_HE_KP = 30\r\n        \r\n    TRAIN_TT_LS = 4\r\n    SM_TT_LS = 4\r\n    CAR_TT_LS = 4\r\n    \r\n    TRAIN_TC_LS = 4\r\n    SM_TC_LS = 2\r\n    CAR_TC_LS = 4\r\n\r\n    TRAIN_HE_LS = 2   \r\n    SM_HE_LS = 2\r\n   \r\n    SM_SEATS_LS =2\r\n    GA_LS = 2\r\n    AGE_LS = 3\r\n    INCOME_LS = 2\r\n    PURPOSE_LS = 4\r\n\r\n    ACT_TR = 2\r\n    ACT_SM = 2\r\n    ACT_CAR = 2\r\n    learning_rate = 0.005 #0.0001 is very stable / 0.0005 is not BAD   \r\n\r\n    LatDNN = build_LatDNN(TRAIN_TT_KP,TRAIN_TT_LS,TRAIN_TC_KP,TRAIN_TC_LS,TRAIN_HE_KP,TRAIN_HE_LS,SM_TT_KP,SM_TT_LS,SM_TC_KP,SM_TC_LS,SM_HE_KP,SM_HE_LS,\r\n                     CAR_TT_KP,CAR_TT_LS,CAR_TC_KP,CAR_TC_LS,SM_SEATS_LS,GA_LS,AGE_LS,INCOME_LS,PURPOSE_LS,ACT_TR,ACT_SM,ACT_CAR)\r\n\r\n    batch_size = 128\r\n    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', restore_best_weights=True, verbose=1, patience=20)\r\n\r\n    history = LatDNN.fit(\r\n        x=x_train,\r\n        y=y_train,\r\n        shuffle=True,\r\n        epochs =200,\r\n        batch_size = batch_size,\r\n        validation_data = [x_valid,y_valid],\r\n        callbacks = [es],\r\n        verbose=1)\r\n\r\n    LatDNN.save_weights('C:/Users/euijin/Documents/LatDNN_Models/LatDNN_ASC_FF_SNU1_'+str(i))\r\n    #LatDNN.load_weights('C:/Users/euijin/Documents/LatDNN_Models/LatDNN_ASC_FF_SNU1_'+str(i))\r\n\r\n\r\n\r\n    # Get the prediction metrics\r\n    test_p = LatDNN.predict(x_test,verbose=0)\r\n    train_p = LatDNN.predict(x_train,verbose=0)\r\n\r\n    predict_train = np.argmax(train_p[0],axis=1)\r\n    predict_test =  np.argmax(test_p[0],axis=1)\r\n\r\n    y_train = np.array(df_train.CHOICE).reshape(-1,3)\r\n    y_test = np.array(df_test.CHOICE).reshape(-1,3)\r\n    y_train_cat = np.argmax(np.array(df_train.CHOICE).reshape(-1,3),axis=1)\r\n    y_test_cat = np.argmax(np.array(df_test.CHOICE).reshape(-1,3),axis=1)\r\n\r\n    test_NLL = round((F.cross_entropy(input=torch.log(tensor(test_p[0])),target=torch.Tensor(y_test)).numpy()).item(),3)\r\n    train_NLL =  round((F.cross_entropy(input=torch.log(tensor(train_p[0])),target=torch.Tensor(y_train)).numpy()).item(),3)\r\n\r\n    train_acc = round(accuracy_score(y_train_cat,predict_train),3)\r\n    test_acc = round(accuracy_score(y_test_cat,predict_test),3)\r\n\r\n    train_brier = round(brier_multi(y_train,train_p[0]),3)\r\n    test_brier = round(brier_multi(y_test,test_p[0]),3)\r\n\r\n    print([train_acc,test_acc,train_NLL,test_NLL,train_brier,test_brier])\r\n# In[ ]:\r\n\r\n\r\nfrom sklearn.metrics import mean_absolute_error,mean_absolute_percentage_error\r\nimport pickle\r\nimport time\r\n\r\n## Repeated Experiments\r\nfor i in range(50):\r\n\r\n    # record start time\r\n    start = time.time()\r\n\r\n    TRAIN_TT_KP = 10\r\n    SM_TT_KP = 30\r\n    CAR_TT_KP = 10\r\n\r\n    TRAIN_TC_KP = 20 # SNU2\r\n    SM_TC_KP = 10 # SNU2\r\n    CAR_TC_KP = 10\r\n    TRAIN_HE_KP = 10\r\n    SM_HE_KP = 30\r\n\r\n    TRAIN_TT_LS = 4\r\n    SM_TT_LS = 4\r\n    CAR_TT_LS = 4\r\n\r\n    TRAIN_TC_LS = 4 #\r\n    SM_TC_LS = 2\r\n    CAR_TC_LS = 4\r\n\r\n    TRAIN_HE_LS = 2   \r\n    SM_HE_LS = 2\r\n    SM_SEATS_LS =2\r\n    \r\n    GA_LS = 2\r\n    AGE_LS = 3\r\n    INCOME_LS = 2\r\n    PURPOSE_LS = 4\r\n\r\n    ACT_TR = 2\r\n    ACT_SM = 2\r\n    ACT_CAR = 2\r\n    learning_rate = 0.005 #0.0001 is very stable / 0.0005 is not BAD   \r\n    \r\n    LatDNN = build_LatDNN(TRAIN_TT_KP,TRAIN_TT_LS,TRAIN_TC_KP,TRAIN_TC_LS,TRAIN_HE_KP,TRAIN_HE_LS,SM_TT_KP,SM_TT_LS,SM_TC_KP,SM_TC_LS,SM_HE_KP,SM_HE_LS,\r\n                     CAR_TT_KP,CAR_TT_LS,CAR_TC_KP,CAR_TC_LS,SM_SEATS_LS,GA_LS,AGE_LS,INCOME_LS,PURPOSE_LS,ACT_TR,ACT_SM,ACT_CAR)\r\n\r\n    batch_size = 128\r\n    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', restore_best_weights=True, verbose=1, patience=20)\r\n\r\n    history = LatDNN.fit(\r\n        x=x_train,\r\n        y=y_train,\r\n        shuffle=True,\r\n        epochs =200,\r\n        batch_size = batch_size,\r\n        validation_data = [x_valid,y_valid],\r\n        callbacks = [es],\r\n        verbose=0)\r\n    \r\n\r\n    LatDNN.save_weights('C:/Users/euijin/Documents/LatDNN_Models_Revision/LatDNN_ASC_FF_'+str(i))\r\n    #LatDNN.load_weights('C:/Users/euijin/Documents/LatDNN_Models/LatDNN_ASC_FF_SNU2_'+str(i))\r\n    \r\n    # record end time\r\n    end = time.time()\r\n    training_time = (end-start)\r\n\r\n\r\n    # Get the prediction metrics\r\n    test_p = LatDNN.predict(x_test,verbose=0)\r\n    train_p = LatDNN.predict(x_train,verbose=0)\r\n\r\n    predict_train = np.argmax(train_p[0],axis=1)\r\n    predict_test =  np.argmax(test_p[0],axis=1)\r\n\r\n    y_train = np.array(df_train.CHOICE).reshape(-1,3)\r\n    y_test = np.array(df_test.CHOICE).reshape(-1,3)\r\n    y_train_cat = np.argmax(np.array(df_train.CHOICE).reshape(-1,3),axis=1)\r\n    y_test_cat = np.argmax(np.array(df_test.CHOICE).reshape(-1,3),axis=1)\r\n\r\n    test_NLL = round((F.cross_entropy(input=torch.log(tensor(test_p[0])),target=torch.Tensor(y_test)).numpy()).item(),3)\r\n    train_NLL =  round((F.cross_entropy(input=torch.log(tensor(train_p[0])),target=torch.Tensor(y_train)).numpy()).item(),3)\r\n\r\n    train_acc = round(accuracy_score(y_train_cat,predict_train),3)\r\n    test_acc = round(accuracy_score(y_test_cat,predict_test),3)\r\n\r\n    train_brier = round(brier_multi(y_train,train_p[0]),3)\r\n    test_brier = round(brier_multi(y_test,test_p[0]),3)\r\n\r\n    print([train_acc,test_acc,train_NLL,test_NLL,train_brier,test_brier])\r\n\r\n    ## Plot the PD and ICE\r\n    from sklearn.inspection import partial_dependence\r\n    from sklearn.inspection import PartialDependenceDisplay\r\n    import matplotlib.pyplot as plt\r\n\r\n    class LatDNNFunction(BaseEstimator, ClassifierMixin):\r\n        def fit(self, X, y):\r\n            self.classes_ = unique_labels(y)\r\n            return self\r\n\r\n        def predict_proba(self, X):\r\n            self.proba_,_,_,_ = LatDNN.predict(X,verbose=0)\r\n            return np.array(self.proba_)\r\n\r\n        def decision_function(self, X):\r\n            self.utility_ = np.array(LatDNN.predict(X)[1:4]).transpose().reshape(-1,3)\r\n            return self.utility_\r\n\r\n    LatDNN_ALL = LatDNNFunction()\r\n    LatDNN_ALL.fit(x_train,y_train)\r\n\r\n    common_params = {\r\n        \"subsample\": 0.99999,\r\n        \"n_jobs\": 1,\r\n        \"grid_resolution\": 20,\r\n        \"centered\": True,\r\n        \"random_state\": 1,\r\n        \"response_method\": 'decision_function',\r\n        \"percentiles\": (0.01, 0.99),\r\n\r\n    }\r\n\r\n    fig,ax = plt.subplots(nrows=1,ncols=2, figsize=(12,6),sharex=False,sharey=True)\r\n\r\n    display_0 = PartialDependenceDisplay.from_estimator(\r\n        LatDNN_ALL,\r\n        x_train,\r\n        features=['TRAIN_TT','TRAIN_TC'], # TC,TT\r\n        kind=[\"both\",\"both\"],\r\n        ice_lines_kw={\"color\": \"gray\",'alpha':0.3},pd_line_kw={\"color\": \"black\",\"lw\":3,'linestyle':'solid','label':'True'},\r\n        ax=ax,\r\n        target = 0,\r\n        **common_params,\r\n    )\r\n\r\n    fig,ax = plt.subplots(nrows=1,ncols=2, figsize=(12,6),sharex=False,sharey=True)\r\n\r\n    display_1 = PartialDependenceDisplay.from_estimator(\r\n        LatDNN_ALL,\r\n        x_train,\r\n        features=['SM_TT','SM_TC'], # TC,TT\r\n        kind=[\"both\",\"both\"],\r\n        ice_lines_kw={\"color\": \"gray\",'alpha':0.3},pd_line_kw={\"color\": \"black\",\"lw\":3,'linestyle':'solid','label':'True'},\r\n        ax=ax,\r\n        target = 1,\r\n        **common_params,\r\n    )\r\n\r\n    fig,ax = plt.subplots(nrows=1,ncols=2, figsize=(12,6),sharex=False,sharey=True)\r\n    \r\n    display_2 = PartialDependenceDisplay.from_estimator(\r\n        LatDNN_ALL,\r\n        x_train,\r\n        features=['CAR_TT','CAR_TC'], # TC,TT\r\n        kind=[\"both\",\"both\"],\r\n        ice_lines_kw={\"color\": \"gray\",'alpha':0.3},pd_line_kw={\"color\": \"black\",\"lw\":3,'linestyle':'solid','label':'True'},\r\n        ax=ax,\r\n        target = 2,\r\n        **common_params,\r\n    )\r\n\r\n\r\n\r\n    ## Population-level Parameters\r\n\r\n    PD_TCNN_TT_TR = display_0.pd_results[0]['average'][0]\r\n    PD_TCNN_TC_TR = display_0.pd_results[1]['average'][0] \r\n\r\n    PD_TCNN_TT_SM = display_1.pd_results[0]['average'][1]\r\n    PD_TCNN_TC_SM = display_1.pd_results[1]['average'][1] \r\n\r\n    PD_TCNN_TT_CAR = display_2.pd_results[0]['average'][2]\r\n    PD_TCNN_TC_CAR = display_2.pd_results[1]['average'][2] \r\n\r\n    PD_values_TT_TR = display_0.pd_results[0]['values'][0]\r\n    PD_values_TC_TR = display_0.pd_results[1]['values'][0]\r\n    \r\n    PD_values_TT_SM = display_1.pd_results[0]['values'][0]\r\n    PD_values_TC_SM = display_1.pd_results[1]['values'][0]\r\n    \r\n    PD_values_TT_CAR = display_2.pd_results[0]['values'][0]\r\n    PD_values_TC_CAR = display_2.pd_results[1]['values'][0]\r\n\r\n    beta_TCNN_TT_TR = (np.diff(PD_TCNN_TT_TR)/np.diff(PD_values_TT_TR))\r\n    beta_TCNN_TC_TR = (np.diff(PD_TCNN_TC_TR)/np.diff(PD_values_TC_TR))\r\n\r\n    beta_TCNN_TT_SM = (np.diff(PD_TCNN_TT_SM)/np.diff(PD_values_TT_SM))\r\n    beta_TCNN_TC_SM = (np.diff(PD_TCNN_TC_SM)/np.diff(PD_values_TC_SM))\r\n\r\n    beta_TCNN_TT_CAR = (np.diff(PD_TCNN_TT_CAR)/np.diff(PD_values_TT_CAR))\r\n    beta_TCNN_TC_CAR = (np.diff(PD_TCNN_TC_CAR)/np.diff(PD_values_TC_CAR))\r\n\r\n\r\n    TC_TR_IDX =(beta_TCNN_TC_TR != 0)\r\n    TC_SM_IDX =(beta_TCNN_TC_SM != 0)\r\n    TC_CAR_IDX =(beta_TCNN_TC_CAR != 0)\r\n\r\n    beta_TCNN_TC_TR = beta_TCNN_TC_TR[TC_TR_IDX]\r\n    beta_TCNN_TC_SM = beta_TCNN_TC_SM[TC_SM_IDX]\r\n    beta_TCNN_TC_CAR = beta_TCNN_TC_CAR[TC_CAR_IDX]\r\n    \r\n    beta_TCNN_TT_TR = beta_TCNN_TT_TR[TC_TR_IDX]\r\n    beta_TCNN_TT_SM = beta_TCNN_TT_SM[TC_SM_IDX]\r\n    beta_TCNN_TT_CAR = beta_TCNN_TT_CAR[TC_CAR_IDX]\r\n\r\n    \r\n    beta_TCNN_VOT_POP_TR = np.median(compute_VOT_POP(beta_TCNN_TT_TR,beta_TCNN_TC_TR))\r\n    beta_TCNN_VOT_POP_SM = np.median(compute_VOT_POP(beta_TCNN_TT_SM,beta_TCNN_TC_SM))\r\n    beta_TCNN_VOT_POP_CAR = np.median(compute_VOT_POP(beta_TCNN_TT_CAR,beta_TCNN_TC_CAR))\r\n\r\n\r\n    ## Individual-level Parameters\r\n\r\n    ICE_TCNN_TT_TR = display_0.pd_results[0]['individual'][0]\r\n    ICE_TCNN_TC_TR = display_0.pd_results[1]['individual'][0]\r\n\r\n    ICE_TCNN_TT_SM = display_1.pd_results[0]['individual'][1]\r\n    ICE_TCNN_TC_SM = display_1.pd_results[1]['individual'][1]\r\n\r\n    ICE_TCNN_TT_CAR = display_2.pd_results[0]['individual'][2]\r\n    ICE_TCNN_TC_CAR = display_2.pd_results[1]['individual'][2]\r\n\r\n    beta_TCNN_TT_TR_ICE = (np.diff(ICE_TCNN_TT_TR)/np.diff(PD_values_TT_TR))\r\n    beta_TCNN_TC_TR_ICE = (np.diff(ICE_TCNN_TC_TR)/np.diff(PD_values_TC_TR))\r\n    beta_TCNN_VOT_TR_ICE = compute_VOT_IND(beta_TCNN_TT_TR_ICE[:,TC_TR_IDX],beta_TCNN_TC_TR_ICE[:,TC_TR_IDX])\r\n\r\n    beta_TCNN_TT_SM_ICE = (np.diff(ICE_TCNN_TT_SM)/np.diff(PD_values_TT_SM))\r\n    beta_TCNN_TC_SM_ICE = (np.diff(ICE_TCNN_TC_SM)/np.diff(PD_values_TC_SM))\r\n    beta_TCNN_VOT_SM_ICE = compute_VOT_IND(beta_TCNN_TT_SM_ICE[:,TC_SM_IDX],beta_TCNN_TC_SM_ICE[:,TC_SM_IDX])\r\n\r\n\r\n    beta_TCNN_TT_CAR_ICE = (np.diff(ICE_TCNN_TT_CAR)/np.diff(PD_values_TT_CAR))\r\n    beta_TCNN_TC_CAR_ICE = (np.diff(ICE_TCNN_TC_CAR)/np.diff(PD_values_TC_CAR))\r\n    beta_TCNN_VOT_CAR_ICE = compute_VOT_IND(beta_TCNN_TT_CAR_ICE[:,TC_CAR_IDX],beta_TCNN_TC_CAR_ICE[:,TC_CAR_IDX])\r\n\r\n\r\n    # Estimation # If we use the median value, we don't need to care about the outlier or somethings.\r\n  \r\n    x_test_E = x_train[['AGES','INCOMES','PURPOSES']].reset_index(drop=True)\r\n    x_test_E['VOT_TR'] = beta_TCNN_VOT_TR_ICE\r\n    x_test_E['VOT_SM'] = beta_TCNN_VOT_SM_ICE\r\n    x_test_E['VOT_CAR'] = beta_TCNN_VOT_CAR_ICE\r\n\r\n    VOT_TCNN_IND = x_test_E[['AGES','INCOMES','PURPOSES','VOT_TR','VOT_SM','VOT_CAR']].groupby(['AGES','INCOMES','PURPOSES']).median()\r\n   \r\n        \r\n    print(np.quantile(VOT_TCNN_IND['VOT_TR'],(0.5,0.01,0.25,0.75,0.99)).round(3))\r\n    print(np.quantile(VOT_TCNN_IND['VOT_SM'],(0.5,0.01,0.25,0.75,0.99)).round(3))\r\n    print(np.quantile(VOT_TCNN_IND['VOT_CAR'],(0.5,0.01,0.25,0.75,0.99)).round(3))\r\n    \r\n\r\n    # Estimation results\r\n    PRED_PERF = np.array([train_acc,test_acc,train_brier,test_brier]).transpose()\r\n    EST_RESULTS = [PRED_PERF,VOT_TCNN_IND,training_time]\r\n    \r\n    \r\n    with open(\"C:/Users/euijin/Documents/LatDNN_Results_Revision/ASC_RESULTS_FF_\"+str(i), \"wb\") as f:\r\n        pickle.dump(EST_RESULTS, f)\r\n    #np.savetxt(\"C:/Users/euijin/Documents/LatDNN_Results_Revision/ASC_RESULTS_FF_\"+str(i),EST_RESULTS)\r\n    #save_clipboard(np.concatenate([PRED_PERF,VOT_TCNN_IND.median(axis=0).round(3)])) \r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nimport scipy.stats as stats\r\n## Results analysis\r\nPRED_PERF_S = []\r\nVOT_TCNN_IND_S = []\r\nVOT_TCNN_index = []\r\n\r\nfor i in range(18):\r\n    with open(\"C:/Users/euijin/Documents/LatDNN_Results_Revision/ASC_RESULTS_FF_\"+str(i),'rb') as f:\r\n        temp = pickle.load(f)  \r\n    PRED_PERF_S.append(temp[0])\r\n    VOT_TCNN_IND_S.append(temp[1])\r\n    VOT_TCNN_index.append(temp[1].index.values)\r\n    \r\n\r\n## Predictability    \r\nPRED_PERF_S = np.array(PRED_PERF_S)\r\nA = np.concatenate([PRED_PERF_S.mean(axis=0).round(3).reshape(-1,1),PRED_PERF_S.std(axis=0).round(3).reshape(-1,1)],axis=1)\r\n\r\n\r\n## Interpretability\r\nIND_stats = []\r\nVOT_TCNN_IND_S_TR = []\r\nVOT_TCNN_IND_S_SM = []\r\nVOT_TCNN_IND_S_CAR = []\r\n\r\nfor i in range(len(VOT_TCNN_IND_S)):\r\n    \r\n    inter_IDX_IND = VOT_TCNN_IND_S[i].index.isin(np.intersect1d(VOT_TCNN_IND_S[i].index,df_VOT_B.index))\r\n        \r\n    TRIND = np.quantile(VOT_TCNN_IND_S[i].iloc[inter_IDX_IND,0],(0.5,0.01,0.25,0.75,0.99)).round(3)\r\n    SMIND = np.quantile(VOT_TCNN_IND_S[i].iloc[inter_IDX_IND,1],(0.5,0.01,0.25,0.75,0.99)).round(3)\r\n    CARIND = np.quantile(VOT_TCNN_IND_S[i].iloc[inter_IDX_IND,2],(0.5,0.01,0.25,0.75,0.99)).round(3)   \r\n    ALLIND = np.concatenate([TRIND,SMIND,CARIND])   \r\n    IND_stats.append(ALLIND)\r\n    \r\n    VOT_TCNN_IND_S_TR.append(VOT_TCNN_IND_S[i].iloc[inter_IDX_IND,0])\r\n    VOT_TCNN_IND_S_SM.append(VOT_TCNN_IND_S[i].iloc[inter_IDX_IND,1])\r\n    VOT_TCNN_IND_S_CAR.append(VOT_TCNN_IND_S[i].iloc[inter_IDX_IND,2])\r\n      \r\nB = np.array(IND_stats)\r\nB = np.concatenate([np.median(B,axis=0).round(3).reshape(-1,1),\r\n                    stats.iqr(B,axis=0).round(3).reshape(-1,1)],axis=1)\r\n\r\n\r\nVOT_TCNN_IND_S_TR = np.concatenate(VOT_TCNN_IND_S_TR)\r\nVOT_TCNN_IND_S_SM = np.concatenate(VOT_TCNN_IND_S_SM)\r\nVOT_TCNN_IND_S_CAR = np.concatenate(VOT_TCNN_IND_S_CAR)\r\n\r\nEST_RESULTS_TCNN = np.vstack([B,A])\r\nsave_clipboard(EST_RESULTS_TCNN)\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nimport scipy.stats as stats\r\n\r\n## Results analysis of MNL\r\n\r\nPRED_PERF_S = []\r\nPARAM_PERF_S = []\r\nVOT_MNL_IND_S = []\r\nVOT_TRUE_S = []\r\nVOT_MNL_index = []\r\n\r\n\r\n## Results analysis of TCNN\r\n\r\nPRED_PERF_S = []\r\nPARAM_PERF_S = []\r\nVOT_TCNN_IND_S = []\r\nVOT_TRUE_S = []\r\nVOT_TCNN_index = []\r\n\r\nfor i in range(1):\r\n    with open(\"C:/Users/euijin/Documents/LatDNN_Results_Revision/ASC_RESULTS_FF_\"+str(i),'rb') as f:\r\n        temp = pickle.load(f)  \r\n    PRED_PERF_S.append(temp[0])\r\n    VOT_TCNN_IND_S.append(temp[1])\r\n    VOT_TCNN_index.append(temp[1].index.values)\r\n\r\ndf_VOT_TCNN = pd.DataFrame(np.vstack(VOT_TCNN_IND_S),index = np.hstack(VOT_TCNN_index),columns = ['VOT_TR','VOT_SM','VOT_CAR'])\r\n\r\n\r\n## Results analysis of DNN\r\nPRED_PERF_S = []\r\nPARAM_PERF_S = []\r\nVOT_DNN_IND_S = []\r\nVOT_TRUE_S = []\r\nVOT_DNN_index = []\r\n\r\nfor i in range(1):\r\n    with open(\"C:/Users/euijin/Documents/DNN_Results_Revision/FUL_RESULTS_FF_\"+str(i),'rb') as f:\r\n        temp = pickle.load(f)  \r\n    PRED_PERF_S.append(temp[0])\r\n    VOT_DNN_IND_S.append(temp[1])\r\n    VOT_DNN_index.append(temp[1].index.values)\r\n\r\ndf_VOT_DNN = pd.DataFrame(np.vstack(VOT_DNN_IND_S),index = np.hstack(VOT_DNN_index),columns = ['VOT_TR','VOT_SM','VOT_CAR'])\r\n\r\n## Results analysis of MNL\r\ndf_VOT_MNL = pd.DataFrame(df_VOT_B.values[:,1:4],index = df_VOT_B.index.values,columns =['VOT_TR','VOT_SM','VOT_CAR'])", "output": "Understood. Knowledge stored."}
{"instruction": "User feedback: Refinement request: i want change...please create full python code", "output": "We incorporate your feedback."}

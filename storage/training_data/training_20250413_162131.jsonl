{"instruction": "Title: A Neural-embedded Discrete Choice Model: Learning Taste Representation with Strengthened Interpretability\nPaper:\nA Neural-embedded Discrete Choice Model:\r\nLearning Taste Representation with Strengthened Interpretability\r\nYafei Hana\r\n, Francisco Camara Pereirab\r\n, Moshe Ben-Akivac\r\n, Christopher Zegrasd\r\na Civil and Environmental Engineering, Massachusetts Institute of Technology\r\n(yafei@alum.mit.edu, corresponding author)\r\nb Department of Technology, Management and Economics, Technical University of Denmark\r\n(camara@dtu.dk)\r\nc Civil and Environmental Engineering, Massachusetts Institute of Technology\r\n(mba@mit.edu)\r\nd Department of Urban Studies and Planning, Massachusetts Institute of Technology\r\n(czegras@mit.edu)\r\narXiv:2002.00922v2 [econ.EM] 1 Jul 2022\r\nABSTRACT\r\nDiscrete choice models (DCMs) require a priori knowledge of the utility functions,\r\nespecially how tastes vary across individuals. Utility misspecification may lead to\r\nbiased estimates, inaccurate interpretations and limited predictability. In this paper,\r\nwe utilize a neural network to learn taste representation. Our formulation consists of\r\ntwo modules: a neural network (TasteNet) that learns taste parameters (e.g., time\r\ncoefficient) as flexible functions of individual characteristics; and a multinomial logit\r\n(MNL) model with utility functions defined with expert knowledge. Taste parameters learned by the neural network are fed into the choice model and link the two\r\nmodules.\r\nOur approach extends the L-MNL model (Sifringer et al., 2020) by allowing\r\nthe neural network to learn the interactions between individual characteristics and\r\nalternative attributes. Moreover, we formalize and strengthen the interpretability\r\ncondition - requiring realistic estimates of behavior indicators (e.g., value-of-time,\r\nelasticity) at the disaggregated level, which is crucial for a model to be suitable\r\nfor scenario analysis and policy decisions. Through a unique network architecture\r\nand parameter transformation, we incorporate prior knowledge and guide the neural\r\nnetwork to output realistic behavior indicators at the disaggregated level.\r\nWe show that TasteNet-MNL reaches the ground-truth model\u2019s predictability\r\nand recovers the nonlinear taste functions on synthetic data. Its estimated value-oftime and choice elasticities at the individual level are close to the ground truth. In\r\ncontrast, exemplary logit models with misspecified systematic utility lead to biased\r\nparameter estimates and lower prediction accuracy. On a publicly available Swissmetro dataset, TasteNet-MNL outperforms benchmarking MNLs and Mixed Logit\r\nmodel\u2019s predictability. It learns a broader spectrum of taste variations within the\r\npopulation and suggests a higher average value-of-time. Our source code is available\r\nfor research and application.\r\nKEYWORDS\r\nDiscrete choice models; Neural networks; Taste heterogeneity; Interpretability;\r\nUtility specification; Machine learning; Deep learning\r\n2\r\n1. Introduction\r\nDiscrete choice model (DCM) is a robust econometric framework to understand and\r\npredict choice behaviors. With a solid foundation in random utility theory, DCM has\r\nthe significant advantage of interpretability: it can explain how individuals choose\r\namong a set of alternatives and provide reliable answers to \u201cwhat-if\u201d scenario questions, which is crucial for making policy decisions.\r\nA DCM requires making assumptions about a choice-maker\u2019s decision rules, information processing strategies, consideration sets and utility functions. Various approaches are developed to capture heterogeneity in these aspects, such as Ba\u00b8sar and\r\nBhat (2004), Hess et al. (2012), van Cranenburgh and Alwosheel (2019), Barseghyan\r\net al. (2021), and Crawford et al. (2021). Under the most widely used random utility\r\nmaximization paradigm, how to characterize taste heterogeneity in the utility specification has been a main research topic. The systematic part of a utility function\r\ndescribes how a choice-maker values an alternative attribute (\u201ctaste\u201d), and how tastes\r\nvary across choice-makers (\u201ctaste heterogeneity\u201d), usually through the interactions\r\nbetween alternative attributes and individual characteristics. Correctly specifying systematic taste heterogeneity can be challenging, especially when the choice problem is\r\nnovel or complicated without a priori knowledge. Misspecified utility functions can\r\nlead to lower predictability, biased estimates and misinformed decisions (Bentz and\r\nMerunka, 2000, Torres et al., 2011, van der Pol et al., 2014).\r\nNonlinear functions (e.g., higher-order polynomial, semi-log transform, piecewise\r\nlinear) can characterize systematic taste heterogeneity in more flexible ways. However,\r\nit is unwieldy to specify such utility functions as there are many possible nonlinear\r\ntransformations to be tested. Advanced DCMs, such as the Mixed Logit Model and\r\nthe Latent Class Choice Model, are developed to model random/unobserved taste heterogeneity not captured by the systematic utility (McFadden and Train, 2000, Hensher\r\nand Greene, 2003, Gupta and Chintagunta, 1994, Gopinath, 1995). These models substantially enrich taste heterogeneity representation and improve predictability. However, they require both the systematic utility and the random error structure known\r\nas a priori. Capturing random heterogeneity alone does not resolve the bias induced\r\nby misspecified systematic utilities (Ben-Akiva et al., 2002).\r\nIn the machine learning (ML) community, Neural Networks (NN) have become\r\nthe state-of-art method in many computer vision and natural language processing\r\napplications due to superior predictability compared to theory-driven models. Neural\r\nnetworks are shown as a universal function approximator (Cybenko, 1989, Hornik,\r\n1991), having the ability to learn any arbitrary function from data. This empowers\r\nthem to learn flexible representation from large datasets, the complexity of which can\r\nbe difficult to achieve with hand-engineered features by domain experts.\r\nNumerous studies have examined NN\u2019s predictability in the context of mobility\r\nchoice and shown higher accuracy. However, a significant limitation of NN is the lack\r\nof interpretability. Interpretability is crucial for high-stakes decisions in transportation\r\nplanning, such as infrastructure investment and congestion pricing. The model should\r\nrepresent the real relationships between explanatory variables and choice outcomes\r\nand provide reliable answers to \u201cwhat-if\u201d questions at the disaggregated level.\r\nRecent studies utilize NN as a tool to learn flexible behavior representation within\r\nthe DCM framework while keeping model interpretability (van Cranenburgh and Alwosheel, 2019, Han, 2019, Sifringer et al., 2020). The key idea is injecting a neural\r\nnetwork into a DCM such that the neural network learns part of the model specification, while the base model is a DCM. We call this type of design the \u201cNeural-embedded\r\n3\r\nDiscrete Choice Model\u201d (NEDCM) (Han, 2019). Sifringer et al. (2020) develop the first\r\ntype of such models (L-MNL and L-NL), using neural networks to learn a data-driven\r\npart of the utility specification.\r\nWe extend Sifringer et al. (2020)\u2019s L-MNL to a structure called TasteNet-MNL. A\r\nneural network (TasteNet) is embedded in an MNL to learn representations of systematic taste heterogeneity. Specifically, a set of taste coefficients in a utility function\r\nare learned by a neural network as functions of individual characteristics. Taste coefficients predicted by the neural network are fed into an MNL with explicitly defined\r\nutilities. Unknown parameters of the TasteNet and the MNL are jointly estimated.\r\nWe formalize and strengthen the model interpretability condition. We show that disaggregated interpretability can be achieved by a constrained network architecture and\r\nparameter transform that encodes expert knowledge. The source code is made publicly\r\navailable1\r\n.\r\nWe organize the rest of this paper as follows. Section 2 reviews previous NN applications to travel choice prediction, learning behavior representation, and the challenge\r\nof interpretability. Section 3 describes the Taste-MNL structure and implementation.\r\nSection 4 shows that TasteNet-MNL can recover the underlying taste heterogeneity\r\nand outperform MNLs with under-specified utilities. Section 5 applies TasteNet-MNL\r\nto the Swissmetro dataset and shows its higher predictability than MNLs and Mixed\r\nLogit benchmarks, and its interpretability at the disaggregated level. Lastly, we discuss\r\nour key contributions, limitations of the model, and future works.\r\n2. Literature Review\r\n2.1. Predictability\r\nNeural networks have been long treated as a black-box for travel choice prediction.\r\nThe majority of these studies find the examined NN outperforms a certain type of\r\nDCM, such as MNL (Kumar et al., 1995, Agrawal and Schorling, 1996, West et al.,\r\n1997, De Carvalho et al., 1998, Omrani, 2015, Lee et al., 2018, Lh\u00b4eritier et al., 2019),\r\nnested logit (Mohammadian and Miller, 2002, Cantarella and de Luca, 2005, Wang\r\net al., 2020c), mixed logit (Wang et al., 2021) and latent class logit model (Lh\u00b4eritier\r\net al., 2019). A few studies find similar prediction performance of NN in comparison\r\nto nested logit (Hensher and Ton, 2000), MNL (Sayed and Razavi, 2000, Zhao et al.,\r\n2018) and mixed logit (Zhao et al., 2018). Neural network\u2019s superior predictability\r\nis attributed to its universal function approximation capacity (Hornik, 1991): a NN\r\nis able to learn complex nonlinear functions, which can be under-represented in a\r\nparametric or semi-parametric DCM setting.\r\nHowever, a deep neural network (DNN) with many hidden layers is prone to overfitting and poor performance on test datasets. Several studies show that increasing\r\nthe number of hidden layers does not improve predictability and can rather harm\r\nit. Nam et al. (2017) shows similar log-likelihood of a DNN compared to the nested\r\nlogit and cross-nested logit baselines. Wong and Farooq (2019) find a feed-forward\r\nDNN suffers from over-fitting as the number of hidden layers increases. With 4 or\r\nmore hidden layers, log-likelihood on the validation dataset no longer improves and\r\nperforms worse than the MNL benchmark. In addition to over-fitting, a DNN often\r\nexperiences large variations across model runs (Glorot et al., 2011). In a recent study,\r\nWang et al. (2021) conduct a systematic comparison of 11 ML classifiers with 3 types\r\n1https://github.com/YafeiHan-MIT/TasteNet-MNL\r\n4\r\nof DCM (logit, nested logit and mixed logit) on 3 datasets with varying sample sizes.\r\nThey find DNNs and ensemble methods achieve the highest predictability on average\r\nand outperform DCMs by 3-4%. This study shows that an overly complex DNN or\r\nDNNs with poorly tuned hyper-parameters can under-perform a DCM due to its large\r\nestimation error.\r\nTo prevent a NN from over-fitting and improve its predictability, regularization\r\nstrategies such as l2 regularization and drop-out, are commonly employed for NN training. Alternatively, more constrained network architectures are proposed with domain\r\nknowledge. Wang et al. (2020a) use alternative-specific networks and find improved\r\npredictability compared to fully-connected DNNs. Sifringer et al. (2020)\u2019s L-MNL integrates a NN with a logit model, and uses NN to learn part of the utility. Though\r\ninterpretability and representation learning is their main motivations, the more customized NN structure can be seen as a regularization strategy. Our study presents a\r\nnew way to integrate NN and MNL, where NN is utilized to learn taste heterogeneity\r\nand constrained by the MNL. We also introduce parameter constraints to incorporate\r\nexpert knowledge and improve predictability.\r\n2.2. Interpretability\r\nA major criticism against NN for travel demand modeling is its black-box nature\r\nand a lack of interpretability. However, the term \u201cinterpretability\u201d has not been\r\nclearly defined or agreed upon. Interpretability has been a hot topic in the machine\r\nlearning community, Lipton (2017) summarizes different motivations behind the quest\r\nfor interpretability, such as trust, understandability, transparency, causality, transferability, informativeness, and fairness. He classifies the properties and techniques that\r\nrender a model interpretable into two general categories: transparency - to what\r\nextent a model can be understood by human, and post-hoc interpretability - the\r\nability to derive useful information from a model. We find this framework helpful to\r\nreflect on what kind of \u201cinterpretability\u201d is needed for transportation applications. In\r\nthe same spirit, Alwosheel (2020) categorizes the effort to make NN interpretable as:\r\nthe ante-hoc approach - incorporating interpretability into the model structure; and\r\nthe post-hoc approach - deriving useful information from models. As interpretability\r\nresearches spread across many disciplines, and our study is not primarily dedicated\r\nto interpretability, our review focuses on interpretability of NNs for travel demand\r\nanalysis.\r\nThe conventional view of NN as a \u201cblack-box\u201d (Hensher and Ton, 2000, Karlaftis\r\nand Vlahogianni, 2011) follows the \u201ctransparency\u201d type of interpretability: A NN offers\r\nno direct behavioral meaning from the parameters, and it is difficult for a human\r\nto understand how it makes certain predictions. In a similar vein, Sifringer et al.\r\n(2020) considers only the parametric part of the utility as interpretable, and associate\r\ninterpretability with obtaining unbiased estimates for the transparent part.\r\nPerhaps a more popular view of interpretability is the \u201cpost-hoc\u201d kind - having\r\nthe ability to obtain useful information from a model (Zhao et al., 2018, Wang et al.,\r\n2020a). Researches from early days have shown the analogy between a logit model and\r\nan FFW, and the ways to derive economic information from NNs (Bentz and Merunka,\r\n2000, Hruschka et al., 2002, 2004). Chiang et al. (2006), Hagenauer and Helbich (2017)\r\nand Golshani et al. (2018) conduct sensitivity analysis to measure variable importance.\r\nWang et al. (2020b) show how to extract a full list of economic indicators from NNs.\r\nThough we have plenty of tools to interpret a NN, a major concern in applying\r\n5\r\nNN for travel demand analysis is the credibility and consistency of its interpretations.\r\nWang et al. (2020b) note that economic indicators aggregated over model runs make\r\nsense, but results from particular training runs or for individual observations can be\r\nunreasonable. For example, choice probability can be non-monotonically decreasing as\r\ncost increases and highly sensitive to a particular model run. The derivative of choice\r\nprobabilities with respect to cost and time can be positive; and value-of-time can be\r\nnegative, zero, arbitrarily large, or infinite.\r\nScenario analysis and policy decision in the field of transportation depends on answers to \u201cwhat-if\u201d questions at the disaggregated level, such as how individuals with\r\nspecific characteristics would respond to a toll increase. Prediction performance alone\r\ndoes not guarantee correct answers to these questions, as a predictive NN does not\r\nnecessarily learn the underlying relationships. With an emphasis on trustworthiness,\r\nAlwosheel (2020) frames interpretability as to what degree the NN learns intuitive and\r\ndesirable relations and hence can be trusted. He adapts the method of prototypical\r\nexamples used in computer vision to diagnose NNs\u2019 trustworthiness for discrete choice\r\nanalysis.\r\nWe share a similar emphasis on trustworthiness in addition to post-hoc information\r\nextraction. We define interpretability for transportation applications as follows:\r\nA model is interpretable if, at the disaggregated level, it can provide credible answers\r\nto \u201cwhat will happen if \u201d questions. We emphasize the credibility of the economic\r\nindicators and interpretability at the disaggregated (both model and choice-maker)\r\nlevel. By \u201ccredible\u201d, we mean the answer should conform with a set of prior knowledge,\r\nfor example, non-positive choice elasticity regarding cost and non-positive value-oftime. Prior knowledge may change over time and vary across application contexts.\r\nThe main interpretability difficulty for neural networks arises from the fact that\r\nmany NNs can fit the data equally well, but not all can provide realistic behavior\r\ninterpretations. To improve interpretability, we design a hybrid structure consisting\r\nof a neural network and an MNL, and add parameter constraints to reflect expert\r\nknowledge. We show that TasteNet-MNL can obtain realistic behavioral indicators at\r\nthe disaggregated level, and that predictability does not necessarily come at the cost\r\nof interpretability.\r\n2.3. Neural networks for learning behavior representation\r\nBeyond the prediction focus in most NN applications, an interesting avenue of studies\r\nattempts NN to uncover the complex relationships in choice behaviors (West et al.,\r\n1997, De Carvalho et al., 1998, Bentz and Merunka, 2000, Wong et al., 2018, van\r\nCranenburgh and Alwosheel, 2019). For example, Wong et al. (2018) use a restricted\r\nBoltzman Machine to capture latent behavior attributes. van Cranenburgh and Alwosheel (2019) develop a NN to learn decision rule heterogeneity.\r\nThere has been a continuing interest in learning flexible utility forms and behavior\r\ninterpretations with NNs, which is the direct precedent of our study. Early studies\r\nconduct Monte-Carlo experiments to show a neural network can capture nonlinearity\r\nin utility functions (West et al., 1997, De Carvalho et al., 1998, Bentz and Merunka,\r\n2000). Non-linearity may reflect the saturation effect or threshold effect of attributes\r\non utility, or non-compensatory decision rules. For example, West et al. (1997) find\r\nthat NNs consistently outperform logit and discriminative analysis when predicting\r\nthe outcome of non-compensatory choice rules. Bentz and Merunka (2000) show the\r\nanalogy between NN and MNL, and NN with hidden layers as a more general version\r\n6\r\nof MNL. With synthetic data and an empirical study, they show that a NN can detect\r\ninteraction and threshold effects in utility, and therefore can be used as a diagnostic\r\ntool to improve MNL utility specification. This sequential approach requires manual\r\nanalysis of NN results to identify the nonlinear effect, and thus applies only to simple\r\nproblems. Hruschka et al. (2002) compare a NN with an MNL and a Latent Class Logit\r\n(LCL) model in an empirical study of brand choice, and find the NN model can identify\r\ninteraction effects, threshold effects, saturation effects and other nonlinear forms (like\r\ninverse S-shape) of attributes on brand utility. A follow-up study by Hruschka et al.\r\n(2004) compares NN with two other MNLs with flexible systematic utility, and draws\r\nsimilar conclusions. These studies show that a NN can outperform an MNL, when the\r\nnonlinearity in attributes are neglected or mistaken. However, these studies have not\r\naddressed nonlinearity in taste. They consider NN as either an alternative to MNL;\r\nor a diagnostic tool to improve the utility specification of MNL, which works only for\r\nsimple problems.\r\nInspired by the study by Bentz and Merunka (2000), Sifringer et al. (2020) propose\r\nto integrate NN and DCM, and learn utility representations with the NN. They divide\r\nthe systematic utility into a \u201cknowledge-driven\u201d part and a \u201cdata-driven\u201d part learned\r\nby a neural network. This structure reduces bias in estimating the knowledge-driven\r\npart of the utility, and enhances predictability while maintaining interpretability. Applying this idea to logit and nested-logit, they propose the L-MNL and L-NL structure.\r\nWe extend L-MNL to TasteNet-MNL, using a NN to learn flexible representation of\r\ntaste functions. In L-MNL, the data-driven part and the knowledge-driven part of the\r\nutility are added. Variables in the two components neither overlap nor interact with\r\neach other. This additive structure can be restrictive since the unused features can\r\ninteract with variables in the knowledge-driven part of the utility. A common scenario\r\nis taste heterogeneity - certain individual characteristics interacting with alternative\r\nattributes. L-MNL can be viewed as learning flexible representations of the Alternative\r\nSpecific Constants (ASCs) as functions of the unused features (individual characteristics and/or attributes of alternatives), though the data-driven part of the utility it\r\nlearns is no longer a \u201cconstant\u201d per alternative, but an Individual Specific Constant\r\nper alternative.\r\nTasteNet-MNL differs from L-MNL and traditional FFWs in three aspects. First,\r\nwe allow all or a subset of taste parameters to be modeled by an FFW as a flexible\r\nfunction. Second, we impose constraints on taste parameters generated by the neural\r\nnetwork according to prior knowledge, to regularize the network and obtain interpretable results. Third, we model taste parameters instead of the systematic utilities\r\nby a NN, different from the conventional application of an FFW. The key idea is to\r\nassign the more complicated or less known task (taste heterogeneity) to a NN while\r\nkeeping the well-known part parametric.\r\n3. Model Structure\r\nProblem Setup Suppose for a choice task, each of N individuals makes a onetime choice from an individual-specific choice set Cn\r\n2\r\n. Observed data for individual\r\nn includes personal characteristics zn, attributes of each alternative xin(\u2200i \u2208 Cn),\r\nand the choice yn. The conditional probability of person n choosing alternative i is\r\nP(yn = i|zn, xjn\u2200j \u2208 Cn).\r\n2This problem setup can be generalized to repeated choice. We choose one-time choice to simplify the notation\r\n7\r\nIf tastes are homogeneous across individuals, the systematic utility (Vin) of alternative i to a person n can be specified as a linear combination of Ki attributes (Eqn. 1),\r\nwhere \u03b2ki represents homogeneous taste for attribute xki; and \u03b20i\r\nis the alternativespecific constant for alternative i.\r\nMore realistically, tastes vary across individuals. A systematic utility typically includes interactions between alternative attributes and individual characteristics. Eqn.\r\n2 shows such an example, where \u03b3pqi represents the interaction effect between the p-th\r\nattribute of alternative i (xpin), and the q-th characteristics (zqn) of individual n; and\r\nIi\r\nis a set of such pairwise interactions between alternative i\u2019s attributes and individual characteristics. Interaction effects are specified based on theory or hypothesis.\r\nGiven numerous function forms and set of interactions, it can be challenging to test\r\nall possible scenarios.\r\nVin = \u03b20i +\r\nX\r\nKi\r\nk=1\r\n\u03b2kixkin (1)\r\nVin = \u03b20i +\r\nX\r\nKi\r\nk=1\r\n\u03b2kixkin +\r\nX\r\n(p,q)\u2208Ii\r\n\u03b3pqixpinzqn (2)\r\n3.1. Learn systematic taste heterogeneity with neural networks\r\nWe use a neural network to model the interactions between alternative attributes and\r\nindividual characteristics. The overall TasetNet-MNL model consists of two modules:\r\na neural network (TasteNet) and a choice module (MNL).\r\nTasteNet models individual n\u2019s taste parameters as flexible functions of this person\u2019s\r\ncharacteristics (Eqn. 3). The network takes person n\u2019s characteristics (zn) as inputs,\r\nand outputs a vector of taste coefficients \u03b2n\r\nT N . w is the weights of the network. \u03b2n\r\nT N\r\ncorrespond to all or a subset of taste coefficients. The meaning of each element of \u03b2n\r\nT N\r\nis determined by the utility specification in the MNL choice module. The elements of\r\n\u03b2n\r\nT N can be alternative-specific, such as mode-specific travel time coefficients in a\r\ntravel mode choice problem.\r\n\u03b2\r\nT N\r\nn = T asteNet(zn; w) (3)\r\nWe specify the choice module as a logit model, which can be extended to other\r\nmodel forms such as nested logit. The systematic utility of alternative i is divided into\r\ntwo parts: 1) a flexible part, defined as a dot product of taste coefficients predicted by\r\nTasteNet and alternative i\u2019s corresponding attributes with flexible coefficients (x\r\nT N\r\nin ,\r\nTN for TasteNet); and 2) a parametric part, a utility function f with attributes xMNL\r\nin\r\nand characteristics zMNL\r\nn\r\nas inputs and \u03b2MNL\r\ni\r\nas parameters (Eqn. 4).\r\nAccording to the condition to obtain unbiased estimates for the MNL part given by\r\nSifringer et al. (2020), inputs to the neural network should not overlap with variables\r\nentering the parametric part of the utility. TasteNet by definition only takes individual\r\ncharacteristics as inputs, and thus separates them from attributes entering the parametric part (z\r\nT N\r\nn \u2229 xMNL\r\nin = \u2205). Additionally, TasteNet and MNL should not share\r\n8\r\nFigure 1.: Diagram of a TasteNet-MNL (TasteNet as a feed-forward neural network\r\nwith 1 hidden layer)\r\nindividual characteristics (z\r\nT N\r\nn \u2229 zMNL\r\nn = \u2205).\r\nThe extent to which we let a neural network learn the taste function is up to\r\nthe modeler. In principle, when we lack confidence about how certain tastes vary\r\namong individuals, we let TasteNet learn. Each taste coefficient is either predicted by\r\nTasteNet or estimated in the parametric utility. This is not necessary but desirable for\r\nclarity. When a taste function is learned by both TasteNet and MNL, the TasteNet will\r\nlearn the \u201cresidual\u201d part of the function and should be combined with the parametric\r\npart to recover the full taste function.\r\nFigure 1 depicts the overall structure of the TasteNet-MNL. TasteNet learns a function mapping from individual characteristics to tastes. The MNL module combines the\r\nflexible and the parametric utility to compute the choice probability for each alternative (Eqn. 5) and choice likelihood. The unknown parameters to estimate are neural\r\nnetwork weights (w) and coefficients (\u03b2MNL) in the parametric utilities. They are\r\nlearned jointly by maximizing the likelihood function.\r\nVin = T asteNet(zn\r\nT N ; w)\r\n0\r\nixin\r\nT N + f(xin\r\nMNL\r\n, zn\r\nMNL; \u03b2i\r\nMNL) (4)\r\nP(yn = i|xn, zn, w, \u03b2\r\nMNL) = e\r\nT asteNet(zn\r\nTN ;w)\r\n0\r\nixin\r\nTN +f(xin\r\nMNL,zn\r\nMNL;\u03b2i\r\nMNL)\r\nP\r\nj\u2208Cn\r\ne\r\nT asteNet(zn\r\nTN ;w)\r\n0\r\njxjnTN +f(xjnMNL,zn\r\nMNL;\u03b2j\r\nMNL)\r\n(5)\r\nTasteNet-MNL is distinguished from previous studies in several ways. First, it extends the L-MNL (Sifringer et al., 2020) by using a neural network to learn the interactions between characteristics and attributes. Second, different from the majority\r\nof neural network applications to discrete choice, TasteNet learns a representation\r\nof taste rather than utility. This gives us direct control over parameters that carry\r\nbehavioral meanings, such as value-of-time (VOT), as they are no longer part of the\r\n9\r\nparameters to estimate, but intermediate outputs to predict. Thirdly, we realize the necessity of incorporating domain knowledge to obtain interpretable results from neural\r\nnetworks. We introduce parameter constraints along with l2 regularization to combat\r\nover-parameterization, a common issue with insufficient data and a cause for large\r\nestimation variability. Avoiding over-fitting of TasteNet allows for correct learning of\r\nboth the taste functions and parameters of the utility functions. Hyper-parameter\r\nsearch and model diagnostics should be carefully conducted to avoid over-fitting and\r\nensure stable and interpretable results.\r\nThe rest of this section provides more details on the network architecture, parameter\r\nconstraints, and estimation.\r\n3.2. TasteNet\r\nThe simplest neural network structure is the feed-forward neural network (also called\r\na multi-layer perceptron (MLP)) for TasteNet. An MLP consists of an input layer,\r\none or more hidden layers and an output layer. Essentially, MLP is a composition of\r\nlinear and nonlinear functions to map inputs to outputs.\r\nIn an MLP with 1 hidden layer of H hidden units, the k-th output of the network\r\n\u03b2\r\nTN\r\nk\r\ncan be written as Eqn. 6, where D is the input dimension, H is the number of\r\nhidden units, A(1) is the first hidden layer\u2019s activation function, and T is the output\r\nlayer\u2019s activation function. Neural network parameters w(1) and w(2) correspond to\r\nweights of connections from input layer to hidden layer 1, and weights of connections\r\nfrom hidden layer 1 to output layer.\r\n\u03b2\r\nT N\r\nk\r\n(z, w) = T[\r\nX\r\nH\r\nh=1\r\nw\r\n(2)\r\nkh A\r\n(1)(\r\nX\r\nD\r\ni=1\r\nw\r\n(1)\r\nhi zi + w\r\n(1)\r\nh0\r\n) + w\r\n(2)\r\nk0\r\n] (6)\r\nA general MLP with L hidden layers can be denoted as\r\nMLP(L, [H1, .., HL], [A(1), ..., A(L)\r\n], T). We need to specify the number of hidden layers L, the size of each hidden layer Hl\r\n, activation function for each hidden\r\nlayer A(l)\r\n, and output transform function T. These hyper-parameters are selected\r\nbased on a model\u2019s prediction performance on a hold-out set for model development.\r\n3.3. Parameter constraints\r\nOver-parameterization is common for neural networks, especially when the sample size\r\nis relatively small compared to the model complexity. Adding constraint is a method\r\nto regularize a neural network. We impose constraints on taste parameters not only to\r\nimprove model generalization ability; but also to ensure a reasonable range for taste\r\nparameters.\r\nA typical constraint is on the signs of parameters. For example, the coefficient for\r\ntravel time or waiting time is usually negative. We incorporate sign constraints through\r\nan output transform function T. For taste parameter \u03b2s with non-negative sign constraints, choices of T can be the rectified linear function (ReLU(\u03b2)) or exponential\r\nfunction (exp(\u03b2)). For \u03b2s with non-positive signs, choices of T can be the rectified\r\nlinear unit \u2212ReLU(\u2212\u03b2) or \u2212 exp(\u2212\u03b2). For \u03b2s without constraints, T is the identity function. Such transformations redistribute the parameters to the desirable range\r\n10\r\nthrough continuous differentiable functions, which resemble the exponential transform\r\nfor scale or time coefficient commonly used in a DCM utility function.\r\nAn advantage of using a transform function for parameter sign constraints is that\r\nthe constraints can be strictly kept. Other methods, such as adding a penalty for\r\nconstraint violation to the learning objective, cannot enforce the constraint on unseen\r\ndata.\r\n3.4. Estimation\r\nThe model is estimated by optimizing an objective function with stochastic gradient\r\ndescent. The goal is to minimize a loss function, which is the average of negative loglikelihood plus a regularization term for the p-norm of neural network weights (Eqn.\r\n7) to prevent the model from over-fitting.\r\nmin\r\nw,\u03b2MNL\r\n\u2212\r\nX\r\nn\r\nlog P(yn|zn, xin\u2200i \u2208 Cn; w, \u03b2\r\nMNL) + \u03bbp||w||p\r\n(7)\r\nTasteNet-MNL is trained in an integrated fashion through back-propagation using\r\nAdam, one of the most successful stochastic gradient descent algorithms for neural network training (Kingma and Ba, 2014). Parameters to estimate include neural network\r\nweights (w) and unknown coefficients in the MNL module (\u03b2MNL).\r\n4. Experiments\r\nWe generate synthetic datasets with an underlying logit model, which contains higherorder interactions between characteristics and attributes in its utility functions. We\r\ncompare TasteNet-MNL with benchmarking MNLs on the synthetic datasets, expecting that the TasteNet-MNL can improve predictability, reduce bias in parameter estimates, and provide more accurate behavioral interpretations, compared to underspecified MNLs.\r\n4.1. Synthetic datasets\r\nThe data generation model is a binary logit, with the systematic utility of alternative\r\ni for person n defined in Eqn. 8. Explanatory variables include three characteristics:\r\nincome (inc), full-time employment dummy (full) and flexible work schedule dummy\r\n(flex); and three alternative attributes: travel cost (cost) and travel time (time) and\r\nwaiting time (wait) (see Table A1 in Appendix A for details). Values of the coefficients are chosen to be realistic: income has a positive effect on value-of-time (VOT)\r\nand value-of-waiting-time (VOWT), full-time workers have higher VOT and VOWT,\r\nand people with flexible schedules have lower VOT and VOWT. We intentionally\r\ninclude second-order interactions between individual characteristics and alternative\r\nattributes to test whether TasteNet can learn them. We assign -1 as the groundtruth cost coefficients for both alternatives, so that outputs of the TasteNet are in\r\nthe willingness-to-pay space, corresponding to VOT and VOWT (Eqn. 8). Alternative\r\nspecific constant (ASC) for alternative 0 is fixed to 0. The random component of each\r\nutility follows an Extreme Value distribution.\r\nWe create two synthetic datasets: TOY UNCORREL with uncorrelated alternative\r\n11\r\nattributes; and TOY CORREL with a 0.6 correlation between time and wait. Having\r\nthe correlated dataset helps examine whether the neural network can learn multiple taste functions accurately and consistently when the corresponding attributes are\r\ncorrelated.\r\nEach dataset contains 14,000 observations, randomly split into training (10,000),\r\ndevelopment (2000) and test (2000) sets. Details about the input data distribution\r\nand synthetic data generation are reported in Appendix A.\r\nVin = ASCi \u2212 costin+\r\n(\u22120.1 \u2212 0.5incn \u2212 0.1fulln + 0.05flexn\r\n\u2212 0.2incn \u2217 fulln + 0.05incn \u2217 flexn + 0.1fulln \u2217 flexn) \u2217 timein+\r\n(\u22120.2 \u2212 0.8incn \u2212 0.3fulln + 0.1flexn\r\n\u2212 0.3incn \u2217 fulln + 0.08incn \u2217 flexn + 0.3fulln \u2217 flexn) \u2217 waitin (8)\r\n4.2. Models in comparison\r\nWe specify three MNL benchmarks with increasing complexity in the utility specifications. MNL-I\u2019s utility functions only include first-order interactions between individual characteristics and attributes (time and wait) (Eqn. 9). Compared to MNL-I,\r\nutilities of MNL-II have one additional interaction inc\u2217full \u2217time (Eqn. 10). MNLTRUE has the same utility function as the data generation model.\r\nV\r\nMNL\u2212I\r\ni = ASCi \u2212 costi + (b0 + b1inc + b2full + b3flex) \u2217 timei\r\n+ (c0 + c1inc + c2full + c3flex) \u2217 waiti (9)\r\nV\r\nMNL\u2212II\r\ni = ASCi \u2212 costi + (b0 + b1inc + b2full + b3flex + b12inc \u2217 full) \u2217 timei\r\n+ (c0 + c1inc + c2full + c3flex + c12inc \u2217 full) \u2217 waiti (10)\r\nFigure 2 shows the structure of the TasteNet-MNL for the synthetic data. Time\r\ncoefficient (\u03b2time) and waiting time coefficient (\u03b2wait is modeled by an MLP. Hyperparameters include the number of hidden layers, the size(s) of hidden layers, type of\r\nregularizer, regularization strength, activation function for hidden layers, and output\r\ntransform function.\r\nWe train a TasteNet-MNL model on the training dataset with different combinations\r\nof hyper-parameters. One hidden layer turns out to be sufficient to achieve the same\r\naccuracy as the data generation model. We vary the number of hidden units from 10 to\r\n100. For each hidden layer size, we apply l2 penalty in [0, 0.0001, 0.001, 0.01]. For the\r\nhidden layer activation function, we experiment with the ReLU and Tanh function.\r\nFor output transformation, we experiment with functions: \u2212ReLU(\u2212\u03b2) and \u2212e\r\n\u2212\u03b2\r\n, to\r\nimpose non-positive constraint on the value of time coefficient \u03b2time and \u03b2wait. For\r\neach combination of hyper-parameters, we train the model 100 times with different\r\nrandom initialization.\r\n12\r\nFigure 2.: TasteNet-MNL Diagram for the Synthetic Data\r\n4.3. Results\r\nThe optimal set of hyper-parameters is selected based on the lowest average negative log-likelihood (NLL) on the development dataset. On the TOY UNCORREL\r\ndataset, the best TasteNet-MNL has 1 hidden layer with 60 hidden units, ReLU for\r\nhidden layer activation, \u2212ReLU(\u2212\u03b2) for output transformation, and l2 penalty 0.001.\r\nOn TOY CORREL dataset, the optimal TasteNet-MNL has 1 hidden layer with 100\r\nhidden units, ReLU for hidden layer activation, \u2212ReLU(\u2212\u03b2) for output transformation, and l2 penalty 0.001. We choose the smallest layer-depth model with similar\r\nperformances to larger models to reduce the chance of model overfitting to spurious\r\nexamples. We compare MNLs and TasteNet-MNL with respect to their predictability,\r\nparameter bias, and interpretability.\r\n4.3.1. Predictability\r\nModel predictability is measured by average negative log-likelihood (NLL) and prediction accuracy (ACC) on training, development and test data. NLL is the total\r\nnegative log-likelihood in equation (7) divided by the number of observations. The\r\nhigher the NLL, the poorer the model fit and predicative power. Prediction accuracy\r\nis the percentage of correct predictions. Table 1 and Table 2 summarize the prediction\r\nperformance of different models on the synthetic datasets.\r\nOn TOY UNCORREL, MNL-I and MNL-II result in higher NLL (0.58, 0.5) than\r\nMNL-TRUE (0.45); and lower prediction accuracy (74%, 76%) than MNL-TRUE\r\n(78%) due to missing interaction terms. The performance gap is widened on the\r\nTOY CORREL dataset: MNL-I and MNL-II achieve higher NLL (0.66, 0.53) compared to MNL-TRUE (0.45) and lower accuracy (71%, 75%) compared to the true\r\nmodel (78%). On both datasets, TasteNet-MNL\u2019s best estimation run, as well as the\r\naverage of the 100 estimation runs reach the same level of predictability as MNLTRUE. The small standard deviation suggests its prediction performance is stable\r\nand significantly higher than the under-specified MNLs.\r\n13\r\nTable 1.: Average Negative Log-likelihood (NLL) and Prediction Accuracy (ACC) for\r\nSynthetic Data with Uncorrelated Alternative Attributes\r\nModel NLL train NLL dev NLL test ACC train ACC dev ACC test\r\nMNL-I 0.59775 0.57773 0.57866 0.712 0.725 0.739\r\nMNL-II 0.50618 0.47657 0.49603 0.755 0.768 0.760\r\nTasteNet-MNL\r\nbest run 0.46523 0.44193 0.44293 0.777 0.785 0.789\r\nmean 0.46688 0.44494 0.44536 0.775 0.787 0.785\r\nstd (0.00159) (0.00141) (0.00225) (0.002) (0.002) (0.003)\r\nMNL-TRUE 0.46424 0.44028 0.44669 0.777 0.786 0.784\r\nGround-truth 0.46460 0.44120 0.44655 0.775 0.786 0.788\r\nTable 2.: Average Negative Log-likelihood (NLL) and Prediction Accuracy (ACC) for\r\nSynthetic Data with Correlated Alternative Attributes\r\nModel NLL train NLL dev NLL test ACC train ACC dev ACC test\r\nMNL-I 0.67756 0.62325 0.66171 0.698 0.706 0.705\r\nMNL-II 0.53260 0.52495 0.53488 0.748 0.741 0.750\r\nTasteNet-MNL\r\nbest run 0.46261 0.46671 0.45345 0.777 0.773 0.776\r\nmean 0.46125 0.47171 0.45332 0.777 0.768 0.776\r\nstd (0.00198) (0.00216) (0.00275) (0.002) (0.003) (0.003)\r\nMNL-TRUE 0.45839 0.47018 0.45228 0.779 0.768 0.778\r\nGround-truth 0.45945 0.46845 0.45110 0.777 0.767 0.781\r\n14\r\nReaders may wonder: does TasteNet-MNL discover the underlying utility function\r\nform? Is this predictability gain attributed to TasteNet\u2019s ability to learn the underlying\r\ntaste functions or simply a result of over-fitting? The next section examines the taste\r\nfunctions learned by TasteNet and compare it with the underlying model.\r\n4.3.2. Parameter Estimates\r\nCoefficients in the MNL utility functions describe the marginal effect of a unit change\r\nin the inputs. A neural network does not provide direct estimates of the coefficients. In\r\nthis experiment, because we know the terms in the underlying utility functions, we can\r\nregress predicted \u03b2time and \u03b2wait for all individuals by TasteNet against these terms to\r\ncompare the coefficient estimates with the ground-truth. This measures how accurately\r\nthe TasteNet recovers the underlying taste functions. The alternative specific constant\r\n(ASC1) is obtained from the MNL component as it is not included in the TasteNet\r\npart of the utility. Table 3 and 4 show the coefficient estimates and error metrics by\r\ndifferent models for the two toy datasets.\r\nOn the TOY UNCORREL dataset, TasteNet-MNL\u2019s parameter bias is slightly\r\nhigher than MNL-TRUE, and much lower than the misspecified MNLs. The best run\r\nof TasteNet-MNL achieves slightly higher parameter errors (RMSE=0.03, MAE=0.02,\r\nMAPE=13.2%) than MNL-TRUE (RMSE=0.024, MAE=0.016, MAPE=8.1%). The\r\naverage bias from 100 TasteNet-MNL runs is higher (RMSE=0.052, MAE=0.038,\r\nMAPE=23.6%) than the best run, but still lower than MNL-I (RMSE=0.09,\r\nMAE=0.05, MAPE=31.5%) and MNL-II (RMSE=0.28, MAE=0.16, MAPE=132%).\r\nThough MNL-II\u2019s utility specification is more complete and closer to the groundtruth, its parameter bias is larger than the simple MNL-I. Adding the second-order\r\ninteractions (time*inc*flex and wait*inc*flex) causes severe bias in related coefficients,\r\nsuch as b time, b time flex, b time inc, b wait, b wait flex and b wait inc. This bigger\r\nbias may not be detected by modelers as MNL-II\u2019s predictability is higher than MNL-I\r\n(Table 1). This suggests that unless the exact utility form is correctly specified, there\r\nis a risk of having large bias with good predictability. A neural network is able to learn\r\ntaste functions from data and mitigate this risk.\r\nWith a 0.6 correlation between time and wait, all MNLs\u2019 parameter bias increase\r\n(Table 4). Standard errors of the coefficients also increase. MNL-TRUE\u2019s parameter\r\nRMSE increases from 0.0238 to 0.0469, almost doubled. This is because when alternative attributes are highly correlated, the decision boundary becomes less certain.\r\nWith the correlated attributes (time and wait), readers may suspect TasteNet-MNL\r\nto suffer from less accurate and more unstable parameter estimates, because the two\r\ntastes are estimated by a shared neural network, which may interfere with each other.\r\nThe best run of TasteNet-MNL\u2019s parameter bias stays almost at the same level, and\r\nthe average bias increases by about 3 to 5%. There is no strong evidence of deteriorated\r\naccuracy or stability caused by interactions between the two taste functions. It seems\r\nthat a linear model is more sensitive to the correlation in the input data.\r\nWhy is it? Our intuition is as follows. Although the two tastes are estimated through\r\na shared neural network, they do not necessarily interact much with each other because 1) each taste output has its own set of weights for the connections between the\r\nhidden layer and the output node; and 2) the number of hidden nodes is not too small\r\n(restrictive). For example, when the hidden layer is fairly large, taste output 1 might\r\nbe mainly connected with a subset of the hidden nodes, while taste output 2 can be\r\nmainly connected with another subset. With a wide hidden layer, there is enough flexibility to learn two taste parameters that do not interfere with each other and together\r\n15\r\nTable 3.: Parameter Estimates by MNLs and TasteNet-MNL (Uncorrelated Data)\r\nCoef MNL-I MNL-II MNL-TRUE TasteNet-MNL TasteNet-MNL Truth\r\n(best run) (100 runs: mean, std)\r\nASC1 -0.1358 -0.1263 -0.1325 -0.1315 -0.1321 -0.10\r\n(0.0273) (0.0262) (0.0257) (0.0207)\r\nb time -0.0875 -0.0330 -0.0964 -0.1028 -0.0983 -0.10\r\n(0.0023) (0.0027) (0.0051) (0.0090)\r\nb time flex 0.1205 0.0050 0.0501 0.0534 0.0534 0.05\r\n(0.0015) (0.0035) (0.0039) (0.0077)\r\nb time full -0.1070 -0.1062 -0.1001 -0.0969 -0.1174 -0.10\r\n(0.0025) (0.0024) (0.0065) (0.0156)\r\nb time full flex 0.0979 0.0823 0.0886 0.10\r\n(0.0044) (0.0156)\r\nb time inc -0.6582 -0.8026 -0.5176 -0.5122 -0.5307 -0.50\r\n(0.0077) (0.0083) (0.0194) (0.0445)\r\nb time inc flex 0.3014 0.0555 0.0819 0.0631 0.05\r\n(0.0085) (0.0135) (0.0317)\r\nb time inc full -0.1897 -0.1914 -0.1447 -0.20\r\n(0.0196) (0.0460)\r\nb wait -0.2188 -0.0625 -0.1874 -0.1902 -0.2091 -0.20\r\n(0.0085) (0.0103) (0.0198) (0.0187)\r\nb wait flex 0.2834 -0.0304 0.1042 0.1013 0.1091 0.10\r\n(0.0055) (0.0133) (0.0148) (0.0148)\r\nb wait full -0.2337 -0.2281 -0.2976 -0.3284 -0.3187 -0.30\r\n(0.0090) (0.0090) (0.0250) (0.0295)\r\nb wait full flex 0.2799 0.2842 0.2454 0.30\r\n(0.0167) (0.0283)\r\nb wait inc -1.0335 -1.4497 -0.8641 -0.8431 -0.8556 -0.80\r\n(0.0277) (0.0318) (0.0756) (0.0727)\r\nb wait inc flex 0.8148 0.0976 0.0720 0.1278 0.08\r\n(0.0319) (0.0511) (0.0609)\r\nb wait inc full -0.2557 -0.2123 -0.2041 -0.30\r\n(0.0761) (0.0762)\r\nRMSE 0.0911 0.2787 0.0238 0.0298 0.0520\r\n(0.0203)\r\nMAE 0.0524 0.1615 0.0158 0.0204 0.0379\r\n(0.0141)\r\nMAPE 0.3146 1.3172 0.0805 0.1320 0.2359\r\n(0.0849)\r\nRMSE: Root Mean Squared Error; MAE: Mean Absolute Error; MAPE: Mean Absolute Percentage Error\r\n16\r\nTable 4.: Parameter Estimates by MNLs and TasteNet-MNL (Correlated Data)\r\nCoef MNL-I MNL-II MNL-TRUE TasteNet-MNL TasteNet-MNL Truth\r\nbest run 100 runs: mean (std)\r\nASC1 -0.0335 -0.0385 -0.0543 -0.0561 -0.0627 -0.10\r\n(0.0282) (0.0265) (0.0258) (0.0528)\r\nb time -0.0871 -0.0310 -0.0988 -0.1090 -0.0963 -0.10\r\n(0.0028) (0.0033) (0.0063) (0.0089)\r\nb time flex 0.1211 0.0016 0.0500 0.0723 0.0592 0.05\r\n(0.0018) (0.0043) (0.0047) (0.0088)\r\nb time full -0.1103 -0.1113 -0.1003 -0.1030 -0.1271 -0.10\r\n(0.0030) (0.0030) (0.0080) (0.0174)\r\nb time full flex 0.1012 0.0861 0.0872 0.10\r\n(0.0054) (0.0142)\r\nb time inc -0.6562 -0.8004 -0.4979 -0.4836 -0.5439 -0.50\r\n(0.0093) (0.0103) (0.0241) (0.0363)\r\nb time inc flex 0.3087 0.0497 0.0248 0.0562 0.05\r\n(0.0104) (0.0161) (0.0283)\r\nb time inc full -0.2077 -0.1866 -0.1279 -0.20\r\n(0.0243) (0.0410)\r\nb wait -0.2256 -0.0663 -0.1790 -0.2196 -0.2079 -0.20\r\n(0.0106) (0.0125) (0.0242) (0.0170)\r\nb wait flex 0.2805 -0.0587 0.0951 0.0834 0.1004 0.10\r\n(0.0068) (0.0161) (0.0178) (0.0173)\r\nb wait full -0.1913 -0.2068 -0.3256 -0.3054 -0.3243 -0.30\r\n(0.0111) (0.0110) (0.0304) (0.0283)\r\nb wait full flex 0.3196 0.2856 0.2735 0.30\r\n(0.0201) (0.0230)\r\nb wait inc -1.1014 -1.4841 -0.9142 -0.8038 -0.8586 -0.80\r\n(0.0340) (0.0385) (0.0922) (0.0680)\r\nb wait inc flex 0.8868 0.0576 0.0886 0.0974 0.08\r\n(0.0388) (0.0612) (0.0544)\r\nb wait inc full -0.1742 -0.2500 -0.1785 -0.30\r\n(0.0929) (0.0654)\r\nRMSE 0.1065 0.2988 0.0469 0.0220 0.0545\r\n(0.0197)\r\nMAE 0.0622 0.1751 0.0261 0.0177 0.0391\r\n(0.0130)\r\nMAPE 0.3537 1.4445 0.1121 0.1571 0.2428\r\n(0.0706)\r\nRMSE: Root Mean Squared Error; MAE: Mean Absolute Error; MAPE: Mean Absolute Percentage Error\r\n17\r\noptimize the objective function.The l2 regularization penalty, if properly tuned, can\r\nkeep the most useful connections, while shrinking the rest towards zero. The weights\r\nof the network may vary much across runs (the well-known unidentifiable nature of a\r\nneural network), but the output of the network - the taste parameters are constrained\r\nby the downstream MNL utility, and therefore can remain relatively stable. Because\r\nMNLs are constrained by the overarching linear structure of the utility, there is less\r\nfreedom to adapt to the changes in the shape of the input distribution. However, with\r\nthe flexibility of the neural network also comes a weakness: a neural network does not\r\nperform well on out-of-distribution samples or samples with low occurrences in the\r\ntraining data. We will discuss this issue in the next section.\r\n4.3.3. Taste Estimates and Taste Functions\r\nWe estimate VOT and VOWT for each individual in the synthetic datasets, and compute the errors compared to the ground-truth (Table 5).\r\nOn the TOY UNCORREL dataset, TasteNet-MNL obtains smaller errors in its\r\npredicted VOT and VOWT than MNL-I and MNL-II. TasteNet-MNL\u2019s mean absolute\r\nerror (MAE) for VOT is 0.15$/hr (0.8% of the true value), compared to MNL-I\u2019s\r\n1.7$/hr (11%) and MNL-II\u2019s 0.8$/hr (4.8%). TasteNet-MNL\u2019s MAE for VOWT is\r\n0.6$/hr (2%) compared to MNL-I\u2019s 4.8$/hr (17.7%) and MNL-II\u2019s 2.3$/hr (7.7%).\r\nOn the TOY CORREL dataset, both MNL-TRUE and TasteNet-MNL\u2019s VOT and\r\nVOWT\u2019s errors increase, but the error magnitudes relative to each other remain the\r\nsame: TasteNet-MNL\u2019s taste error is about 3 times as big as MNL-TRUE\u2019s. TasteNetMNL maintains its significant lead over MNL-I and MNL-II.\r\nTable 5.: Errors of the Estimated Value of Time and Value of Waiting Time (Unit: $\r\n/ Hour)\r\nInput data Error metrica MNL-I MNL-II MNL-TRUE TasteNet-MNL TasteNet-MNL\r\nbest run 100 runs: mean (std)\r\nUncorrelated Data RMSE VOT 1.76 1.00 0.06 0.19 0.25 (0.06)\r\nMAE VOT 1.70 0.80 0.04 0.15 0.20 (0.06)\r\nMAPE VOT 11.0% 4.8% 0.3% 0.8% 1.2% (0.4%)\r\nRMSE VOWT 4.87 2.81 0.28 0.72 0.79 (0.28)\r\nMAE VOWT 4.82 2.31 0.24 0.60 0.66 (0.29)\r\nMAPE VOWT 17.7% 7.7% 0.9% 2.0% 2.3% (1.0%)\r\nCorrelated Data RMSE VOT 1.76 1.01 0.10 0.36 0.29 (0.08)\r\nMAE VOT 1.70 0.81 0.09 0.30 0.23 (0.08)\r\nMAPE VOT 11.2% 4.9% 0.6% 2.1% 1.4% (0.5%)\r\nRMSE VOWT 4.97 2.94 0.68 1.27 1.02 (0.29)\r\nMAE VOWT 4.82 2.17 0.49 0.97 0.86 (0.26)\r\nMAPE VOWT 16.9% 6.3% 2.3% 4.8% 3.4% (1.2%)\r\nRMSE: Root Mean Squared Error; MAE: Mean Absolute Error; MAPE: Mean Absolute Percentage Error\r\nVOT: Value of Time; VOWT: Value of Waiting Time\r\nTo further investigate the shape of the taste functions learned by the neural network compared with other models, we create a dataset with 200 individuals with uniformly distributed characteristics. Note that this input distribution is different from\r\nthat of the training data. 50 individuals are in each of the four groups defined by\r\nall combinations of full-time (yes/no) and flexible schedule (yes/no). The income of\r\nindividuals from each group is evenly distributed from 0 to 60$ per hour with a step\r\n18\r\nsize 1.2. We plot the predicted VOTs and VOWTs against the continuous income for\r\nthe four types of individuals. Figure 3 shows the results from applying models for the\r\nTOY UNCORREL dataset and Figure 4 shows the results from applying models for\r\nthe TOY CORREL dataset. The ground-truth utility functions are the same between\r\nthe two.\r\nMNL-I cannot distinguish the differences in the shape of VOT and VOWT functions\r\nbetween the full flex group and the nofull noflex group, due to missing all 3 secondorder interaction terms. Adding two second-order interactions to MNL-II causes larger\r\nbias, and significant changes in the slope of the VOT and VOWT functions of income\r\nfor each group.\r\nUnsurprisingly MNL-TRUE\u2019s estimated taste functions are the closest to the\r\nground-truth, as its utility has the exact terms as the underlying utility. Comparing\r\nMNL-TRUE\u2019s taste functions between uncorrelated data (Figure 3) and correlated\r\ndata (Figure 4), we notice the MNL-TRUE\u2019s VOWT function has an apparent deviation from the ground-truth due to the correlation in attributes.\r\nTasteNet-MNL overall recovers the true shape of the taste functions for both the\r\nuncorrelated and the correlated case. However, the functions are not strictly linear.\r\nNear the limits of income, we observe larger bias and variance. This is because the\r\nmajority of the training samples have income between 10 and 50 $/hr (Figure 5), with\r\nonly 208 (2.1%) of the 10,000 training samples beyond this range. With insufficient\r\nexamples in a certain region of the input space, the neural network is unable to learn an\r\naccurate and precise taste function. This reveals a major weakness of TasteNet-MNL\r\nthat it does not generalize well to out-of-distribution samples.\r\nDespite this weakness, TasteNet-MNL has the ability to recover the underlying\r\ntaste functions for the majority of the population, given that there is enough data to\r\nlearn from. It can lower the risk of getting the taste functions systematically wrong\r\nwithout knowing the true utility form. Whether the data is sufficient depends on\r\nthe complexity of the problem. An MNL model tends to generalize better to out-ofdistribution samples because of the highly restricted global utility functions. However,\r\nthis generalization ability is only possible if we have a correct specification of the utility\r\nfunctions. As the MNL-II example shows, a misspecified MNL can lead to greater bias\r\nfor the out-of-distribution samples than a TasteNet-MNL does.\r\nIt is worth noting that in this synthetic data experiment, we examine a particular\r\nform of nonlinearity - higher-order interactions - for the convenience of illustration\r\nand understanding. Based on the universal function approximation property of NN,\r\nTasteNet can be employed to learn general forms of nonlinearity in taste functions.\r\n5. Model Application: Swissmetro Mode Choice\r\nWe apply TasteNet-MNL to a publicly available dataset \u2013 Swissmetro to model mode\r\nchoice for inter-city travel. The purpose of this application is to 1) examine whether\r\nTasteNet-MNL is able to predict more accurately compared to a manually specified,\r\nrelatively sophisticated MNL; and 2) whether TasteNet-MNL can draw reasonable\r\nbehavioral interpretations and, if so, how its interpretations differ from those of the\r\nMNLs. To compare with TasteNet-MNL, we set up three benchmarking MNL models\r\nwith increasing complexity in the utility function and random coefficient logit models.\r\n19\r\nFigure 3.: Estimated Value of Time and Value of Waiting Time v.s. Income for 4\r\nGroups (data: TOY UNCORREL)\r\nFigure 4.: Estimated Value of Time and Value of Waiting Time v.s. Income for 4\r\nGroups (data: TOY CORREL)\r\n20\r\nFigure 5.: Histogram of Income in Training Data\r\n5.1. Data\r\nThe Swissmetro is a proposed revolutionary mag-lev underground system. To assess\r\npotential demand, the Swissmetro Stated Preference (SP) survey collected data from\r\n1,192 respondents (441 rail-based travelers and 751 car users), with 9 choices from each\r\nrespondent. Each respondent is asked to choose one mode out of a set of alternatives\r\nfor inter-city travel given the attributes of each mode (e.g. travel time, headway and\r\ncost). The universal choice set includes train (TRAIN), Swissmetro (SM), and car\r\n(CAR). For individuals without a car, the choice set includes only TRAIN and SM.\r\nTable 6 provides a description of the variables. For more information, readers can refer\r\nto Bierlaire (2018).\r\nThe original data has 10,728 observations, downloaded in Jan 20193\r\n. After removing observations with unknown age, \u201cother\u201d trip purpose and unknown choice, we\r\nretain 10,692 observations. We randomly split the data into training (\u201ctrain\u201d), development(\u201cdev\u201d) and test(\u201ctest\u201d) set with 7,484, 1,604 and 1,604 observations, respectively.\r\n5.2. Benchmarks\r\nWe set up four benchmarks: three MNL models and a Mixed Logit model. MNL-A\r\nis similar to Bierlaire et al.(2001)\u2019s MNL specification but with some enhancements:\r\n1) the value of travel time and value of headway is made mode-specific; 2) all levels of\r\nage and luggage categories are included; and 3) cost coefficients are fixed to -1.0 for\r\ndirectly reading VOT from time coefficients (Table 7). In the benchmark MNL-B, we\r\nadd the interaction terms: time*age, time*income and time*purpose (Table 8). The\r\nthird benchmark MNL-C is an MNL with all pairs of first-order interactions between\r\ncharacteristics and attributes (Table 9). This model is equivalent to a TasteNet-MNL\r\nwith all taste coefficients modeled by a neural network with no hidden layer. The\r\nMixed Logit model has a normally distributed travel time coefficient for each alternative, along with interactions between individual characteristics with travel time for\r\neach alternative. Intercepts and coefficients for heading are alternative specific and not\r\nrandomly distributed. Estimated model coefficients for the benchmarking models are\r\nshown in Table 7, 8, 9 and 10.\r\n3Data link: https://biogeme.epfl.ch/data.html\r\n21\r\nTable 6.: Description of Variables in the Swissmetro Dataset\r\nAlternative Alternative attributes Availability\r\nTRAIN time, headway, cost (train tt, train hw, train co) train av\r\nSM (Swissmetro) time, headway, seatsa\r\n, cost (sm tt, sm hw,\r\nsm seats, sm co)\r\nsm av\r\nCAR time, cost (car tt, car co) car av\r\nPerson/Trip variable Variable levels\r\nAGE 0: age \u2264 24, 1: 24 < age \u2264 30, 2: 39 < age \u2264 54,\r\n3: 54 < age \u2264 65, 4: 65 < age\r\nMALE 0: female, 1: male\r\nINCOME (thousand CHF per year) 0: under 50, 1: between 50 and 100, 2: over 100,\r\n3: unknown\r\nFIRST (First class traveler) 0: no, 1: yes\r\nGA (Swiss annual season ticket) 0: no GA, 1: owns a GA\r\nPURPOSE 0: Commuter, 1: Shopping, 2: Business, 3:\r\nLeisure\r\nWHO (Who pays) 0: self, 1: employer, 2: half-half\r\nLUGGAGE 0: none, 1: one piece, 2: several pieces\r\na. Seats configuration in Swissmetro: seats=1 if airline seats, 0 otherwise.\r\nTable 7.: Estimated Coefficients of MNL-A\r\nVariable Description Train Swissmetro Car\r\nConstant 0.1227 0.5726\r\nTravel time (minutes) -1.3376 -1.4011 -1.0177\r\nHeadway (minutes) -0.4509 -0.8171\r\nSeats (airline seating = 1) 0.1720\r\nCost (CHF) -1 (fixed) -1 (fixed) -1 (fixed)\r\nGA (annual ticket = 1) 2.0656 0.5319\r\nAge\r\n1: 24 < age \u2264 30 -0.7548\r\n2: 39 < age \u2264 54 -0.9457\r\n3: 54 < age \u2264 65 -0.4859\r\n4: 65 \u2264 age 0.6995\r\nLuggage\r\n1:one piece -0.1538\r\n2:several pieces -0.9230\r\n22\r\nTable 8.: Estimated Coefficients of MNL-B\r\nVariable Description Train Swissmetro Car\r\nConstant 0.0056 0.4674\r\nTravel time (minutes) -0.5006 -0.4010 -0.5600\r\nTravel time * Age\r\n0: age \u2264 24\r\n1: 24 < age \u2264 39 -0.6354 -0.3307 -0.5696\r\n2: 39 < age \u2264 54 -0.8475 -0.6101 -0.6105\r\n3: 54 < age \u2264 65 -0.1566 0.1419 -0.0915\r\n4: 65 < age 0.3265 -0.243 -0.0234\r\nTravel time * Income\r\n0: under 50\r\n1: 50 to 100 -0.2688 0.1739 0.1623\r\n2: over 100 -1.0181 -0.436 -0.4093\r\n3: unknown 0.0852 0.2828 -0.0923\r\nTravel time * Purpose\r\n0: Commute\r\n1: Shopping -0.2081 -0.6192 -0.6062\r\n2: Business -0.1574 -0.8688 -0.1833\r\n3: Leisure -0.59 -0.9706 -0.0162\r\nHeadway (minutes) -0.6158 -0.7011\r\nSeats (airline seating = 1) 0.189\r\nCost (CHF) -1 (fixed) -1 (fixed) -1 (fixed)\r\nGA (annual ticket = 1) 1.6162 0.2988\r\nLuggage\r\n1:one piece -0.1714\r\n2:several pieces -0.6718\r\n23\r\nTable 9.: Estimated Coefficients of MNL-C\r\nCoefficients for alternative attributes\r\nz (characteristics) TRAIN TT SM TT CAR TT TRAIN HE SM HE SM SEATS TRAIN ASC SM ASC\r\nIntercept -0.0671 0.1455 0.0059 0.1713 0.0646 0.3064 0.2953 0.2067\r\nMale -0.1526 -0.0477 0.0742 -0.2384 0.0706 -0.1016 0.0671 0.149\r\nAge\r\n1: (24,39] -0.0965 -0.2422 -0.1093 0.0044 0.5682 0.0517 -0.1634 0.4285\r\n2: (39,54] -0.1467 -0.2022 -0.195 -0.2397 -0.0105 -0.2135 -0.2692 0.0959\r\n3: (54,65] 0.0256 0.1201 0.0251 -0.2379 -0.0807 0.1619 -0.0861 -0.0344\r\n4: (65,) -0.1712 0.1435 0.1105 0.6032 -0.1488 -0.1529 0.618 -0.351\r\nIncome\r\n1: 50-100 0.0494 -0.039 0.0098 -0.1884 -0.2972 0.2349 -0.1776 0.1944\r\n2: over 100 -0.2825 -0.1697 -0.2662 0.1393 0.0372 0.5288 -0.0406 -0.0789\r\n3: unknown 0.0289 0.1467 -0.2037 0.1484 -0.0721 -0.4196 0.1621 -0.0459\r\nFirst class -0.1927 -0.0807 -0.3297 -0.4768 0.1183 0.1302 0.2228 -0.2085\r\nWho pay\r\n1: employer -0.2154 -0.1668 0.1231 0.028 -0.0045 0.0882 0.1191 0.3986\r\n2: half-half 0.1537 0.4771 0.4391 -0.0311 0.3917 0.3114 -0.2414 -0.0332\r\nPurpose\r\n1:Shopping 0.2339 -0.219 0.19 0.1509 0.0493 0.1994 0.4238 0.6996\r\n2:Business -0.0872 -0.3524 -0.181 -0.0544 -0.0195 -0.0647 0.0605 -0.2941\r\n3:Leisure -0.2678 -0.2778 -0.0043 0.3245 -0.4552 -0.0289 -0.302 -0.4739\r\nLuggage\r\n1:one piece -0.0375 0.0861 0.2525 0.58 -0.1993 0.0413 0.3364 0.3239\r\n2:several pieces 0.022 -0.1785 -0.2731 -0.2946 0.0814 -0.1225 -0.0041 0.2158\r\nAnnual ticket 0.5912 -0.0075 -0.3181 0.2652 -0.2032 -0.5815 0.3576 0.1351\r\nL-MNL is not selected as a benchmark because it is a special case covered by\r\nTasteNet-MNL for the Swissmetro dataset: all unused variables in the data-driven part\r\nof the L-MNL utility are individual characteristics, so that the data-driven utility in\r\nL-MNL corresponds to the ASC taste functions estimated by TasteNet. Given a more\r\ncomplex dataset, we could have a unified version of L-MNL and TasteNet-MNL, whose\r\nutility consists of a data-driven component from L-MNL, a TasteNet component, and\r\na knowledge-driven part.\r\n5.3. Taste-MNL\r\nThe TasteNet-MNL structure for Swissmetro data is shown in Figure 6. We specify\r\nthe utility function of each alternative in the MNL module. Coefficient for cost is fixed\r\nto -1 so that the coefficient for time is the negative VOT. There are 7 coefficients in\r\nthe MNL utilities, including two alternative specific constants. We assume all these\r\ncoefficients (\u201ctastes\u201d) are functions of individual characteristics and model them by\r\nTasteNet. This is a special case of the general structure: the set of \u03b2MNL is empty\r\nand all taste parameters are modeled by TasteNet as \u03b2\r\nT N (Eqn. 4 and 5).\r\nThe TasteNet module consists of a linear layer from input z to hidden layer h\r\n(1), a\r\nnonlinear activation A(1) for the hidden layer, followed by a linear layer from hidden\r\nlayer to output layer and an output activation function T for the output. We choose\r\nonly 1 hidden layer, since the predicted log-likelihoods on hold-out datasets do not\r\nimprove with more hidden layers. Input z includes all characteristics: age, gender,\r\nincome, first-class, who pays for travel cost, trip purpose and luggage. We experiment\r\nwith various sets of hyper-parameters and activation functions shown in Table 11.\r\nAmong all TasteNet-MNL scenarios, the one with 110 hidden units, relu for hidden layer activation, negative exponential for non-positive output activation, and no\r\n24\r\nTable 10.: Estimated Coefficients of Mixed Logit Model\r\nVariable Description Train Swissmetro Car\r\nConstant -0.763 -1.453\r\nTravel time (minutes) -1.691 -0.265 -0.897\r\nTravel time Std (minutes) 1.329 0.621 0.351\r\nTravel time * Male -0.537 -0.149 -0.009\r\nTravel time * Age\r\n0: age \u2264 24\r\n1: 24 < age \u2264 39 -1.429 -1.345 -0.949\r\n2: 39 < age \u2264 54 -1.420 -1.533 -0.893\r\n3: 54 < age \u2264 65 -0.730 -0.876 -0.411\r\n4: 65 < age 0.432 -0.549 0.140\r\nTravel time * Income\r\n0: under 50\r\n1: 50 to 100 0.052 0.808 0.582\r\n2: over 100 -0.391 0.452 0.209\r\n3: unknown 0.899 1.359 0.633\r\nTravel time * Purpose\r\n0: Commute\r\n1: Shopping -0.754 -1.676 -1.312\r\n2: Business -0.135 -1.114 -0.258\r\n3: Leisure -1.062 -1.477 -0.196\r\nTravel time * Luggage\r\n1: One piece 0.658 0.714 0.496\r\n2: Several pieces -0.922 -1.477 -1.597\r\nTravel time * Annual Ticket 1.542 -0.301 -0.788\r\nHeadway (minutes) -0.969 -1.365\r\nSeats (airline seating = 1) 0.559\r\nCost (CHF) -1 (fixed) -1 (fixed) -1 (fixed)\r\nFigure 6.: Diagram of TasteNet-MNL for Swissmetro Dataset\r\n25\r\nregularization achieves the best prediction performance on the development dataset.\r\nTable 11.: Options of Hyper-parameters & Activation Functions\r\nOptions Values\r\nHidden activation relu, tanh\r\nOutput activation (for non-positive parameters) \u2212relu(\u2212\u03b2), \u2212exp(\u2212\u03b2)\r\nHidden layer size [10, 20, ..., 100]\r\nl1 or l2 regularization a\r\n[0, 0.0001, 0.001, 0.01]\r\na Either l1 or l2 regularization is used.\r\n5.4. Results\r\n5.4.1. Prediction Performance\r\nTasteNet-MNL out-performs all benchmarking models\u2019 prediction accuracy. We use\r\naverage negative log-likelihood (NLL) and prediction accuracy (ACC) to measure predictability. To avoid model over-fitting, we split data into training, development and\r\ntest set, and present the performance on each subset. As test set is not used in model\r\ntraining or hyper-parameter selection, prediction performance on it provides a reliable\r\ncomparison across models. The result is shown in Table 12.\r\nFrom MNL-A to MNL-C, as more interactions between attributes and individual\r\ncharacteristics are added, the predicted NLL is reduced from 0.755 (MNL-A) to 0.698\r\n(MNL-C). Mixed-Logit\u2019s performance is similar to MNL-C (0.703). The best run of\r\nTasteNet-MNL (minimal NLL on development set) reduces test NLL from 0.698 to\r\n0.645, and improves ACC from 0.678 to 0.703 and F1 score from 0.574 to 0.62. The\r\nmean and standard deviation of each predictability metric from 100 runs also shows\r\nconsistent improvement: NLL reduced to 0.652, ACC improved to 0.7 and F1 score\r\nimproved to 0.62. The improved predictability is attributed to the fact that TasteNet\r\nlearns a more complex representation of systematic taste heterogeneity.\r\nTable 12.: Average Negative Log-likelihood (NLL), Prediction Accuracy (ACC) and\r\nF1 Score by Model\r\nNLL ACC F1\r\nModel train dev test train dev test train dev test\r\nMNL-A 0.762 0.728 0.755 0.662 0.691 0.66 0.535 0.557 0.534\r\nMNL-B 0.73 0.708 0.72 0.678 0.69 0.678 0.578 0.585 0.573\r\nMNL-C 0.704 0.691 0.698 0.685 0.706 0.678 0.588 0.611 0.574\r\nMixed-Logit 0.712 0.699 0.703 0.681 0.701 0.686 0.588 0.606 0.586\r\nTasteNet (no2) 0.607 0.646 0.645 0.737 0.718 0.703 0.668 0.634 0.620\r\nTasteNet (100 runs) 0.605 0.650 0.652 0.735 0.718 0.703 0.674 0.645 0.621\r\nmean (std) (0.0182) (0.0055) (0.008) (0.0094) (0.0047) (0.0055) (0.016) (0.009) (0.010)\r\n26\r\n5.4.2. Estimated Tastes\r\nSince interpretability is a primary concern for transportation application, we need to\r\ninvestigate whether the interpretations make sense. We show several ways to diagnose.\r\nAs TasteNet-MNL are direct extensions to MNLs, and Mixed Logit does not show\r\nsuperior predictability, we focus on the comparisons between MNLs and TasteNetMNL\u2019s interpretations here and in the following sections.\r\nFirst, we compare the average taste estimations for observations in the Swissmetro\r\ndataset by different models. We apply each model to obtain individual estimates,\r\nand compute the averages (Table 13). Since the cost coefficients are fixed to -1.0, all\r\ntaste coefficients are in the willingness-to-pay (WTP) space measured by Swiss Franc\r\n(CHF).\r\nTable 13.: Average Tastes Estimated by Different Models\r\nMode Taste MNL-A MNL-B MNL-C TasetNet (vs MNL-C) TasteNet mean (std)\r\nTRAIN TT -1.338 -1.710 -1.846 -2.327 (26%) -2.376 (0.179)\r\nHE -0.451 -0.616 -0.880 -1.102 (25%) -1.161 (0.077)\r\nASC -0.198 0.234 0.368 0.801 (117%) 0.824 (0.135)\r\nSM TT -1.401 -1.514 -1.505 -1.764 (17%) -1.905 (0.135)\r\nHE -0.817 -0.701 -1.039 -1.733 (67%) -1.709 (0.131)\r\nSEATS 0.172 0.189 0.420 0.266 (-37%) 0.270 (0.079)\r\nASC 0.648 0.510 0.512 0.669 (31%) 0.675 (0.064)\r\nCAR TT -1.018 -1.251 -1.354 -1.685 (24%) -1.713 (0.101)\r\nTT: time. HE: headway. ASC: alternative specific constant\r\nWe find that from MNL-A to MNL-C, average values of travel time (VOT) and\r\nvalues of headway (VOHE) increase with more interaction terms being added to the\r\nutility function. For example, train VOT increases from 1.34 to 1.85 CHF per minute.\r\nSwissmetro VOT increases from 1.4 to 1.51 CHF per minute, and car VOT rises from\r\n1 to 1.35 CHF per minute. Both MNL-B and MNL-C suggest that VOT of train is\r\nhigher than that of Swissmetro or car. MNL-C also gives higher average VOHE for\r\nboth train and Swissmetro than MNL-B or MNL-A.\r\nTasteNet-MNL gives the largest average VOT and VOHE among all models (Table\r\n13). Its average VOT estimates for train, swissmetro and car are 26%, 17%, and 24%\r\nhigher, respectively than those predicted by MNL-C. Its average VOHE estimates for\r\ntrain and swissmetro are 25% and 67% higher than those estimated by MNL-C.\r\nWe further investigate where the higher average VOTs come from. We plot histograms for each taste parameter and for each model (Figure 7). As interactions are\r\nincrementally added from MNL-A to MNL-C, the model captures more taste variations. Compared to MNLs with linear utilities, TasteNet-MNL discovers a wider range\r\nof taste variations. In particular, the VOTs and VOHEs for all travel modes have\r\nlonger tails on the high end of WTP. The last row of histograms (Figure 7) shows the\r\nmean and standard deviation from 100 runs. Overall, we see a consistent pattern in\r\nthe distributions, with different amount of variability across estimation runs. There\r\nis greater uncertainty in Train\u2019s VOT estimates, and Train and Swissmetro\u2019s ASC\r\nestimates. The bigger uncertainty in Train\u2019s VOT (compared to Car or Swissmetro\u2019s)\r\ncoincides with the larger standard deviation of the randomly distributed travel time\r\ncoefficient for Train (1.33), compared to the standard deviation of 0.62 for Swissmetro\r\nand 0.35 for Car (Table 10).\r\n27\r\nBased on the synthetic data experiments, we have confidence to believe that\r\nTasteNet-MNL\u2019s better predictability is due to more accurately capturing the underlying taste functions. However, readers should be aware of the risk of over-fitting\r\nTasteNet to outliers, especially when dealing with a noisier dataset or using more complex neural networks. We recommend conducting a thorough interpretability check at\r\nboth the aggregate and the disaggregate level, and comparing the results against the\r\nbenchmarking models to spot any spurious behaviors. The next two sections provide\r\nadditional ways to check.\r\n5.4.3. Taste Functions\r\nAn interpretable model should provide a reasonable function relationship between\r\nchoice outcome and input at the disaggregate level. We propose a diagnostic tool for\r\nmodel interpretability: visualizing the taste function.\r\nEach model provides a taste function that maps individual characteristics to a type\r\nof taste value (e.g. VOT, VOHE). We want to check whether TasteNet-MNL learns\r\nsensible taste functions at the individual level, in comparison to benchmarking MNLs.\r\nSince function input z is multi-dimensional, we cannot directly visualize the functions. Instead, we pick an individual with characteristics z. We vary one dimension of\r\nz: zi\r\n, while keeping other dimensions fixed zj6=i\r\n. We plot a particular taste parameter\r\nas a function of zi\r\n: \u03b2k = fmodel(zi\r\n; zj6=i).\r\nFor example, we pick a person with characteristics shown in Table 14. We vary this\r\nperson\u2019s income and ask each model a question: what are the VOTs for such a person\r\nas his income varies? We compare the answers given by different models. Figure 8\r\nshows the VOTs and VOHEs estimated by different models versus income with other\r\ncharacteristics fixed.\r\nTable 14.: Example Person Selected for Comparing Taste Function by Models\r\nCharacteristics Value\r\nzf ixed MALE Male\r\nAGE (39,54]\r\nPURPOSE Commute\r\nWHO Self\r\nLUGGAGE One piece\r\nGA Yes\r\nFIRST No\r\nzvary INCOME 0: under 50, 1: 50 to 100, 2: over 100\r\nCompared with the benchmarking MNLs, VOT and VOHE estimated by TasteNetMNL all fall within credible ranges. TasteNet-MNL gives more or less different estimates. Swissmetro VOT estimates are not very different between TasteNet-MNL and\r\nMNL-C. Regarding train VOT, TasteNet-MNL gives smaller estimates for all three\r\nincome groups than MNL-C. Car VOT estimated by TasteNet-MNL is higher for\r\nhigher-income groups and lower for the lowest income group. With respect to VOHEs,\r\nTasteNet-MNL gives higher estimates for train and lower estimates for Swissmetro for\r\n28\r\nFigure 7.: Population Taste Distributions by Models (Swissmetro Dataset)\r\n29\r\nFigure 8.: Tastes as Functions of Income for a Selected Person by Different Models\r\n30\r\nall income levels. MNL-C shows a monotonic relationship between VOT and income\r\nonly for train VOT, while TasteNet-MNL identifies the monotonicity for swissemtro\r\nVOT and car VOT.\r\nAs we do not know the ground truth, the interpretability and credibility of the model\r\ninevitably depend on expert knowledge and judgment. We draw many individual cases,\r\nvisualize the taste functions, and compare across models. Overall, taste parameters by\r\nTasteNet-MNL fall within similar ranges as the MNLs do. Yet a particular taste of a\r\nspecific individual given by TasteNet-MNL can agree with or differ from the MNLs.\r\nBased on TasteNet-MNL\u2019s better predictability, we trust that it obtains more accurate\r\ntaste estimates for individuals.\r\n5.4.4. Elasticity\r\nElasticity is another aspect to check model interpretability and catch abnormal neural\r\nnetwork behaviors. We first apply each model to calculate disaggregate point elasticity of Swissmetro mode choice with respect to Swissmetro travel time for each\r\nobservation. The point elasticity equation for MNL models is shown in Eqn. 11 and\r\nfor TasteNet MNL in Eqn. 12. TasteNet-MNL\u2019s individual elasticity estimates differ\r\nfrom MNL-C by 0.2287 on average.\r\nWith individual elasticities, we then compute aggregate elasticity, which measures\r\na group of decision-makers\u2019 response to an incremental change in a variable. This\r\nis defined in Eqn 13 as the percentage change in the expected share of the group\r\nchoosing alternative i (Wi) with respect to one percentage change in variable xki.\r\nIt is equivalent to a weighted average of the individual elasticities using the choice\r\nprobabilities as weights.\r\nE\r\nPn(i)\r\nxkin = (1 \u2212 Pn(i))xkin\u03b2k (11)\r\nE\r\nPn(i)\r\nxkin = (1 \u2212 Pn(i))xkin\u03b2k(zn) (12)\r\nE\r\nW(i)\r\nxki =\r\n\u2202W(i)\r\n\u2202xki\r\nxki\r\nW(i)\r\n=\r\nP\r\nn Pn(i)E\r\nPn(i)\r\nP xkin\r\nn Pn(i)\r\n(13)\r\nThe aggregate elasticities of Swissmetro mode share with respect to Swissmetro\r\ntravel time are -0.43, -0.45, and -0.41 for MNL-A, MNL-B and MNL-C, compared to\r\n-0.437 for TasteNet-MNL. We further compare aggregate elasticity by group, such as\r\nincome (Table 15). TasteNet-MNL suggests higher elasticities for low income and high\r\nincome groups than MNL-C. But overall, TasteNet-MNL gives choice elasticities close\r\nto MNLs and within reasonable range. We recommend performing this experiment\r\nsystematically against a dummy model (e.g. MNL-C) as a sanity check.\r\n31\r\nTable 15.: Aggregate Choice Elasticity of Swissmetro w.r.t Travel Time by Income\r\nGroup\r\nINCOME0 INCOME1 INCOME2\r\nMNL-A -0.3765 -0.4297 -0.4706\r\nMNL-B -0.3975 -0.3923 -0.5329\r\nMNL-C -0.3759 -0.3706 -0.4653\r\nTasteNet-MNL -0.4200 -0.3982 -0.4810\r\n6. Conclusions & Discussions\r\nIn this paper, we utilize a neural network to learn taste representation while keeping\r\nmodel interpretability. Departing from a traditional either-or approach, we integrate\r\nneural networks and DCMs to take advantage of both.\r\nWith a synthetic dataset, we show that TasteNet-MNL can learn the underlying\r\ntaste function. It achieves the same level of accuracy as the true model in predicting\r\nchoice and deriving economic indicators. Exemplary MNLs with misspecified utility\r\nresult in parameter bias and lower prediction accuracy. On the Swissmetro dataset,\r\nTasteNet-MNL not only surpasses MNLs and Mixed Logit model benchmarks in predictability, but also achieves interpretable economic indicators: individual-level VOTs\r\nand elasticities from TasteNet-MNL are comparable to the results of the benchmarking\r\nMNLs. Moreover, TasteNet-MNL discovers a greater range of taste variations in the\r\npopulation than the benchmarking MNLs. The average VOT estimates by TasteNetMNL are higher than the MNLs\u2019.\r\nWe demonstrate the benefit of integrating neural networks and DCM. Neural networks can learn complex functions from data and reduce bias in manual specifications.\r\nThe theory behind a DCM can guide the neural networks to output meaningful results.\r\nA high-level idea behind TasteNet-MNL is to assign the more complex or unknown\r\npart of the model (e.g. taste heterogeneity) to a neural network, and keep the wellunderstood part (e.g. the trade-offs between alternative attributes) parametric. This\r\nidea can be applied to other settings. For example, a neural network can be used to\r\nmodel class membership in a latent class choice model.\r\nTasteNet-MNL is most suitable for a fairly large dataset (thousands of examples or\r\nmore) with a rich set of individual characteristics. Modelers can compare TasteNetMNL against MNL benchmarks provided by human experts to understand the predictability gap and existence of misspecified taste heterogeneity. Modelers should apply proper regularization strategies in order not to over-fit the model. Based on our\r\nfindings from the synthetic data experiments, TasteNet-MNL yields large estimation\r\nerrors on out-of-distribution samples. Users should be aware of this risk and careful\r\nwhen applying the model to samples near or outside the training data\u2019s input distribution. We recommend estimating the models for multiple times (e.g. 100) with\r\ndifferent initialization to measure the uncertainty in estimates; and conducting a systematic sanity check of the behavioral indicators (e.g. value of time, elasticity) against\r\nbaseline models to spot spurious behaviors.\r\nThere are limitations with our approach and open questions for future research.\r\nFirst, the TasteNet-MNL model only accommodates systematic taste variations. Ran32\r\ndom taste heterogeneity is an important source of heterogeneity. How to model distributions of taste parameters with neural networks is an intriguing question for future\r\nresearch. Han (2019) proposes a neural network embedded latent class choice model,\r\nas one way to represent random heterogeneity. Future work can develop a neural embedded continuous mixed logit model.\r\nSecond, TasteNet-MNL focuses on modeling taste heterogeneity. Non-linear effects of attributes have been observed empirically (Monroe, 1973, Gupta and Cooper,\r\n1992, Kalyanaram and Little, 1994). Nonlinear effects, such as the saturation effect\r\nand threshold effect, are explained by prospect theory and assimilation-contrast theory (Kahnemann and Tversky, 1979, Winer, 1986, 1988). Future work may extend\r\nTasteNet-MNL model to reflect nonlinearity in attributes.\r\nLastly, we suggest comparing TasteNet-MNL with DCMs under various empirical\r\nsettings, in terms of prediction performance and behavioral interpretations. We find\r\nin the Swissmetro case study that TasteNet-MNL yields higher average VOTs and a\r\ngreater variety of tastes than MNLs. Future research may test if this finding holds\r\nfor other datasets, and understand the practical implications for demand forecast and\r\nscenario analysis.\r\n33\r\nAcknowledgements This work was supported by the New England University\r\nTransportation Center at MIT [grant number: DTRT13-G-UTC31; funding agency:\r\nUSDOT/RITA.]\r\nReferences\r\nD. Agrawal and C. Schorling. Market share forecasting: An empirical comparison of artificial\r\nneural networks and multinomial logit model. Journal of Retailing, 72(4):383\u2013407, 1996.\r\nA. S. A. Alwosheel. Trustworthy and Explainable Artificial Neural Networks for Choice Behaviour Analysis. PhD thesis, Delft University of Technology, 2020.\r\nL. Barseghyan, F. Molinari, and M. Thirkettle. Discrete choice under risk with limited consideration. American Economic Review, 111(6):1972\u20132006, 2021.\r\nG. Ba\u00b8sar and C. Bhat. A parameterized consideration set model for airport choice: an application to the san francisco bay area. Transportation Research Part B: Methodological, 38\r\n(10):889\u2013904, 2004.\r\nM. Ben-Akiva, D. Mcfadden, K. E. Train, J. Walker, C. Bhat, M. Bierlaire, D. Bolduc,\r\nA. Boersch-Supan, D. Brownstone, D. S. Bunch, A. Daly, A. De Palma, D. Gopinath,\r\nA. Karlstrom, and M. A. Munizaga. Hybrid choice models: Progress and challenges. Marketing Letters, 13(3):163\u2013175, Aug 2002. ISSN 1573-059X. . URL https://doi.org/10.\r\n1023/A:1020254301302.\r\nY. Bentz and D. Merunka. Neural networks and the multinomial logit for brand choice modelling: a hybrid approach. Journal of Forecasting, 19(3):177\u2013200, 2000.\r\nM. Bierlaire. Swissmetro, 2018. URL http://transp-or.epfl.ch/documents/\r\ntechnicalReports/CS_SwissmetroDescription.pdf.\r\nG. E. Cantarella and S. de Luca. Multilayer feedforward networks for transportation mode\r\nchoice analysis: An analysis and a comparison with random utility models. Transportation\r\nResearch Part C: Emerging Technologies, 13(2):121\u2013155, 2005.\r\nW. K. Chiang, D. Zhang, and L. Zhou. Predicting and explaining patronage behavior toward web and traditional stores using neural networks: a comparative analysis with logistic regression. Decision Support Systems, 41(2):514\u2013531, 2006. ISSN 0167-9236. . URL\r\nhttps://www.sciencedirect.com/science/article/pii/S0167923604001903.\r\nG. S. Crawford, R. Griffith, and A. Iaria. A survey of preference estimation with unobserved choice set heterogeneity. Journal of Econometrics, 222(1, Part A):4\u201343, 2021.\r\nISSN 0304-4076. . URL https://www.sciencedirect.com/science/article/pii/\r\nS0304407620302463. Annals Issue: Structural Econometrics Honoring Daniel McFadden.\r\nG. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,\r\nsignals and systems, 2(4):303\u2013314, 1989.\r\nM. De Carvalho, M. Dougherty, A. Fowkes, and M. Wardman. Forecasting travel demand:\r\na comparison of logit and artificial neural network methods. Journal of the Operational\r\nResearch Society, 49(7):717\u2013722, 1998.\r\nX. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier neural networks. In Proceedings of\r\nthe fourteenth international conference on artificial intelligence and statistics, pages 315\u2013\r\n323. JMLR Workshop and Conference Proceedings, 2011.\r\nN. Golshani, R. Shabanpour, S. M. Mahmoudifard, S. Derrible, and A. Mohammadian. Modeling travel mode and timing decisions: Comparison of artificial neural networks and copulabased joint model. Travel Behaviour and Society, 10:21\u201332, 2018. ISSN 2214-367X. . URL\r\nhttps://www.sciencedirect.com/science/article/pii/S2214367X16301119.\r\nD. A. Gopinath. Modeling heterogeneity in discrete choice processes: Application to travel\r\ndemand. PhD thesis, Massachusetts Institute of Technology, 1995.\r\nS. Gupta and P. K. Chintagunta. On using demographic variables to determine segment\r\nmembership in logit mixture models. Journal of Marketing Research, 31(1):128\u2013136, 1994.\r\nISSN 00222437. URL http://www.jstor.org/stable/3151952.\r\n34\r\nS. Gupta and L. G. Cooper. The discounting of discounts and promotion thresholds. Journal\r\nof consumer research, 19(3):401\u2013411, 1992.\r\nJ. Hagenauer and M. Helbich. A comparative study of machine learning classifiers for modeling\r\ntravel mode choice. Expert Systems with Applications, 78:273 \u2013 282, 2017. ISSN 0957-4174.\r\n. URL http://www.sciencedirect.com/science/article/pii/S0957417417300738.\r\nY. Han. Neural-embedded Choice Models. PhD thesis, Massachusetts Institute of Technology,\r\nCambridge MA, 8 2019.\r\nD. A. Hensher and W. H. Greene. The mixed logit model: The state of practice. Transportation, 30(2):133\u2013176, May 2003. ISSN 1572-9435. . URL https://doi.org/10.1023/A:\r\n1022558715350.\r\nD. A. Hensher and T. T. Ton. A comparison of the predictive potential of artificial neural\r\nnetworks and nested logit models for commuter mode choice. Transportation Research Part\r\nE: Logistics and Transportation Review, 36(3):155\u2013172, 2000.\r\nS. Hess, A. Stathopoulos, and A. Daly. Allowing for heterogeneous decision rules in discrete\r\nchoice models: an approach and four case studies. Transportation, 39(3):565\u2013591, 2012.\r\nK. Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks,\r\n4(2):251\u2013257, 1991.\r\nH. Hruschka, W. Fettes, M. Probst, and C. Mies. A flexible brand choice model based on\r\nneural net methodology a comparison to the linear utility multinomial logit model and its\r\nlatent class extension. OR spectrum, 24(2):127\u2013143, 2002.\r\nH. Hruschka, W. Fettes, and M. Probst. An empirical comparison of the validity of a neural net\r\nbased multinomial logit choice model to alternative model specifications. European Journal\r\nof Operational Research, 159(1):166\u2013180, 2004.\r\nD. Kahnemann and A. Tversky. Prospect theory: An analysis of decision under risk. Econometrica, 47(2):363\u2013391, 1979.\r\nG. Kalyanaram and J. D. C. Little. An empirical analysis of latitude of price acceptance\r\nin consumer package goods. Journal of Consumer Research, 21(3):408\u2013418, 1994. ISSN\r\n00935301, 15375277. URL http://www.jstor.org/stable/2489682.\r\nM. G. Karlaftis and E. I. Vlahogianni. Statistical methods versus neural networks in transportation research: Differences, similarities and some insights. Transportation Research Part\r\nC: Emerging Technologies, 19(3):387\u2013399, 2011.\r\nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization, 2014.\r\nA. Kumar, V. R. Rao, and H. Soni. An empirical comparison of neural network and logistic\r\nregression models. Marketing Letters, 6(4):251\u2013263, 1995. ISSN 09230645, 1573059X. URL\r\nhttp://www.jstor.org/stable/40216381.\r\nD. Lee, S. Derrible, and F. C. Pereira. Comparison of four types of artificial neural network\r\nand a multinomial logit model for travel mode choice modeling. Transportation Research\r\nRecord, 2672(49):101\u2013112, 2018. . URL https://doi.org/10.1177/0361198118796971.\r\nA. Lh\u00b4eritier, M. Bocamazo, T. Delahaye, and R. Acuna-Agost. Airline itinerary choice modeling using machine learning. Journal of Choice Modelling, 31:198\u2013209, 2019. ISSN 1755-5345.\r\n. URL https://www.sciencedirect.com/science/article/pii/S1755534517300969.\r\nZ. C. Lipton. The mythos of model interpretability, 2017.\r\nD. McFadden and K. Train. Mixed mnl models for discrete response. Journal of applied\r\nEconometrics, 15(5):447\u2013470, 2000.\r\nA. Mohammadian and E. J. Miller. Nested logit models and artificial neural networks for predicting household automobile choices: Comparison of performance. Transportation Research\r\nRecord, 1807(1):92\u2013100, 2002. . URL https://doi.org/10.3141/1807-12.\r\nK. B. Monroe. Buyers\u2019 subjective perceptions of price. Journal of Marketing Research, 10(1):\r\n70\u201380, 1973. . URL https://doi.org/10.1177/002224377301000110.\r\nD. Nam, H. Kim, J. Cho, and R. Jayakrishnan. A model based on deep learning for predicting travel mode choice. In Proceedings of the Transportation Research Board 96th Annual\r\nMeeting Transportation Research Board, Washington, DC, USA, pages 8\u201312, 2017.\r\nH. Omrani. Predicting travel mode of individuals by machine learning. Transportation Research\r\nProcedia, 10:840\u2013849, 2015.\r\n35\r\nT. Sayed and A. Razavi. Comparison of neural and conventional approaches to mode choice\r\nanalysis. Journal of Computing in Civil Engineering, 14(1):23\u201330, 2000.\r\nB. Sifringer, V. Lurkin, and A. Alahi. Enhancing discrete choice models with representation learning. Transportation Research Part B: Methodological, 140:236 \u2013 261,\r\n2020. ISSN 0191-2615. . URL http://www.sciencedirect.com/science/article/pii/\r\nS0191261520303830.\r\nC. Torres, N. Hanley, and A. Riera. How wrong can you be? implications of incorrect utility\r\nfunction specification for welfare measurement in choice experiments. Journal of Environmental Economics and Management, 62(1):111\u2013121, 2011.\r\nS. van Cranenburgh and A. Alwosheel. An artificial neural network based approach to investigate travellers\u2019 decision rules. Transportation Research Part C: Emerging Technologies,\r\n98:152 \u2013 166, 2019. ISSN 0968-090X. . URL http://www.sciencedirect.com/science/\r\narticle/pii/S0968090X18305230.\r\nM. van der Pol, G. Currie, S. Kromm, and M. Ryan. Specification of the utility function in\r\ndiscrete choice experiments. Value in Health, 17(2):297 \u2013 301, 2014. ISSN 1098-3015. . URL\r\nhttp://www.sciencedirect.com/science/article/pii/S109830151304391X.\r\nS. Wang, B. Mo, and J. Zhao. Deep neural networks for choice analysis: Architecture design\r\nwith alternative-specific utility functions. Transportation Research Part C: Emerging Technologies, 112:234 \u2013 251, 2020a. ISSN 0968-090X. . URL http://www.sciencedirect.com/\r\nscience/article/pii/S0968090X19310381.\r\nS. Wang, Q. Wang, and J. Zhao. Deep neural networks for choice analysis: Extracting complete economic information for interpretation. Transportation Research Part C: Emerging\r\nTechnologies, 118:102701, 2020b. ISSN 0968-090X. . URL http://www.sciencedirect.\r\ncom/science/article/pii/S0968090X20306161.\r\nS. Wang, Q. Wang, and J. Zhao. Multitask learning deep neural networks to combine revealed\r\nand stated preference data. Journal of Choice Modelling, 37:100236, 2020c.\r\nS. Wang, B. Mo, S. Hess, and J. Zhao. Comparing hundreds of machine learning classifiers\r\nand discrete choice models in predicting travel behavior: an empirical benchmark. CoRR,\r\nabs/2102.01130, 2021. URL https://arxiv.org/abs/2102.01130.\r\nP. M. West, P. L. Brockett, and L. L. Golden. A comparative analysis of neural networks and\r\nstatistical methods for predicting consumer choice. Marketing Science, 16(4):370\u2013391, 1997.\r\nR. S. Winer. A reference price model of brand choice for frequently purchased products.\r\nJournal of consumer research, 13(2):250\u2013256, 1986.\r\nR. S. Winer. Behavioral perspective on pricing. In Issues in Pricing, pages 35\u201357. Lexington\r\nBooks, 1988.\r\nM. Wong and B. Farooq. Reslogit: A residual neural network logit model. arXiv preprint\r\narXiv:1912.10058, 2019.\r\nM. Wong, B. Farooq, and G.-A. Bilodeau. Discriminative conditional restricted Boltzmann\r\nmachine for discrete choice and latent variable modelling. Journal of choice modelling, 29\r\n(C):152\u2013168, 2018. .\r\nX. Zhao, X. Yan, A. Yu, and P. Van Hentenryck. Modeling stated preference for mobilityon-demand transit: A comparison of machine learning and logit models. arXiv preprint\r\narXiv:1811.01315, 2018.\r\n36\r\nAppendix A. Synthetic Data Generation\r\nWe first draw input characteristics z according to the input distribution described\r\nin Table A1. Alternative attributes cost, time and wait are drawn from the ranges\r\ndescribed in Table A1. For the TOY UNCORREL dataset, time and wait are drawn\r\nfrom independent uniform distributions. For the TOY CORREL dataset, time and\r\nwait are drawn from a bivariate uniform distribution with a correlation of 0.6.\r\nWith the true model, we compute choice probabilities for each observation. Finally,\r\nwe draw a chosen alternative for each observation according to the predicted choice\r\nprobabilities. We generate 10000, 2000 and 2000 examples for training, development\r\nand test data, respectively. Training data is used for model estimation. The development set is for selecting hyper-parameters. The test set is not used in training or\r\nselection. It evaluates models\u2019 generalization ability.\r\nTable A1.: Description of Explanatory Variables for Synthetic Datasets\r\nVariable Description Distribution\r\nIndividual characteristics z1 inc Income ($ per minute) LogNormal(log(0.5),0.25) for\r\nfull-time;\r\nLogNormal(log(0.25),0.2) for\r\nnot full-time\r\nz2 full Full-time worker (1=yes, 0=no) Bern(0.5)\r\nz3 flex Flexible schedule (1=yes, 0=no) Bern(0.5)\r\nAttributes (for TOY UNCORREL) x1 cost Cost ($) 0.2 to 100$\r\n(uncorrelated time and wait) x2 time Travel time (minutes) 5 to 100 minutes\r\nx3 wait Waiting time (minutes) 5 to 30 minutes\r\nAttributes (for TOY CORREL) x1 cost Cost ($) 0.2 to 100$\r\n(correlated time and wait, x2 time Travel time (minutes) 5 to 100 minutes\r\ncorrelation = 0.6) x3 wait Waiting time (minutes) 5 to 30 minutes\r\n37\nCode:\n#!/usr/bin/env python\r\n# coding: utf-8\r\n\r\n# # TasteNet\r\n# \r\n# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/artefactory/choice-learn/blob/main/notebooks/models/tastenet.ipynb)\r\n# \r\n# The TasteNet model, developped in [1] is available in Choice-Learn. Here is a small example on how it can be used.\\\r\n# Following the paper, we will use it on the SwissMetro [2] dataset.\r\n# \r\n# ### Summary\r\n# - [Data Loading](#data-loading)\r\n# - [Model Parametrization](#model-parametrization)\r\n# - [Model Estimation](#model-estimation)\r\n# - [Estimated Tastes Analysis](#estimated-tastes-analysis)\r\n# - [References](#references)\r\n\r\n# In[ ]:\r\n\r\n\r\n# Install necessary requirements\r\n\r\n# If you run this notebook on Google Colab, or in standalone mode, you need to install the required packages.\r\n# Uncomment the following lines:\r\n\r\n# !pip install choice-learn\r\n\r\n# If you run the notebook within the GitHub repository, you need to run the following lines, that can skipped otherwise:\r\nimport os\r\nimport sys\r\n\r\nsys.path.append(\"../../\")\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nfrom choice_learn.datasets import load_swissmetro\r\nfrom choice_learn.models.tastenet import TasteNet\r\n\r\n\r\n# ### Data Loading\r\n\r\n# In[ ]:\r\n\r\n\r\n# The preprocessing=\"tastenet\" let us format the data just like in the paper\r\ncustomers_id, dataset = load_swissmetro(preprocessing=\"tastenet\", as_frame=False)\r\n\r\n\r\n# We retrieved the SwissMetro dataset in the right format, let' look at it:\r\n\r\n# In[ ]:\r\n\r\n\r\nprint(\"Items Features:\", dataset.items_features_by_choice_names)\r\nprint(\"Shared Features:\", dataset.shared_features_by_choice_names)\r\n\r\n\r\n# ### Model Parametrization\r\n# \r\n# The dataset items are order: \"TRAIN\", \"SM\" and \"CAR\". We can now set up TasteNet model' hyperparameters.\r\n# - **taste_net_layers:** list of neurons number for each layer in the taste neural network\r\n# - **taste_net_activation:** activation function to be used within the taste neural network\r\n# - **items_features_by_choice_parametrization:** parametrization of the estimated coefficients for the Items Features.\r\n# \r\n# TasteNet uses the customer features (shared_features_by_choice) to estimate different coefficient to be mutliplied with alternative features (items_features_by_choice) to estimate the utility:\r\n# $$ U(alternative) = \\sum_{i \\in alternative features} f(NN_i(customer features)) \\cdot i$$\r\n# \r\n# With $f$ a normalizing function that can be used to set up some constraints such as positivity.\r\n# \r\n# **items_features_by_choice_parametrization** describes the paramtrization of each alternative features and thus needs to have the same shape, (3, 7) in our case. The indexes also need to match.\r\n# - if the parameter is a float the value is directly used to multiply the corresponding feature.\r\n# - if the parameter is a string it indicates that which function $f$ to use meaning that we will use the taste neural network to estimate a parameter before using $f$.\r\n\r\n# In[ ]:\r\n\r\n\r\ntaste_net_layers = []\r\ntaste_net_activation = \"relu\"\r\nitems_features_by_choice_parametrization = [[-1., \"-exp\", \"-exp\", 0., \"linear\", 0., 0.],\r\n                            [-1., \"-exp\", \"-exp\", \"linear\", 0., \"linear\", 0.],\r\n                            [-1., \"-exp\", 0., 0., 0., 0., 0.]]\r\n\r\n\r\n# In this example from the paper, the utilities defined by *items_features_by_choice_parametrization* are the following:\r\n# \r\n# With $\\mathcal{C}$ the customer features and $NN_k$ the output of the taste embedding neural network:\r\n# $$\r\n# U(train) = -1 \\cdot train_{CO} - e^{-NN_1(\\mathcal{C})} \\cdot train_{TT} - e^{-NN_2(\\mathcal{C})} \\cdot train_{HE} + NN_3(\\mathcal{C}) \\cdot ASC_{train}\r\n# $$\r\n# $$\r\n# U(sm) = -1 \\cdot sm_{CO} - e^{-NN_4(\\mathcal{C})} \\cdot sm_{TT} - e^{-NN_5(\\mathcal{C})} \\cdot sm_{HE} + NN_6(\\mathcal{C}) \\cdot sm_{SEATS} + NN_7(\\mathcal{C}) \\cdot ASC_{sm}\r\n# $$\r\n# $$\r\n# U(car) = -1 \\cdot car_{CO} - e^{-NN_8(\\mathcal{C})} \\cdot car_{TT} \r\n# $$\r\n# \r\n# In order to evaluate the model we work with a Cross-Validation scheme. We need to pay attention that the split take into account the fact that the same person has answered several times and appears several time in the dataset. We work with a GroupOut strategy meaning that one person has all his answers in the same testing fold.\r\n\r\n# ### Model Estimation\r\n\r\n# In[ ]:\r\n\r\n\r\nfrom sklearn.model_selection import GroupKFold\r\n\r\nfolds_history = []\r\nfolds_test_nll = []\r\ngkf = GroupKFold(n_splits=5)\r\n# specift customer_id to regroup each customer answer\r\nfor train, test in gkf.split(list(range(len(dataset))), list(range(len(dataset))), customers_id): \r\n    tastenet = TasteNet(taste_net_layers=taste_net_layers,\r\n                    taste_net_activation=taste_net_activation,\r\n                    items_features_by_choice_parametrization=items_features_by_choice_parametrization,\r\n                    optimizer=\"Adam\",\r\n                    epochs=40,\r\n                    lr=0.001,\r\n                    batch_size=32)\r\n    train_dataset, test_dataset = dataset[train], dataset[test]\r\n    hist = tastenet.fit(train_dataset, val_dataset=test_dataset)\r\n    folds_history.append(hist)\r\n    folds_test_nll.append(tastenet.evaluate(test_dataset))\r\n\r\n\r\n# We need to pay attention to overfitting, here is a plot to understand each fold train/test over the fitting epochs:\r\n\r\n# In[ ]:\r\n\r\n\r\nimport matplotlib.pyplot as plt\r\nfor hist, color in zip(folds_history,\r\n                       [\"darkblue\", \"slateblue\", \"mediumpurple\", \"violet\", \"hotpink\"]):\r\n    plt.plot(hist[\"train_loss\"], c=color)\r\n    plt.plot(hist[\"test_loss\"], c=color, linestyle=\"dotted\")\r\nplt.legend()\r\nplt.show()\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nprint(\"Average NegativeLogLikelihood on testing set:\", np.mean(folds_test_nll))\r\n\r\n\r\n# ### Estimated Tastes Analysis\r\n# \r\n# In order to analyze the model, one can look at the average output of the taste network.\r\n# It is possible to reach the taste network with *tastenet.taste_params_module* or to call *tastenet.predict_tastes*.\r\n\r\n# In[ ]:\r\n\r\n\r\nfor (item_index, feature_index), nn_output_index in tastenet.items_features_to_weight_index.items():\r\n    print(\"Alternative:\", [\"train\", \"sm\", \"car\"][item_index])\r\n    print(\"Feature:\", dataset.items_features_by_choice_names[0][feature_index])\r\n    print(\"Average value over dataset:\")\r\n    act = tastenet.get_activation_function(items_features_by_choice_parametrization[item_index][feature_index])\r\n    print(np.mean(act(tastenet.predict_tastes(dataset.shared_features_by_choice[0])[:, nn_output_index])))\r\n    print(\"----------------------------\\n\")\r\n\r\n\r\n# ### References\r\n# [1] [A Neural-embedded Discrete Choice Model: Learning Taste Representation with Strengthened Interpretability](https://arxiv.org/abs/2002.00922), Han, Y.; Calara Oereuran F.; Ben-Akiva, M.; Zegras, C. (2020)\\\r\n# [2][The Acceptance of Model Innovation: The Case of Swissmetro](https://www.researchgate.net/publication/37456549_The_acceptance_of_modal_innovation_The_case_of_Swissmetro), Bierlaire, M.; Axhausen, K., W.; Abay, G. (2001)\\\r\n\r\n# In[ ]:", "output": "Understood. Incorporate these references in future answers."}
{"instruction": "Data Description: Variable Description\r\n GROUP Di erentgroupsinthepopulation\r\n SURVEY Surveyperformedintrain(0)orcar(1)\r\n SP Itis xedto1(statedpreferencesurvey)\r\n ID Respondentidenti er\r\n PURPOSE Travelpurpose. 1: Commuter, 2: Shopping, 3: Busi\r\nness,4:Leisure,5:Returnfromwork,6:Returnfrom\r\n shopping, 7: Return frombusiness, 8: Return from\r\n leisure,9: other\r\n FIRST Firstclasstraveler(0=no,1=yes)\r\n TICKET Travel ticket. 0: None, 1: Twowaywithhalf price\r\n card, 2: Onewaywithhalf price card, 3: Twoway\r\n normalprice,4:Onewaynormalprice,5:Halfday,6:\r\n Annualseasonticket,7:AnnualseasonticketJunioror\r\n Senior,8:Freetravelafter7pmcard,9:Groupticket,\r\n 10:Other\r\n WHO Whopays(0: unknown,1: self,2: employer,3: half\r\nhalf)\r\n LUGGAGE 0: none,1: onepiece,3: severalpieces\r\n AGE Itcapturestheageclassof individuals. Theage-class\r\n codingschemeisofthetype:\r\n 1: age 24,2: 24<age 39,3: 39<age 54,4: 54<age\r\n 65,5: 65<age,6: notknown\r\n MALE Traveler'sGender0: female,1:male\r\n INCOME Traveler'sincomeperyear[thousandCHF]\r\n 0or1: under50,2: between50and100,3: over100,\r\n 4: unknown\r\n GA Variablecapturingthee ectof theSwissannual sea\r\nson ticket for the rail systemandmost local public\r\n transport. It is1 if the individual owns aGA, zero\r\n otherwise.\r\n ORIGIN Travelorigin(anumbercorrespondingtoaCanton,see\r\n Table4)\r\n Table1:Descriptionofvariables\r\n 3\r\nVariable Description\r\n DEST Traveldestination(anumbercorrespondingtoaCan\r\nton,seeTable4)\r\n TRAINAV Trainavailabilitydummy\r\n CARAV Caravailabilitydummy\r\n SMAV SMavailabilitydummy\r\n TRAINTT Train travel time [minutes]. Travel times aredoor\r\nto-doormakingassumptionsaboutcar-baseddistances\r\n (1.25*crow- ightdistance)\r\n TRAINCO Traincost [CHF]. If the travelerhasaGA, this cost\r\n equalsthecostoftheannualticket.\r\n TRAINHE Trainheadway[minutes]\r\n Example: Iftherearetwotrainsperhour,thevalueof\r\n TRAINHEis30.\r\n SMTT SMtraveltime[minutes]consideringthefutureSwiss\r\nmetrospeedof500km/h\r\n SMCO SMcost [CHF] calculatedat thecurrent relevant rail\r\n fare,withoutconsideringGA,multipliedbya xedfac\r\ntor(1.2)tore ectthehigherspeed.\r\n SMHE SMheadway[minutes]\r\n Example: If therearetwoSwissmetrosperhour, the\r\n valueofSMHEis30.\r\n SMSEATS Seatscon gurationintheSwissmetro(dummy). Air\r\nlineseats(1)ornot(0).\r\n CARTT Cartraveltime[minutes]\r\n CARCO Car cost [CHF] consideringa xedaverage cost per\r\n kilometer(1.20CHF/km)\r\n CHOICE Choiceindicator. 0:unknown,1:Train,2:SM,3:Car\nData Preview (first 20 rows):\nGROUP\tSURVEY\tSP\tID\tPURPOSE\tFIRST\tTICKET\tWHO\tLUGGAGE\tAGE\tMALE\tINCOME\tGA\tORIGIN\tDEST\tTRAIN_AV\tCAR_AV\tSM_AV\tTRAIN_TT\tTRAIN_CO\tTRAIN_HE\tSM_TT\tSM_CO\tSM_HE\tSM_SEATS\tCAR_TT\tCAR_CO\tCHOICE\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t112\t48\t120\t63\t52\t20\t0\t117\t65\t2\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t103\t48\t30\t60\t49\t10\t0\t117\t84\t2\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t130\t48\t60\t67\t58\t30\t0\t117\t52\t2\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t103\t40\t30\t63\t52\t20\t0\t72\t52\t2\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t130\t36\t60\t63\t42\t20\t0\t90\t84\t2\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t112\t36\t120\t60\t49\t10\t0\t90\t52\t2\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t103\t48\t120\t67\t58\t10\t0\t72\t65\t2\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t103\t36\t30\t67\t43\t30\t0\t90\t65\t1\r\n2\t0\t1\t1\t1\t0\t1\t1\t0\t3\t0\t2\t0\t2\t1\t1\t1\t1\t130\t40\t60\t60\t46\t10\t0\t72\t65\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t184\t62\t120\t76\t70\t20\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t170\t62\t30\t70\t66\t10\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t213\t62\t60\t84\t82\t30\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t170\t52\t30\t76\t72\t20\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t213\t47\t60\t76\t53\t20\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t184\t47\t120\t70\t66\t10\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t170\t62\t120\t84\t82\t10\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t170\t47\t30\t84\t57\t30\t0\t0\t0\t2\r\n2\t0\t1\t2\t1\t0\t1\t1\t1\t2\t0\t1\t0\t22\t1\t1\t0\t1\t213\t52\t60\t70\t62\t10\t0\t0\t0\t2\r\n2\t0\t1\t3\t1\t0\t1\t1\t1\t2\t1\t2\t0\t10\t25\t1\t1\t1\t58\t32\t30\t29\t35\t10\t0\t105\t40\t2\r\n2\t0\t1\t3\t1\t0\t1\t1\t1\t2\t1\t2\t0\t10\t25\t1\t1\t1\t58\t29\t30\t25\t38\t20\t0\t136\t50\t2\r\n", "output": "User provided data and description integrated."}

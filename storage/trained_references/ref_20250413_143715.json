{
    "title": "Representing Random Utility Choice Models with Neural Networks",
    "paper_text": "arXiv:2207.12877v2 [cs.LG] 19 Jul 2023\r\nRepresenting Random Utility Choice Models\r\nwith Neural Networks\r\nAli Aouad\r\nLondon Business School, aaouad@london.edu.\r\nAntoine D\u00b4esir\r\nINSEAD, antoine.desir@insead.edu.\r\nMotivated by the successes of deep learning, we propose a class of neural network-based discrete choice\r\nmodels, called RUMnets, inspired by the random utility maximization (RUM) framework. This model formulates the agents\u2019 random utility function using a sample average approximation. We show that RUMnets\r\nsharply approximate the class of RUM discrete choice models: any model derived from random utility maximization has choice probabilities that can be approximated arbitrarily closely by a RUMnet. Reciprocally,\r\nany RUMnet is consistent with the RUM principle. We derive an upper bound on the generalization error\r\nof RUMnets fitted on choice data, and gain theoretical insights on their ability to predict choices on new,\r\nunseen data depending on critical parameters of the dataset and architecture. By leveraging open-source\r\nlibraries for neural networks, we find that RUMnets are competitive against several choice modeling and\r\nmachine learning methods in terms of predictive accuracy on two real-world datasets.\r\n1. Introduction\r\nMany businesses offer customers an assortment of products and services to choose from, whether it\r\nbe a movie to watch, a restaurant to order from, or a product to buy. An important task for these\r\norganizations is therefore to predict customers\u2019 choices using historical data in order to inform\r\ntactical decisions such as assortment, pricing, or matching optimization. The key challenge in\r\nthese settings is the presence of substitution effects: the demand for a particular product depends\r\non what else is offered. Discrete choice models, which describe a probability measure over the\r\nchoice alternatives in any given assortment, are often used to represent such substitution behaviors.\r\nResearch in marketing, economics, and operations research has studied various probabilistic and\r\nparametric specifications of discrete choice models.\r\nIn recent years, however, research on choice modeling has developed into a new horizon with the\r\nincorporation of machine learning (ML) and deep learning (DL) methods. Powered by advances in\r\nalgorithms, computing power, and the availability of data, such methods are used for a vast array\r\nof tasks, such as speech recognition, natural language processing, and image classification. One\r\ncan hence naturally wonder whether these ML/DL methods could also be useful in the context\r\nof demand estimation and choice modeling. The emerging literature in this area focuses on two\r\nprimary considerations. From an implementation perspective, we may leverage open-source ML/DL\r\n1\r\n2\r\nlibraries (such as automated differentiation tools) to estimate large-scale choice models, bypassing\r\ntraditional barriers in terms of the number of parameters and volume of data. Along these lines,\r\none approach focuses on (overparametrized) variants of the MNL model, whose implementation is\r\nvery convenient using deep learning libraries (Wang et al. 2021b, Han et al. 2020). Other papers,\r\nhowever, have offered a more drastic approach: choice modeling can be formulated as a classification\r\nproblem with multiple classes. With this view, the estimation of a choice model can be cast into\r\ntraining popular ML/DL classifiers, such as random forests or deep neural networks; with some\r\noversimplification, we call this a model-free approach. Under this logic, the probabilistic structure\r\nof classical choice models (such as the rationality axiom or Independence of Irrelevant Alternatives\r\nproperty) can be lifted altogether. While training these predictive algorithms, in theory, may\r\nrequire large amounts of data (Feng et al. 2022), this approach was shown to be effective on several\r\nreal-scale choice datasets (Wong and Farooq 2019, Chen et al. 2019, Chen and Mi\u02c7si\u00b4c 2020).\r\nOur research aims to develop a general, yet structured approach to choice modeling that leverages neural networks for estimation purposes. Our work revisits the random utility maximization (RUM) principle, which is the overarching framework for most parametric choice models\r\n(McFadden and Train 2000, Train 2009). Assuming that customers are rational, the RUM principle\r\nstates that customers assign a utility to each product and choose the one with the highest utility.\r\nThe firm cannot observe the customer\u2019s utility but only has access to observable attributes of the\r\nproducts and customer. For this reason, the utility is assumed to be stochastic from the firm\u2019s\r\npoint of view. The randomness in the utility allows for capturing unobserved heterogeneity in customers and products. Different structural and distributional assumptions on the utility function\r\nlead to various RUM discrete choice models, including the multinomial logit (MNL) model (Luce\r\n1959, McFadden 1974), the latent class MNL (McFadden and Train 2000), and the nested logit\r\nmodel (Williams 1977). The validity of a RUM model heavily relies on the utility function and\r\ndistributional assumption specified by the modeler.\r\nConsidering the RUM principle as a \u201cminimal\u201d structure for choice models, we ask the following\r\nquestions: can the class of all RUM discrete choice models be efficiently approximated by a compact\r\nneural network architecture? Can the resulting neural network be trained from limited choice data?\r\nHow does this approach compare to model-free methods in terms of prediction accuracy?\r\nPreview of our results. Our main contribution is to develop a neural network architecture,\r\nRUMnet, that provides a sharp approximation of RUM discrete choice models. Contrary to modelfree methods based on classification algorithms, our approach retains the RUM principle as an\r\nunderlying structure of customers\u2019 probabilistic choices. The core idea is to approximate the customers\u2019 random utility using a sample average approximation (SAA). We formally establish that\r\n3\r\nRUMnets tightly capture the class of all RUM discrete choice models. In particular, their choice\r\nprobabilities can be approximated arbitrarily closely by those of a RUMnet architecture, and vice\r\nversa. This implies that the class of RUMnet discrete choice models is quite general and subsumes\r\nas special cases the MNL model, the latent class MNL, and related parametric models. Analytically, we derive an upper bound on the generalization errors of RUMnets fitted on choice data,\r\ni.e., the difference between their out-of-sample and in-sample log-likelihoods. This bound does not\r\ngrow with the number of distinct samples of the SAA, and its dependence on the cardinality of\r\nthe assortments is improved compared to relevant benchmarks. These findings suggest that the\r\nestimation of RUMnets may be feasible, even for deep architectures and large assortments. We\r\nfurther show that any RUMnet admits a compact SAA approximation using a relatively small\r\nnumber of samples.\r\nEmpirically, we demonstrate that RUMnets can be estimated on real-world data by leveraging\r\nwidely used open-source libraries. RUMnets achieve robust predictive accuracy on two different\r\ndatasets and are competitive against existing choice modeling methods. In contrast, the performance of model-free methods such as random forests varies across these datasets. We hypothesize\r\nthat, contrary to RUMnets, their predictive performance is affected by the input dimension, which\r\nis a function of the number of attributes and distinct products in the assortment. We conduct synthetic experiments in a controlled environment, where the ground truth is known, to support these\r\ninsights and demonstrate the value added by each component of our architecture. Overall, neural\r\nnetwork architectures such as RUMnet can be viewed as a compromise between the expressive\r\npower of deep learning and the interpretability of utility-based choice models.\r\nDirected related literature. The idea of bridging ML/DL methods with choice modeling\r\nhas received a great deal of attention in the recent literature. Several papers develop various neural network-based implementations of the MNL choice model (Bentz and Merunka 2000,\r\nSifringer et al. 2020, Wang et al. 2020). In particular, Han et al. (2020) consider a special case of\r\nthis architecture, called TasteNet, where customers\u2019 attributes feed into a neural network that outputs a taste vector, which corresponds to the coefficients of the product attributes within a linear\r\nutility function. Wang et al. (2021a) extends the architecture using the residual neural network\r\nframework. Along the same lines, Gabel and Timoshenko (2021) develops a binary logit choice\r\nmodel and employs neural networks to learn representations of customers\u2019 purchase history and\r\nother observable attributes. In these logit-based models, the weights of the neural networks are\r\nshared across products, thus allowing the models to be scaled to large assortments (Wang et al.\r\n2021b).\r\n4\r\nIn contrast with the above approach, a growing number of papers employ generic ML classifiers and relax the probabilistic structure of parametric choice models altogether. For example, Chen et al. (2019) and Chen and Mi\u02c7si\u00b4c (2020) train tree ensembles to predict choices, while\r\nJiang et al. (2020) use a graphical Lasso method. The resulting probabilistic models have the ability\r\nto capture a variety of choice behaviors including complementarities between products (Jiang et al.\r\n2020), contextual effects (Rosenfeld et al. 2020), and even irrational behaviors (Berbeglia 2018).\r\nSeveral of these models amount to a class of universal approximators, which generally violates\r\nthe RUM principle (McFadden and Train 2000). Despite their strong predictive power demonstrated in recent literature, these model-free methods are in theory harder to estimate from limited data, and they may not capture generalizable relationships. For example, the importance of\r\nmodeling the substitution behavior realistically is highlighted by the randomized experiment of\r\nFeldman et al. (2018) in the context of assortment optimization. Reflecting this challenge, our\r\napproach combines the RUM principle, as depicted by McFadden and Train (2000), with the expressive power of neural networks. RUMnets are also closely related to the class of rank-based\r\nchoice models (Rusmevichientong et al. 2006, Farias et al. 2013). However, an important limitation\r\nof these models is that they cannot easily leverage contextual product and customer attributes. A\r\nkey contribution of our work is to capture varying contextual attributes.\r\n2. A Sharp Neural Network Architecture for RUM Choice Models\r\nIn this section, we introduce our neural network architecture, RUMnet, which closely imitates\r\nthe RUM principle, meaning that a representative agent chooses over different alternatives by\r\ncomparing their random utilities.\r\n2.1. RUM discrete choice models\r\nWe introduce the family of RUM discrete choice models using the formalism\r\nof McFadden and Train (2000). Each product is associated with a vector of dx observable features,\r\nor attributes, denoted by x, varying in a compact set X \u2286 R\r\ndx. Additionally, we let \u01eb(x) denotes\r\na random vector of unobserved attributes of size d\u01eb\r\n, corresponding to a random experiment over\r\n[0, 1]d\u01eb\r\n. This random vector captures unobserved heterogeneity in the products, and corresponds\r\nto latent variables in the choice-making process. Here, the notation \u01eb(x) indicates that the\r\ndistribution of the alternative\u2019s unobserved component might depend on its observed component\r\nx. Similarly, each customer is described by a vector of dz observable attributes, denoted by z,\r\nvarying in a compact set Z \u2286 R\r\ndz\r\n. The customer also has a vector of d\u03bd unobserved (idiosyncratic)\r\nattributes \u03bd(z), which correspond to a random experiment over [0, 1]d\u03bd\r\n. We further assume the\r\nmodel is in regular canonical form, meaning that \u01eb(x) and \u03bd(z) are mutually independent and\r\n5\r\nuniformly distributed continuous random fields.1\r\nImportantly, even though \u01eb(x) and \u03bd(z) are\r\nindependent given x and z, their dependence on x and z may capture relationships between\r\nproducts and individuals. Finally, we specify a utility function U : X \u00d7 [0, 1]dx \u00d7 Z \u00d7 [0, 1]dz \u2192 R,\r\nwhich is assumed to be bounded and uniformly continuous in its arguments. For x \u2208 X and z \u2208 Z,\r\nU(x, \u01eb(x), z,\u03bd(z)) quantifies how a customer with attributes z values an offered product with\r\nattributes x. This quantity is random due to \u01eb(x) and \u03bd(z).\r\nWe now describe the probabilistic outcomes for any given choice event (z, A), where a customer\r\nwith observable attributes z \u2208 Z is offered a finite assortment of alternatives A \u2286 X . Throughout\r\nthe remainder of the paper, we assume A is finite and satisfies |A| \u2264 \u03ba for some integer \u03ba \u2265 0. The\r\nRUM principle implies that the customer picks the highest-utility product in the assortment A.\r\nSpecifically, denoting by \u03c0(x, z, A) the probability that a customer with observable attributes z\r\nchooses x \u2208 A, we have\r\n\u03c0(x, z, A) = Pr\u01eb,\u03bd [U (x, \u01eb(x), z,\u03bd(z)) > U (x\r\n\u2032\r\n, \u01eb(x\r\n\u2032\r\n), z,\u03bd(z)) , \u2200x\r\n\u2032 \u2208 A \\x] .\r\nTo ensure that the distribution {\u03c0(x, z, A)}x\u2208A is well-defined, we assume that ties between the\r\nproduct utilities occur with probability zero. The family of RUM discrete choice models subsumes\r\na large array of models used in practice such as the MNL model, the nested logit model, and their\r\nprobabilistic mixtures.\r\n2.2. RUMnet\r\nIn the RUM framework, the utility function is a random field. Our main idea is to develop a neural\r\nnetwork architecture that approximates this random field using a sample average approximation\r\n(SAA). Each sample expresses the unobserved attributes using a different functional form, which we\r\nwill then represent as a feed-forward neural network with unknown parameters. More precisely, in\r\nthe RUM framework presented in Section 2.1, there are three functions that we wish to approximate.\r\nThe first one, U(\u00b7), can be straightforwardly replaced by a neural network. The two other functions,\r\n\u01eb(\u00b7) and \u03bd(\u00b7), are random and instead of approximating them directly, we propose to use a sample\r\naverage approximation. More specifically, for some large K, we can informally write\r\n\u03c0(x, z, A) \u2248\r\n1\r\nK2\r\n\u00b7\r\nX\r\nK\r\nk1=1\r\nX\r\nK\r\nk2=1\r\n1 {U (x, \u01ebk1\r\n(x), z,\u03bdk2\r\n(z)) > U (x\r\n\u2032\r\n, \u01ebk1\r\n(x\r\n\u2032\r\n), z,\u03bdk2\r\n(z)) , \u2200x\r\n\u2032 \u2208 A \\x} ,\r\n(1)\r\n1 The former assumption is standard and assumes that the observable attributes fully explain any probabilistic\r\ndependence between offered products and customer characteristics. The assumption of uniform distribution can be\r\nenforced without loss of generality under very mild conditions; see McFadden and Train (2000, Lem. 3).\r\n6\r\nwhere, for each k1 \u2208 [K], \u01ebk1\r\n(\u00b7) denotes an independent sample of the random field \u01eb(\u00b7). Analogously,\r\nfor each k2 \u2208 [K], \u03bdk2\r\n(\u00b7) is an independent sample of the random field \u03bd(\u00b7). Since \u01ebk1\r\n(\u00b7) and \u03bdk2\r\n(\u00b7)\r\nare now deterministic functions, we can try to estimate them using neural networks approximately.\r\nIn SAA, the modeler has access to independent samples from the stochastic variable. By contrast,\r\nin our setting, we do not have direct access to such samples; the empirical distribution is obtained\r\nby fitting our neural network architecture to the observed choice data.2\r\nIn the remainder of this section, we formalize the family of choice models resulting from this\r\ncombination of neural networks. We subsequently show in Section 3 that it approximates any RUM\r\ndiscrete choice model arbitrarily closely.\r\nFeed-forward neural networks. We use feed-forward neural networks as building blocks to\r\nconstruct our model architecture, which refers to the combination of these neural networks. For\r\nour purposes, a feed-forward neural network is a function N(\u00b7) : R\r\ndinput \u2192 R\r\ndoutput, where dinput is the\r\nsize of the input and doutput the size of the output. We give a more formal account in Appendix A.\r\nTo obtain generalization guarantees, we assume size-based and norm-based capacity restrictions\r\non such neural networks. Specifically, for every scalar M \u2265 0 and integers \u2113, w \u2265 0, let \u0398\u2113,w\r\nM be the\r\nfamily of feed-forward neural networks N(\u00b7) of depth \u2113 and width w such that M is an upper bound\r\non the \u21131-norm of incoming weights in each node as well as the \u21131-norm of the final layer\u2019s output.\r\nRUMnet architecture. Let d = dx + d\u01eb + dz + d\u03bd be the dimension of the input vector to the\r\nutility function U(\u00b7) that underlies the customer choices in the RUM framework. Additionally,\r\nlet K \u2265 0 be an integer that controls the number of samples we use to approximate the random\r\nfields. With this notation at hand, we introduce a RUMnet architecture, which is a family of neural\r\nnetworks, comprising of the following building blocks:\r\n1. Utility function: There is a feed-forward neural network NU (\u00b7) \u2208 \u0398\r\n\u2113,w\r\nM such that NU (\u00b7) is a\r\nmapping from R\r\nd\r\nto R that serves as an approximation of the function U(\u00b7).\r\n2. Unobserved product attributes: For every k1 \u2208 [K], there is a feed-forward neural network\r\nN\u01ebk1\r\n(\u00b7) \u2208 \u0398\r\n\u2113,w\r\nM such that N\u01ebk1\r\n(\u00b7) is a mapping from R\r\ndx to R\r\nd\u01eb\r\n. Intuitively, each of these neural\r\nnetworks serves as an approximation to one sample of the random field \u01eb(\u00b7).\r\n3. Unobserved customer attributes: For every k2 \u2208 [K], there is a feed-forward neural network\r\nN\u03bdk2\r\n(\u00b7) \u2208 \u0398\r\n\u2113,w\r\nM such that N\u03bdk2\r\n(\u00b7) is a mapping from R\r\ndz to R\r\nd\u03bd\r\n. Intuitively, each of these neural\r\nnetworks serves as an approximation to one sample of the random field \u03bd(\u00b7).\r\n2 Note that a similar interpretation of sample-based approximation was provided for the rank-based choice models (Farias et al. 2013). In particular, one can interpret the SAA of RUMnets as an extension of rank-based choice\r\nmodels to contextual choice data. The non-contextual setting corresponds to the special case where x is a product\r\nindicator and z is fixed.\r\n7\r\nConsequently, we denote by N d\r\n(K,\u0398\r\n\u2113,w\r\nM ) the collection of all RUMnet architectures\r\n(NU (\u00b7), {N\u01ebk1\r\n(\u00b7)}\r\nK\r\nk1=1, {N\u03bdk2\r\n(\u00b7)}\r\nK\r\nk2=1), where d = (dx, d\u01eb\r\n, dz, d\u03bd). The parameters of a given architecture are the number of samples of the SAA as well as the depth and width of each building block.\r\nThe number of samples controls the \u201clatent\u201d (random) effects, whereas the size of each neural\r\nnetwork determines the \u201cdegree of nonlinearity\u201d expressed by the utility function.\r\nRUMnet discrete choice model. We now specify the discrete choice model induced by any\r\ngiven RUMnet architecture N = (NU (\u00b7), {N\u01ebk1\r\n(\u00b7)}\r\nK\r\nk1=1, {N\u03bdk2\r\n(\u00b7)}\r\nK\r\nk2=1) \u2208 N d\r\n(K,\u0398\r\n\u2113,w\r\nM ). For any choice\r\nevent (z, A) \u2208 Z \u00d72\r\nX , we compute the probability \u03c0\r\nRUMnet\r\nN (x, z, A) that a customer with observable\r\nattributes z chooses x \u2208 A as follows:\r\n\u03c0\r\nRUMnet\r\nN (x, z, A) = 1\r\nK2\r\n\u00b7\r\nX\r\nK\r\nk1=1\r\nX\r\nK\r\nk2=1\r\nPr\u03b4\r\nh\r\nNU\r\n\u0010\r\nx, N\u01ebk1\r\n(x), z, N\u03bdk2\r\n(z)\r\n\u0011\r\n+ \u03b4x >\r\nNU\r\n\u0010\r\nx\r\n\u2032\r\n, N\u01ebk1\r\n(x\r\n\u2032\r\n), z, N\u03bdk2\r\n(z)\r\n\u0011\r\n+ \u03b4x\u2032 , \u2200x\r\n\u2032 \u2208 A \\x\r\ni\r\n,\r\nwhere \u03b4 = {\u03b4x}x\u2208A is a sequence of independent real-valued random variables that follow the same\r\ndistribution. Comparing the expression of \u03c0\r\nRUMnet\r\nN (\u00b7) with our sample average approximation (1),\r\nwe have replaced the indicator with a probability distribution over \u03b4. We only require that this\r\ndistribution has a strictly positive density on R.\r\n3 Nevertheless, in our implementation, we will\r\nassume that \u03b4 follows a standard Gumbel distribution, which amounts to replacing the \u201cargmax\u201d\r\nindicator with the \u201csoftmax\u201d operation. One can interpret this as a smoothing operation, made\r\nfor estimation purposes, where we add Gumbel white noise to the utilities.4\r\nIt is important to\r\nnote that any alternative smoothing operator (i.e., adding white noise with closed-form expressions\r\nfor the resulting choice probabilities) could potentially be used. For instance, one could use the\r\nexponomial operator, i.e., adding exponentially distributed white noise (Alptekino\u02d8glu and Semple\r\n2016, 2021), noting that the resulting choice probabilities can be expressed using standard neural\r\nnetwork operations. We chose the softmax because it is widely used and well-optimized for neural\r\nnetwork estimation libraries.\r\n2.3. Interpretation as a neural network\r\nFor any given RUMnet architecture N \u2208 N d\r\n(K,\u0398\r\n\u2113,w\r\nM ), we can interpret the computation of the\r\nprobabilities {\u03c0\r\nRUMnet\r\nN (x, z, A)}x\u2208A associated with a choice event (z, A) as the output of a highly\r\n3 As explained in Section 4.1, in the absence of the latter property, commonly used loss functions (e.g., log-likelihood)\r\nexhibit a gradient of zero with respect to the parameters of the neural network in a set of positive measure.\r\n4 A similar step is made in the RUM approximation result of McFadden and Train (2000, Proof of Thm. 1): Gumbel\r\nnoise is added to smooth the problem and get closed-form expressions for the choice probabilities.\r\n8\r\nstructured neural network. The input to this neural network consists of a vector formed by concatenating the observed product attributes x1, . . . ,x|A| together with the observed customer attributes z. The distribution {\u03c0\r\nRUMnet\r\nN (x, z, A)}x\u2208A is the output of a computation graph comprising\r\nfour \u201cmeta-layers\u201d, which sequentially perform the following:\r\n\u2022 Input vector. Let x1, . . . ,x|A| be an arbitrary numbering of the alternatives in the assortment\r\nA. The input to our neural network is the vector (L|A|\r\ni=1 xi) \u2295 z formed by concatenating the\r\nobserved product attributes x1, . . . ,x|A| together with the observed customer attributes z.\r\n\u2022 Meta-layer 1: Generating the samples of unobserved attributes. In the first layer, for each\r\ni \u2208 {1, . . . , |A|} and k1 \u2208 [K], xi\r\nis passed through N\u01ebk1\r\n(\u00b7) and, for each k2 \u2208 [K], z is passed through\r\nN\u03bdk2\r\n(\u00b7). Here, N\u01ebk1\r\n(xi) can be thought of as the k1-th sample of unobserved product attributes for\r\nthe alternative xi\r\n, while N\u03bdk2\r\n(z) is the k2-th sample of unobserved attributes for customer z. In\r\nwhat follows, we write h\r\nk1,k2\r\ni = xi \u2295 N\u01ebk1\r\n(xi)\u2295z \u2295 N\u03bdk2\r\n(z) \u2208 R\r\nd\r\nfor the intermediate variables, i.e.,\r\nthe ouput of the first meta-layer and input to the second layer.\r\n\u2022 Meta-layer 2: Computing the alternative-specific utilities. For each i \u2208 {1, . . . , |A|} and\r\n(k1, k2) \u2208 [K2\r\n], h\r\nk1,k2\r\ni\r\nis passed through NU (\u00b7). The resulting quantity u\r\nk1,k2\r\ni = NU (h\r\nk1,k2\r\ni\r\n) stands for\r\nthe utility of the choice alternative xi\r\nfor the sample (k1, k2). The output of this second meta-layer\r\nis {\r\nL|A|\r\ni=1 u\r\nk1,k2\r\ni }(k1,k2)\u2208[K2]\r\n.\r\n\u2022 Meta-layer 3: Converting the utilities into probabilities. Next, the utilities are converted into\r\nprobabilities using a softmax layer. More precisely, for each i \u2208 {1, . . . , |A|} and (k1, k2) \u2208 [K2\r\n], we\r\nlet \u03c0\r\nk1,k2\r\ni = e\r\nu\r\nk1,k2\r\ni /(\r\nP|A|\r\nj=1 e\r\nu\r\nk1,k2\r\nj ).\r\n\u2022 Meta-layer 4: Averaging. For each i \u2208 {1, . . . , |A|}, the final output of the neural network is an\r\naverage over all samples, yielding the choice probabilities \u03c0\r\nRUMnet\r\nN (xi\r\n, z, A) = 1\r\nK2 \u00b7\r\nP\r\nk1,k2\r\n\u03c0\r\nk1,k2\r\ni\r\n.\r\nThe design of the first two meta-layers carefully combines several feed-forward neural network\r\nbuilding blocks, as illustrated in Figure 1. The third meta-layer simply consists of a softmax operator. The final meta-layer integrates the choice probabilities over all distinct samples of unobserved\r\nattributes. Figure 2 visualizes the overall architecture, where \u03c0\r\nRUMnet\r\nN (\u00b7) is determined as the output\r\nof a single neural network.\r\n2.4. Connection with related models\r\nBefore we characterize the expressive power of RUMnets, we briefly discuss the connection to\r\nlogit-based choice models, which are special cases of our model family.\r\nFrom MNL to RUMnets. In its simplest form, the MNL model assumes that the utility for a\r\nproduct x \u2208 X is given by\r\nU(x) = \u03b2\r\nT x + \u01ebx, (MNL)\r\n9\r\nInput layer\r\nxi\r\nz\r\nMeta-Layer 1\r\nN\u01ebk1\r\nN\u03bdk2\r\nh\r\nk1,k2\r\ni Meta-Layer 2\r\nNU u\r\nk1,k2\r\ni\r\nFigure 1 Illustration of the first two meta-layers. Note that we show the computation for one particular\r\nproduct attribute xi and sample (k1, k2), but we apply the same transformation for each x \u2208 A and (k1, k2) \u2208 K2\r\n.\r\nInput layer\r\nproduct features\r\nx1\r\n. . .\r\nxn\r\ncustomer features\r\nz\r\nMeta-Layer 1 Meta-Layer 2 Meta-Layer 3 Meta-Layer 4\r\nN\u01ebk1 . . .\r\nN\u01ebk1\r\nK\r\nN\u03bdk2\r\nK\r\nNU\r\nNU. . .\r\nK2\r\nsoftmax\r\nK2\r\naverage\r\nOutput layer\r\n\u03c0\r\nRUMnet\r\nN (x1, z)\r\n\u03c0\r\nRUMnet\r\nN (xn, z) . . .\r\nchoice probabilities\r\nFigure 2 A RUMnet architecture N = (NU (\u00b7), {N\u01ebk1\r\n(\u00b7)}\r\nK\r\nk1=1, {N\u03bdk2\r\n(\u00b7)}\r\nK\r\nk2=1).\r\nwhere \u03b2 is a vector of parameters and {\u01ebx}x\u2208A is an i.i.d. sequence of standard Gumbel shocks.\r\nNote that under this linear specification of the utility, the MNL model cannot leverage the customer\r\nattributes z. To overcome this limitation, researchers often manually specify some non-linearity by\r\nintroducing cross-terms in the utility function. A more recent approach to capture taste heterogeneity across individuals, proposed by Han et al. (2020), is to specify the utility as follows:\r\nU(x, z) = \u03b2\r\nT x + N\r\nTasteNet(z)\r\nT x + \u01ebx, (TasteNet)\r\nwhere NTasteNet(\u00b7) : R\r\ndz \u2192 R\r\ndx is a feed-forward neural network. Note that TasteNet can express\r\nnon-linear transformations NTasteNet(z) of the customer attributes z. However, the additive term\r\n10\r\nNTasteNet(\u00b7)\r\nT x still assumes a very specific form for how the customer attributes interact with the\r\nproduct attributes. In fact, nothing prevents us from allowing the utility to be a general function\r\nof x and z, just as in the RUM framework. Perhaps the most natural approach is to take TasteNet\r\na step further and define DeepMNL as a RUM discrete choice model where the utility is given by:\r\nU(x, z) = N\r\nDeepMNL(x, z) + \u01ebx, (DeepMNL)\r\nwhere NDeepMNL(\u00b7) : R\r\ndx+dz \u2192 R is an arbitrary feed-forward neural network. Here, DeepMNL can\r\nbe viewed as a special case of the RUMnet architecture in which there are no unobserved attributes\r\n(K = 0); several variants of DeepMNL have been proposed in the literature (Bentz and Merunka\r\n2000, Sifringer et al. 2020, Wang et al. 2020). With respect to Figure 2, this approach consists in\r\ndropping the first meta-layer, and only keeping the second and third meta-layers. Since there is no\r\nlatent heterogeneity, there is no need for the fourth (averaging) meta-layer either.\r\nThe above sequence of choice models illustrates a gradual increase of complexity and nonlinearity in the deterministic portion of the utility function, which can be conveniently expressed\r\nand estimated using neural networks. As noted by Train (2009), if the researcher could specify\r\nNDeepMNL(x, z) \u201csufficiently that the remaining, unobserved portion of utility is essentially white\r\nnoise\u201d, then the DeepMNL family would be ideal. However, regardless of how complex the deterministic portion of the utility is, DeepMNL (and the above special cases) are subject to the same\r\nrestrictions as the MNL choice model. These approaches do not control for latent (unobserved)\r\nfactors that may influence the utility, which are captured by various probabilistic structures in the\r\nprevious literature on discrete choice modeling. Contrary to DeepMNL, our RUMnet architecture\r\ncaptures latent heterogeneity by incorporating unobserved product and customer attributes. The\r\nempirical (sample-based) distribution for these unobserved attributes endows RUMnets with nearly\r\nuniversal expressive power, as established in the next section.\r\nConnection to latent class MNL. We conclude this section by noting that each RUMnet model\r\n\u03c0\r\nRUMnet\r\nN can be viewed as an instance of the latent class MNL model (also known as a discrete\r\nmixture of MNL models) with nonlinear utility effects. However, since we designed the neural\r\nnetwork architecture to mimic the RUM principle, our family of based choice models imposes a\r\nspecific parametrization of the random utilities. Compared with standard implementations of the\r\nlatent class MNL model, customer segments share a unique utility function, but they differ in\r\nthe unobserved attributes, which are random inputs to the utility function. From an estimation\r\nperspective, the fact that there is a unique utility function induces weight-sharing across customer\r\nsegments, which is an important distinctive property relative to standard formulations of latent\r\nclass MNL models.\r\n11\r\n3. Expressive Power of RUMnets\r\nIn this section, we show that RUMnets tightly describe the class of all RUM discrete choice models.\r\nIn particular, we show that any RUM discrete choice model can be approximated arbitrarily closely\r\nby a RUMnet.\r\nProposition 1. For every RUM discrete choice model \u03c0(\u00b7) of dimension d and for every \u03b7 > 0,\r\nthere exists a RUMnet architecture N \u2208 N d\r\n(K,\u0398\r\n\u2113,w\r\nM ) such that, for all choice events (z, A) \u2208 Z \u00d72\r\nX ,\r\nmaxx\u2208A\r\n\f\r\n\f\u03c0(x, z, A) \u2212 \u03c0\r\nRUMnet\r\nN (x, z, A)\r\n\f\r\n\f \u2264 \u03b7 .\r\nTo prove Proposition 1, we revisit the celebrated result of McFadden and Train (2000, Theorem 1) showing that mixed MNL models, i.e., continuous mixtures of multinomial logit models,\r\nuniformly approximate the class of RUM discrete choice models. Quoting McFadden and Train\r\n(2000), \u201cone limitation of Theorem 1 [in that paper] is that it provides no practical indication of\r\nhow to choose parsimonious mixing families, or how many terms are needed to obtain acceptable\r\napproximations to \u03c0(x, z, A)\u201d. Proposition 1 shows the following property: the choice probabilities\r\nof RUM discrete choice models are uniformly approximated by finite mixtures of MNL models\r\nsuch as RUMnets on any choice event in the continuous domain. Building on this result, we will\r\nestablish in Proposition 4 that there exist accurate data-dependent approximations according to\r\nthe KL-divergence with a relatively small number of samples K.\r\nFor this purpose, our proof in Appendix B extends the ideas in McFadden and Train (2000)\r\nby combining a refined covering lemma with concentration bounds. At a high level, the proof\r\nconsists in showing that when we approximate the random utility function using feed-forward\r\nneural networks, the choice probabilities do not change much. Specifically, we analyze the likelihood\r\nof a \u201cpreference reversal\u201d for every pair of alternatives (x, z) and (x\r\n\u2032\r\n, z) with x 6= x\r\n\u2032 by controlling\r\nthe variations of the utility function. Key to this analysis is the existence of a finite covering of\r\nX\r\n2 \u00d7 Z on which we can bound the errors incurred by our SAA approximation. Next, we establish\r\nthat the reciprocal of Proposition 1 holds as well.\r\nProposition 2. For every RUMnet architecture N \u2208 N d\r\n(K,\u0398\r\n\u2113,w\r\nM ) and for every \u03b7 > 0, there\r\nexists a RUM discrete choice model \u03c0(\u00b7) of dimension d such that, for all choice events (z, A) \u2208\r\nZ \u00d7 2\r\nX , maxx\u2208A\r\n\f\r\n\f\u03c0\r\nRUMnet\r\nN (x, z, A) \u2212 \u03c0(x, z, A)\r\n\f\r\n\f \u2264 \u03b7 .\r\nThe proof of Proposition 2 is straightforward: following the interpretation of our neural network\r\narchitecture given in Section 2.4, RUMnet architectures directly represent the random choices of\r\na utility-maximizing agent, while a small random perturbation of the RUMnet ensures that the\r\nregularity conditions of McFadden and Train (2000) are met.\r\n12\r\n4. Dealing with the Curse of Dimensionality\r\nGiven the expressive power of RUMnets demonstrated by Proposition 1, one important concern is\r\nthe risk of overfitting, which could affect the model\u2019s ability to generalize to new, previously unseen\r\ndata. Hence, in this section, we provide theoretical guarantees on the out-of-sample error of the\r\nfitted RUMnets. More specifically, we provide an upper bound on the generalization error, which\r\nmeasures the degree of overfitting, i.e., it characterizes how the (out-of-sample) expected risk is\r\nrelated to the minimum (in-sample) empirical risk attained by our hypothesis class. We exploit\r\nthis generalization error bound to establish a compact representation property, showing that any\r\nRUMnet architecture can be accurately represented using a relatively \u201csmall\u201d number of samples.\r\n4.1. Estimation framework\r\nWe formulate the estimation of the neural network of Section 2.3 in the standard Empirical Risk Minimization (ERM) framework; e.g., see Shalev-Shwartz and Ben-David (2014,\r\nChap. 2). We assume that our data set is given by a sample of T i.i.d. observations S =\r\n{(y1, z1, A1), . . .,(yT , zT , AT )}, where (zt\r\n, At) is the t-th choice event and yt \u2208 At\r\nis the product\r\npicked by the corresponding customer. To ease the exposition, we further assume that the assortments have a uniform cardinality |At\r\n| = \u03ba for all t \u2208 [T]. Notation-wise, D stands for the marginal\r\ndistribution of each observation in the sample, i.e., S \u223c DT\r\n. By a slight abuse of notation, we\r\nsometimes use (zt\r\n, At) \u223c D (instead of (yt\r\n, zt\r\n, At) \u223c D) to indicate that the choice event is generated according to D. We do not impose the so-known realizability assumption, meaning that our\r\nmodeling approach can be misspecified, i.e., we do not assume that D is described by some RUM\r\ndiscrete choice model.\r\nWe proceed by formulating our estimation criterion. As an input to our estimation procedure,\r\nwe specify the parameters d,K, w, \u2113,M \u2265 0 so that we restrict our estimation procedure to the\r\nhypothesis class N d\r\n(K,\u0398\r\n\u2113,w\r\nM ) of RUMnet architectures. To fully specify this class of neural networks,\r\nthe activation functions are chosen as ReLUs (Rectified Linear Units), which are in popular use.\r\nAdditionally, we assume that all customer and product attributes are pre-normalized to lie in\r\nthe range [\u22121, 1]. Our estimator is based on the ERM principle with respect to the negative loglikelihood loss function. Specifically, given a sample S and a discrete choice model \u03c0(\u00b7), we let\r\nLS(\u03c0) be the empirical negative log-likelihood, namely\r\nLS (\u03c0) = \u2212\r\n1\r\nT\r\n\u00b7\r\nX\r\nT\r\nt=1\r\nlog (\u03c0 (yt\r\n, zt\r\n, At)) . (2)\r\nWith this definition, our approach is to choose the RUMnet architecture that minimizes the above\r\nempirical loss over all RUMnet architectures. Formally, NERM\r\nS = arg minN\u2208Nd(K,\u0398\r\n\u2113,w\r\nM )\r\nLS(\u03c0N ). Note\r\n13\r\nthat the fitted RUMnet architecture NERM\r\nS\r\nis a function of the observed (random) sample S. Additionally, let \u03c0\r\nERM\r\nS\r\n(\u00b7) be the associated discrete choice model, i.e., \u03c0\r\nERM\r\nS\r\n(\u00b7) = \u03c0\r\nRUMnet\r\nNERM\r\nS\r\n(\u00b7). It is worth\r\nhighlighting that the ERM principle is equally applicable to other loss functions such as meansquared error or accuracy. Nonetheless, our analysis will focus on log-likelihood-based estimation,\r\nin accordance with the fitting procedure used for other classes of choice models. From a practical\r\nperspective, estimating a RUMnet discrete choice model using the ERM principle reduces to training a neural network for multi-label classification. The log-likelihood loss function is often referred\r\nto as the cross-entropy loss in the ML practice. Thus, the ERM rule can be implemented using\r\ngraph optimization and automatic differentiation tools such as Keras (Chollet et al. 2015).\r\n4.2. Learning error guarantees and compact representation\r\nWe define the true error as the expected out-of-sample loss, where the expectation is taken\r\nover the unknown distribution D. Specifically, for any discrete choice model \u03c0(\u00b7), let L\r\ntrue\r\nD (\u03c0) =\r\nES\u2032\u223cDT [LS\u2032(\u03c0)]. Note that this is ideally what we want to minimize. However, since we cannot\r\ndirectly measure out-of-sample performance, as the distribution is unknown, we minimize the empirical error defined in Equation (2). The next claim quantifies the gap between these errors.\r\nProposition 3. With probability at least (1 \u2212 \u03b4), the sampled training set S satisfies\r\nL\r\ntrue\r\nD\r\n\r\n\u03c0\r\nERM\r\nS\r\n\u0001\r\n\u2264 LS\r\n\r\n\u03c0\r\nERM\r\nS\r\n\u0001\r\n+ c1 \u00b7\r\nr\r\n\u03ba\r\n3\r\nlog(d)\r\nT\r\n\u00b7 e\r\n2M(2M)\r\n\u2113 + (8M + 4 log \u03ba)\u00b7\r\nr\r\n2 \u00b7 ln(4/\u03b4)\r\nT\r\n.\r\nThe proof is presented in Appendix C.1, and it follows from standard notions of Rademacher\r\ncalculus applied to the RUMnet architecture. As expected for neural networks with per-unit \u21131-\r\nbounds, the error bound of Proposition 3 indicates an exponential dependence on depth \u2113 of the\r\nneural network. Moreover, we expect a sample complexity of \u2126(1/pmin) = \u2126(\u03bae2M) to estimate\r\nchoice probabilities that can be as small as pmin =\r\n1\r\n\u03ba\r\n\u00b7 e\r\n\u22122M (see Claim EC.3 in Appendix C.1).\r\nYet, our bound on the generalization error reveals two interesting properties. First, the bound\r\ndoes not depend on the number of samples K. In other words, the generalization error bound of\r\nProposition 3 does not degrade with the added complexity from increasing the sample size K, contrary to other dimensions of the RUMnet architecture. Second, it is worth noticing a subquadratic\r\ndependence O(\u03ba\r\n3\r\n2 T\r\n\u2212 1\r\n2 ) on the number of choice alternatives \u03ba, which is better than an existing\r\nbound of \u2126(\u03ba\r\n\u2113/2T\r\n\u2212 1\r\n2 ) by Wang et al. (2021b). For another comparison point, choice modeling can\r\nbe viewed as a structured form of classification problem. In this context, classical Rademacher\r\ncomplexity bounds on margin-based learning for multi-label classification have a quadratic scaling\r\nO(\u03ba\r\n2T\r\n\u2212 1\r\n2 ) (Koltchinskii and Panchenko 2000, Cortes et al. 2013, Mohri et al. 2018), unless further\r\nrestrictions are imposed on the hypothesis class or a more complex analysis is use\r\n14\r\nBuilding on Proposition 3, we next explore the number of samples K required in the RUMnet\r\narchitecture to accurately describe any RUMnet choice model for a fixed data distribution. Our\r\nnext claim shows that, with a small O(\u01eb) loss in accuracy in terms of expected KL-divergence,\r\nthe number of samples K can be chosen in the order of O(\r\n1\r\n\u01eb\r\n2 poly(log 1\r\n\u01eb\r\n, \u2113, \u03ba, eM)). This finding\r\nsuggests that the latent heterogeneity of the model should be commensurate to the complexity of\r\nthe feed-forward neural network building blocks of our architecture.\r\nProposition 4. For every neural network architecture N \u2208 N d\r\n(K,\u0398\r\n\u2113,w\r\nM ), let N\u2032 \u2208 N d\r\n(K\u2032\r\n,\u0398\r\n\u2113,w\r\nM )\r\nbe the random neural network architecture obtained from N by taking K\u2032 = \u2308\r\n1\r\n2\u01eb\r\n2 \u00b7 log \u03b4 \u00b7\r\n(\u03bae2M)\r\n2\r\nlog(\u2308max{\r\nc1\r\n\u01eb\r\n2 \u03ba\r\n3\r\nlog(d)e\r\n4M(2M)\r\n2\u2113\r\n, 128(8M + log \u03ba)\r\n2\r\nln 4\r\n\u03b4\r\n}\u2309)\u2309 samples of the unobserved attributes {N\u01ebk1\r\n(\u00b7)}\r\nK\r\nk1=1 and {N\u03bdk2\r\n(\u00b7)}\r\nK\r\nk2=1 uniformly at random with replacement. With probability\r\nat least 1 \u2212 \u03b4, we have\r\nE(z,A)\u223cD\r\nKL\r\n\u03c0\r\nRUMnet\r\nN (\u00b7, z, A)\r\n\f\r\n\f\u03c0\r\nRUMnet\r\nN\u2032 (\u00b7, z, A)\r\n\u0001 \u2264 3\u01eb .\r\nThis claim can be viewed as the counterpart of Theorem 4 in the paper by Chierichetti et al. (2018)\r\nfor feature-dependent choice models in the continuous domain, rather than the discrete domain.\r\nThe proof appears in Appendix C.2 and combines Proposition 3 with concentration bounds.\r\n5. Numerical Estimation on Synthetic Data\r\nIn this section, we present numerical experiments on synthetic data. The objective is to gain\r\ninsights into the expressive capabilities of RUMnets by successively varying the ground truth model\r\nthat generates the synthetic choice data. In particular, we show that the proposed model and\r\nestimation method can infer non-linearity in the utility function as well as the presence of customer\r\nheterogeneity. The setup is purposely as simple as possible, and, in particular, it does not include\r\ncustomer attributes.\r\n5.1. Experiment setup\r\nWe begin by describing three generative processes to construct synthetic datasets. For each setting,\r\nwe consider a sequence of T = 10, 000 customers, each being presented with an assortment At\r\nof \u03ba = 10 products chosen uniformly at random from a universe of 50 products. Each product\r\nis endowed with a vector of attributes x = (x1, x2, \u03b4) where x1 and x2 are picked uniformly over\r\nthe interval [0, 1] and \u03b4 \u2208 R\r\n50 is an indicator vector allowing us to introduce fixed effects for each\r\nproduct. More precisely, \u03b4i = 1 if i is the index of the chosen product and 0 otherwise. In each\r\nsetting, we assume that customers choose according to a random utility model. However, the utility\r\nspecification differs in each setting. We use this ground truth model to generate a choice event\r\nyt \u2208 At\r\n, which corresponds to the product picked by the corresponding customer. Using thes\r\n15\r\ngenerated observations S = {(y1, A1), . . .,(yT , AT )}, we use the framework described in Section\r\n4.1 to fit various RUMnet models. In particular, we experiment with (\u2113, w) \u2208 {(0, 0),(1, 3),(2, 5)},\r\nwhere recall that \u2113 denotes the depth of the network and w its width. For our RUMnet architecture,\r\nwe also test out different number of samples K \u2208 {2, 5} controlling for the latent heterogeneity of\r\ncustomers and products. The DeepMNL model serves as a benchmark since it does not capture any\r\nlatent heterogeneity (see Section 2.4). For each setting, we use 20% of the data as a validation set\r\nfor early stopping (see Appendix E.1 for additional details on the implementation). Finally, we also\r\ngenerate a sequence of 1, 000 customers that we use as a testing set. We report the log-likelihood\r\nloss on the test set averaged over ten different instances.\r\nSetting 1: MNL model. The ground truth model is simply an MNL model. In particular, for\r\nevery vector of product attributes x, the utility is given by\r\nU\r\n1\r\n(x) = \u03b2\r\nT x + \u01eb,\r\nwhere the entries of \u03b2 are picked uniformly at random over the interval [\u22121, 1] and \u01eb is a standard\r\nGumbel shock, which is sampled independently across product and customers.\r\nSetting 2: nonlinear utility. Here, we assume that the ground truth model is described by a\r\nnon-linear utility function. In particular, for every vector of product attributes x, the utility is\r\ngiven by\r\nU\r\n2\r\n(x) = \u03b21 \u00b7 x1 + \u03b22 \u00b7 x2 + \u03b23 \u00b7 x\r\n2\r\n1 + \u03b24 \u00b7 x1 \u00b7 x2 + \u03b25 \u00b7 x\r\n2\r\n2 + \u03b3\r\nT\r\n\u03b4 + \u01eb,\r\nwhere each entry of \u03b2 and \u03b3 is picked uniformly over the interval [\u22121, 1] and \u01eb is a standard\r\nGumbel shock, which is sampled independently across products and customers. Additionally, recall\r\nthat \u03b4i = 1 if i is the index of the chosen product and 0 otherwise. To accentuate the non-linearity\r\neffects, for this setting only, we sample x1 and x2 uniformly at random over the interval [0, 10].\r\nSetting 3: heterogeneity. Our last ground truth model exhibits customer heterogeneity. Specifically, for every vector of product attributes x, the utility is given by\r\nU\r\n3\r\n(x) = b \u00b7\u03b2\r\nT x + (1 \u2212 b)\u00b7 \u03b3\r\nT x + \u01eb,\r\nwhere the coordinates of \u03b2 and \u03b3 are picked uniformly over the interval [\u2212100, 100], \u01eb is a standard\r\nGumbel shock, which is sampled independently across product and customers, and b is a Bernoulli\r\nrandom variable with probability of success Pr[b = 1] = 0.3. Note that this model is a latent class\r\nMNL model with two customer segments; we increase the scale of the parameters \u03b2 and \u03b3 to\r\naccentuate the violation of the IIA property.\r\n16\r\n2\r\n2.1\r\n2.2\r\n2.3\r\n(0,0) (1,3) (2,5)\r\n(\u2113, w)\r\nLog-likelihood loss\r\n(a) Setting 1: MNL model\r\n0.1\r\n0.2\r\n0.3\r\n(0,0) (1,3) (2,5)\r\n(\u2113, w)\r\nLog-likelihood loss\r\nDeep MNL\r\nRUMnet (K = 2)\r\nRUMnet (K = 5)\r\nGround truth\r\n(b) Setting 2: Non-linearity\r\n0.6\r\n0.8\r\n1\r\n1.2\r\n(0,0) (1,3) (2,5)\r\n(\u2113, w)\r\nLog-likelihood loss\r\n(c) Setting 3: Heterogeneity\r\nFigure 3 Average out-of-sample log-likelihood loss for different settings and models.\r\n5.2. Results\r\nIn Setting 1, the dataset is generated using an MNL model. In this case, the utility function is\r\nlinear and does not exhibit any heterogeneity. As a result, as illustrated in Figure 3a, the average\r\nlog-likelihood loss is insensitive to the complexity of the feed-forward neural network (\u2113, w) as well\r\nas the number of samples K. In Setting 2, we construct a non-linear utility function to generate\r\nthe synthetic dataset. As can be seen in Figure 3b, RUMnet is able to capture this non-linearity by\r\nincreasing the complexity of the the feed-forward neural network. In particular, as (\u2113, w) \u201cincreases\u201d,\r\nthe out-of-sample log-likelihood of RUMnet improves and it approaches that of the ground truth\r\nmodel, which is represented by the orange line. In this case, note that increasing K does not add\r\nmuch value. This is consistent with the fact that our ground truth model does not exhibit any\r\nform of customer heterogeneity. Unlike Setting 2, recall that Setting 3 has two customer classes\r\neach choosing according to a distinct MNL model. In this case, as illustrated in Figure 3c, the\r\nperformance of RUMnet improves with K but it is mostly insensitive to the complexity (\u2113, w) of the\r\nfeed-forward neural networks. Consequently, our synthetic experiments validate the value added of\r\neach component of our RUMnet architecture.\r\n5.3. Additional setting: non-contextual choice data\r\nRanking-based models are known to approximate random utility choice models in non-contextual\r\nsettings (Farias et al. 2013). In this setting, the observable product attributes x are a one-hot\r\nencoding of a finite universe of products. There are no observable customer attributes z (or equivalently, these are constant). While we are not aware of any method to estimate ranking-based\r\nmodels in contextual settings, we can easily implement RUMnets on non-contextual choice data.\r\nWe explore this approach in Appendix D.1 on synthetic data. We observe that RUMnets are competitive against an existing implementation of ranking-based models, both in terms of predictive\r\naccuracy and running times.\r\n17\r\n6. Numerical Estimation on Real Data\r\nWe test the predictive accuracy of RUMnets against a comprehensive set of benchmarks on two\r\nreal-world datasets, which reflect complementary choice prediction settings.\r\n6.1. Benchmarks\r\nWe implement various choice models and classification algorithms proposed in previous literature.\r\nMNL and neural network-based extensions. The first set of benchmarks comprises the MNL\r\nmodel and the two neural network-based extensions, TasteNet and DeepMNL, which we presented\r\nin Section 2.4. While the MNL model specifies a linear utility function, the subsequent models capture nonlinear effects of increasing complexity, and can be viewed as state-of-the-art benchmarks.\r\nFrom an implementation perspective, we formulate each of these models as a neural network with\r\na softmax prediction layer.\r\nModel-free approaches. In addition, we test two model-free benchmarks that treat the choice\r\nprediction task as a multi-label classification problem and forego any additional structure on the\r\nprobabilistic choices:\r\n1. Random Forest (RF): We train a variant of the random forests for choice prediction which\r\nwas very recently introduced by Chen et al. (2019) and Chen and Mi\u02c7si\u00b4c (2020). Importantly, these\r\npapers focus on a different observational setting, where the assortment composition varies but each\r\nproduct\u2019s attributes are essentially fixed. Due to contextual variations in product and customer attributes, we implement a featurized version of this methodology, which was suggested by Chen et al.\r\n(2019). We note that the optimization-based estimation methods developed by Chen and Mi\u02c7si\u00b4c\r\n(2020) are not easily applicable in this setting.\r\n2. Vanilla Neural Network (VNN): The input to this feed-forward neural network is the concatenation of all product and customer attributes. The output is a vector of length |A| that represents\r\nthe utility of each choice alternative, which is then passed through a softmax layer to compute the\r\ncorresponding choice probabilities.\r\n6.2. Implementation specifications\r\nExcept for the RF model, we implement all other benchmarks using Keras. Appendix E.1 details our\r\napproach for estimating the neural networks-based models. We use label smoothing as a norm-based\r\nregularization method on the neural network\u2019s outputs and specify Exponential Linear Unit (ELU)\r\nactivation functions (Clevert et al. 2015). For each model, we conduct an extensive parameter\r\nsearch in the training process. We use the validation set to tune the hyper-parameters on each\r\ndata split. We test different complexities for the building-block neural networks. In particular, for\r\neach neural network-based model, we vary the parameters (\u2113, w) \u2208 {(3, 10),(5, 20),(10, 30)}, where\r\n18\r\nTable 1 Out-of-sample predictive performance of the fitted choice models\r\nSwissmetro Expedia\r\nModel Log-likelihood loss Accuracy Log-likelihood loss Accuracy\r\nMNL 0.830\r\n(0.5 \u00b7 10\u22122)\r\n0.623\r\n(3.6 \u00b7 10\u22123)\r\n2.563\r\n(1.3 \u00b7 10\u22123)\r\n0.306\r\n(6.8 \u00b7 10\u22124)\r\nTasteNet 0.554\r\n(1.3 \u00b7 10\u22122)\r\n0.785\r\n(3.5 \u00b7 10\u22123)\r\n2.112\r\n(1.8 \u00b7 10\u22123)\r\n0.406\r\n(1.0 \u00b7 10\u22123)\r\nDeepMNL 0.560\r\n(1.5 \u00b7 10\u22122)\r\n0.778\r\n(5.2 \u00b7 10\u22123)\r\n2.049\r\n(2.5 \u00b7 10\u22123)\r\n0.417\r\n(1.0 \u00b7 10\u22123)\r\nRUMnet 0.546\r\n(1.5 \u00b7 10\u22122)\r\n0.797\r\n(5.4 \u00b7 10\u22123)\r\n2.018\r\n(2.1 \u00b7 10\u22123)\r\n0.425\r\n(5.6 \u00b7 10\u22124)\r\nVanilla Neural Network 0.584\r\n(1.2 \u00b7 10\u22122)\r\n0.756\r\n(4.3 \u00b7 10\u22123)\r\n2.677\r\n(4.6 \u00b7 10\u22123)\r\n0.312\r\n(1.2 \u00b7 10\u22123)\r\nRandom Forest 0.527\r\n(0.6 \u00b7 10\u22122)\r\n0.774\r\n(4.7 \u00b7 10\u22123)\r\n2.934\r\n(1.8 \u00b7 10\u22123)\r\n0.309\r\n(7.4 \u00b7 10\u22124)\r\nrecall that \u2113 denotes the depth of the network and w its width.5 For our RUMnet architecture, we\r\nalso vary number of samples K \u2208 {5, 10} controlling for the latent heterogeneity of customers and\r\nproducts. We report the performance of the DeepMNL model of Section 2.4, which is essentially a\r\nRUMnet model that does not capture any latent heterogeneity, i.e., K = 1. For RFs, we jointly vary\r\nthe number of trees in the forest in {50, 100, 200, 400} and the maximum tree depth in {5, 10, 20};\r\nwe then pick the forest with the best loss on the validation set.\r\nWe use 10-fold cross-validation and report the averaged log-likelihood loss and classification over\r\nthe ten splits. More precisely, we use a 80/10/10 split for the train/validation/test sets respectively.\r\nImportantly, the test set is never seen during training, which enables us to assess the out-of-sample\r\nperformance of the models. In addition to the log-likelihood loss defined in Equation (2), we also\r\nreport the accuracy, defined as the percentage of correct predictions for the chosen alternative.\r\n6.3. Results\r\nThe first dataset (Swissmetro data) consists of responses to a survey in Switzerland to assess the\r\npotential demand for a new mode of public transportation, called Swissmetro. The alternatives\r\ninclude Swissmetro, Train or Car (\u03ba = 3), and the number of observations in the data is in the\r\norder of 10K. The second one (Expedia data) is a larger dataset. This dataset counts around 400K\r\ndistinct search queries, 36 hotel features, and 56 customer and search features. This setting mirrors\r\nlarge-scale transaction data with assortments formed by many distinct alternatives (\u03ba = 39). More\r\ndetails on each dataset are presented in Appendices E.2 and E.3.\r\nThe predictive performance of the models is reported in Table 1. RUMnet emerges as the most\r\npredictive method out of the benchmarks in terms of accuracy. All the tested models significantly\r\n5 We omit certain instantiations of (\u2113, w) if the corresponding running times are prohibitive, or if the number of\r\nparameters becomes excessive.\r\n19\r\noutperform the basic MNL benchmark in terms of log-likelihood loss and prediction accuracy.\r\nThe gaps in the log-likelihoods exceed 20% on both datasets, thereby indicating that a linear\r\nutility-based model is too restrictive to predict choices in these settings accurately. Now, comparing\r\nTasteNets and DeepMNLs to RUMnets, we can attribute the gain in predictive performance to\r\nthe incorporation of latent heterogeneity. Table EC.5 in Appendix E.4 provides a more detailed\r\nperspective on the effects of the architecture design. Considering TasteNets and DeepMNLs, we\r\nsee that increasing the complexity of the neural networks from (\u2113, w) = (3, 10) to (\u2113, w) = (5, 20)\r\nimproves predictive performance; this phenomenon can be imputed to a more complex nonlinear\r\nutility function. However, there are no marginal gains from fitting even larger neural networks\r\n(\u2113, w) = (10, 30). These observations suggest that it is impossible to \u201cmake up\u201d for the lack of\r\nlatent heterogeneity using a more complex utility function. This aligns with our controlled synthetic\r\nexperiments in Section 5, where we varied the parametric and probabilistic specification of the\r\nground truth utility. The differences in predictive performance between RUMnets and DeepMNL\r\nare statistically significant only on Expedia data. In terms of log-likelihood loss, the paired t-tests\r\nwith respect to the 10 splits have p-values of 5.56 \u00b7 10\u22122 on Swissmetro data and 1.11 \u00b7 10\u22128 on\r\nExpedia data.\r\nThe model-free methods, RF and VNN, achieve strong predictive performance on the Swissmetro\r\ndata. In fact, RFs achieve lower negative log-likelihoods than RUMnets in that setting (although\r\nthe gap is not statistically significant with a paired t-test p-value of 0.134). In stark contrast, both\r\nmethods perform poorly on the Expedia dataset. We note that RFs suffer from a high level of\r\noverfitting, as indicated by the gap in performance on our training and test data (see Table EC.5,\r\nAppendix E.4).6 This noteworthy phenomenon might be related to the large number of distinct\r\nalternatives and the assortment size of Expedia data. Indeed, for such model-free methods, an\r\nexplosion of the number of parameters seems unavoidable as the input dimension increases (see\r\nTable EC.3, Appendix E.1). This potential explanation is supported by controlled synthetic experiments, which we present in the next section.\r\n6.4. Model-free methods on high-dimensional synthetic data\r\nAlthough the predictive performance of the random forest (RF) approach is comparable to that of\r\nRUMnet on the Swissmetro dataset, this method has a poor performance on the Expedia dataset\r\nwith a high level of overfitting (see Section 6.3). To explain this phenomenon, we hypothesize\r\nthat the performance of RFs degrades as the dimension of the underlying prediction problem\r\nincreases, i.e., the number of product and customer attributes and/or the number of distinct choice\r\n6 Note that the hyper-parameters of RFs are tuned using the validation set: our estimation method enables selecting\r\nsmaller tree depths or fewer trees in the forest. Yet, these choices do not result in better out-of-sample performance.\r\n20\r\n0 20 40 60 80 100\r\n1.5\r\n1.52\r\n1.54\r\n1.56\r\n1.58\r\n1.6\r\nNumber of products in the universe\r\nLog-likelihood loss\r\nRUMnet\r\nRandom forest\r\nRandom guess\r\nGround Truth\r\nFigure 4 Log-likelihood loss on the test set as a function of the number of products. RUMnet has parameters\r\n(\u2113, w) = (2, 5) and K = 5. Note that the y-axis log-likelihood losses vary from 1.48 to 1.61.\r\nalternatives which together determine the dimension of the RF inputs. In this section, we conduct\r\nsynthetic experiments that support this hypothesis.\r\nIn particular, we re-use Setting 1 of our synthetic experiments, described in Section 5. Recall\r\nthat in this setting, we generate observations using an MNL ground truth model. Each customer is\r\npresented with \u03ba products chosen uniformly randomly from a universe of P products. We fix \u03ba = 5\r\nand experiment with P \u2208 {5, 10, 25, 50, 100}. Each product is endowed with a vector of attributes\r\nx = (x1, x2, \u03b4) where x1 and x2 are picked uniformly over the interval [0, 1] and \u03b4 \u2208 R\r\nP\r\nis an\r\nindicator vector allowing us to introduce fixed effects for each product. Here, note that the input\r\nsize grows with the number of products in the universe P allowing to test how the performance of\r\nthe random forest scales with P. Moreover, for the random forest method, we vary the number of\r\ntrees in the forest in {200, 300, 400} and the maximum tree depth in {5, 10, 20}; we then pick the\r\nforest with the best loss on the validation set.\r\nFigure 4 shows the average log-likelihood loss on the test set averaged over ten splits . We also\r\nreport on this figure the average log-likelihood loss of the RUMnet model with parameters (\u2113, w) =\r\n(2, 5) and K = 5, as well as the average log-likelihood loss of a naive model, termed RandomGuess,\r\nthat prescribes a uniform choice probability distribution over the \u03ba offered products. We observe\r\nthat as P increases, the performance of RFs diverges from that of the ground truth model and\r\nnearly matches that of RandomGuess. This shows that when the input dimension is large, the\r\nrandom forest approach is unable to learn generalizable patterns from the data. On the other\r\nhand, the predictive performance of the RUMnet model is quite close to the ground truth model.\r\nAlthough this is expected because the ground truth model is in the RUM family, the performance\r\nof RUMnet is insensitive to the value of the number of products P, as P increases. This highlights\r\n21\r\n0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n1\r\n0.1 0.2 0.3\r\nCost of Swissmetro\r\nChoice probability\r\nTrain\r\nSwissMetro\r\nCar\r\n(a) Customer type 1\r\n0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n1\r\n0.1 0.2 0.3\r\nCost of Swissmetro\r\nChoice probability\r\nTrain\r\nSwissMetro\r\nCar\r\n(b) Customer type 2\r\n0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n1\r\n0.1 0.2 0.3\r\nCost of Swissmetro\r\nChoice probability\r\nTrain\r\nSwissMetro\r\nCar\r\n(c) Customer type 3\r\nFigure 5 Different substitution patterns predicted by DeepMNL with parameters (\u2113, w) = (2, 5).\r\nthat our method scales well with respect to the number of attributes and distinct products in the\r\nuniverse in contrast with model-free methods.\r\n6.5. Visualization\r\nNeural networks are often viewed as \u201cblack-box\u201d tools that are hard to interpret. Hence, it is\r\nimportant to explore what type of substitution patterns are captured by the fitted RUMnets and\r\ncontrast them with other methods. As a first step, we visualize how the RUMnet predictions vary\r\nas a function of the price, which is arguably one of the most important operational dimension, using\r\nthe Swissmetro dataset. In order to visualize how RUMnets predict the customers\u2019 substitution\r\npatterns, we use the K-means method to determine a balanced partition of the customer features\r\nand subsequently define the centroid of each cluster as a distinct customer type. Figure 5 plots\r\nthe predicted choice probabilities of each customer types as a function of the Swissmetro cost.\r\nOur clustering reveals three distinct choice behaviors: Customer 1 is mostly price insensitive and\r\nchooses the Swissmetro with high probability even when increasing its cost. Both Customers 2 and\r\n3 are price sensitive and choose the Swissmetro option when its cost is low enough. Interestingly,\r\nthey differ in what they substitute to when the cost of the Swissmetro is too high: Customer 2\r\nchooses the Train option whereas Customer 3 chooses the Car option.\r\nFor all customer types, the choice probability of the Swissmetro decreases with its cost. This\r\nshows that RUMnets capture realistic substitution patterns with respect to price. Note that in\r\npractice, these relationships may or may not be observed due to a variety of issues, including\r\nthe presence of endogeneity and overfitting. In fact, in the case of random forests, the predicted\r\nchoice probability for Swissmetro does not always decrease as its price increases. Figure EC.3\r\nin Appendix E.5 illustrates this phenomenon. This finding shows that model-free methods may\r\ncapture unrealistic dependencies on product attributes, in spite of their strong predictive accuracy.\r\n22\r\n7. Conclusion\r\nIn this paper, we introduce a new class of neural network-based choice models. Our neural network architecture, RUMnet, provides an efficient approximation of RUM discrete choice models.\r\nUnlike traditional RUM discrete choice models, which rely on specific structural and distributional\r\nassumptions, RUMnets leverage the universality of feed-forward neural networks to learn the structure of the random utility function from data. Moreover, our method is practical as RUMnets can\r\nbe trained using existing open-source libraries and achieves a competitive predictive performance.\r\nBy satisfying the RUM principle, we believe that our structured machine-learning approach yields\r\nbetter generalization to high-dimensional inputs than other model-free methods, as illustrated by\r\nour numerical findings. This work opens several intriguing research directions.\r\nFor instance, it might be possible to exploit the flexibility of the RUMnet architecture for\r\nmore complicated choice estimation problems such as multi-item and dynamic discrete choice. One\r\ndrawback of RUMnets is a larger computational cost compared to simpler neural network-based\r\narchitectures; it may be valuable to enhance our implementation for large-scale applications. Another important research avenue is to embed our neural network-based choice model in assortment\r\noptimization and pricing decision problems.\r\nReferences\r\nAlptekino\u02d8glu, Ayd\u0131n, John H Semple. 2016. The exponomial choice model: A new alternative for\r\nassortment and price optimization. Operations Research 64(1) 79\u201393.\r\nAlptekino\u02d8glu, Ayd\u0131n, John H Semple. 2021. Heteroscedastic exponomial choice. Operations Research 69(3) 841\u2013858.\r\nBartlett, Peter L, Shahar Mendelson. 2002. Rademacher and gaussian complexities: Risk bounds\r\nand structural results. Journal of Machine Learning Research 3(Nov) 463\u2013482.\r\nBentz, Yves, Dwight Merunka. 2000. Neural networks and the multinomial logit for brand choice\r\nmodelling: a hybrid approach. Journal of Forecasting 19(3) 177\u2013200.\r\nBerbeglia, Gerardo. 2018. The generalized stochastic preference choice model. arXiv preprint\r\narXiv:1803.04244 .\r\nBerbeglia, Gerardo, Agust\u00b4\u0131n Garassino, Gustavo Vulcano. 2022. A comparative empirical study of\r\ndiscrete choice models in retail operations. Management Science 68(6) 4005\u20134023.\r\nBierlaire, M. 2018. Swissmetro. URL: https://transpor. epfl. ch/documents/technicalReports/CS SwissmetroDescription. pdf .\r\n23\r\nChen, Ningyuan, Guillermo Gallego, Zhuodong Tang. 2019. The use of binary choice forests to\r\nmodel and estimate discrete choices. Available at SSRN 3430886 .\r\nChen, Yi-Chun, Velibor Mi\u02c7si\u00b4c. 2020. Decision forest: A nonparametric approach to modeling\r\nirrational choice. Available at SSRN 3376273 .\r\nChierichetti, Flavio, Ravi Kumar, Andrew Tomkins. 2018. Discrete choice, permutations, and\r\nreconstruction. Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete\r\nAlgorithms. SIAM, 576\u2013586.\r\nChollet, Fran\u00b8cois, et al. 2015. Keras. https://keras.io.\r\nClevert, Djork-Arn\u00b4e, Thomas Unterthiner, Sepp Hochreiter. 2015. Fast and accurate deep network\r\nlearning by exponential linear units (elus). arXiv preprint arXiv:1511.07289 .\r\nCortes, Corinna, Mehryar Mohri, Afshin Rostamizadeh. 2013. Multi-class classification with maximum margin multiple kernel. International Conference on Machine Learning. PMLR, 46\u201354.\r\nFarias, Vivek F, Srikanth Jagabathula, Devavrat Shah. 2013. A nonparametric approach to modeling choice with limited data. Management science 59(2) 305\u2013322.\r\nFeldman, Jacob, Dennis Zhang, Xiaofei Liu, Nannan Zhang. 2018. Customer choice models versus\r\nmachine learning: Finding optimal product displays on alibaba. Available at SSRN 3232059 .\r\nFeng, Qi, J George Shanthikumar, Mengying Xue. 2022. Consumer choice models and estimation:\r\nA review and extension. Production and Operations Management 31(2) 847\u2013867.\r\nGabel, Sebastian, Artem Timoshenko. 2021. Product choice with large assortments: A scalable\r\ndeep-learning model. Management Science .\r\nHan, Yafei, Christopher Zegras, Francisco Camara Pereira, Moshe Ben-Akiva. 2020. A neuralembedded choice model: TasteNet-MNL modeling taste heterogeneity with flexibility and\r\ninterpretability. arXiv:2002.00922 .\r\nHornik, Kurt. 1991. Approximation capabilities of multilayer feedforward networks. Neural networks 4(2) 251\u2013257.\r\nHornik, Kurt, Maxwell Stinchcombe, Halbert White. 1989. Multilayer feedforward networks are\r\nuniversal approximators. Neural networks 2(5) 359\u2013366.\r\nJiang, Zhaohui Zoey, Jun Li, Dennis Zhang. 2020. A high-dimensional choice model for online\r\nretailing. Available at SSRN 3687727 .\r\nKidger, Patrick, Terry Lyons. 2020. Universal approximation with deep narrow networks. Conference on learning theory. PMLR, 2306\u20132327.\r\n24\r\nKoltchinskii, Vladimir, Dmitriy Panchenko. 2000. Rademacher processes and bounding the risk of\r\nfunction learning. High dimensional probability II. Springer, 443\u2013457.\r\nLuce, R.D. 1959. Individual choice behavior: A theoretical analysis. Wiley.\r\nMaurer, Andreas. 2016. A vector-contraction inequality for rademacher complexities. International\r\nConference on Algorithmic Learning Theory. Springer, 3\u201317.\r\nMcFadden, Daniel. 1974. Conditional logit analysis of qualitative choice behavior. Frontiers in\r\nEconometrics 2 105\u2013142.\r\nMcFadden, Daniel, Kenneth Train. 2000. Mixed mnl models for discrete response. Journal of\r\napplied Econometrics 15(5) 447\u2013470.\r\nMohri, Mehryar, Afshin Rostamizadeh, Ameet Talwalkar. 2018. Foundations of machine learning.\r\nMIT press.\r\nM\u00a8uller, Rafael, Simon Kornblith, Geoffrey E Hinton. 2019. When does label smoothing help?\r\nAdvances in neural information processing systems 32.\r\nNeyshabur, Behnam, Ryota Tomioka, Nathan Srebro. 2015. Norm-based capacity control in neural\r\nnetworks. Conference on learning theory. PMLR, 1376\u20131401.\r\nRosenfeld, Nir, Kojin Oshiba, Yaron Singer. 2020. Predicting choice with set-dependent aggregation. International Conference on Machine Learning. PMLR, 8220\u20138229.\r\nRusmevichientong, Paat, Benjamin Van Roy, Peter W Glynn. 2006. A nonparametric approach to\r\nmultiproduct pricing. Operations Research 54(1) 82\u201398.\r\nShalev-Shwartz, Shai, Shai Ben-David. 2014. Understanding machine learning: From theory to\r\nalgorithms. Cambridge university press.\r\nSifringer, Brian, Virginie Lurkin, Alexandre Alahi. 2020. Enhancing discrete choice models with\r\nrepresentation learning. Transportation Research Part B: Methodological 140 236\u2013261.\r\nSzegedy, Christian, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna. 2016. Rethinking the inception architecture for computer vision. Proceedings of the IEEE conference on\r\ncomputer vision and pattern recognition. 2818\u20132826.\r\nTrain, Kenneth E. 2009. Discrete Choice Methods with Simulation. Cambridge University Press,\r\nCambridge, United Kingdom.\r\nWang, Shenhao, Baichuan Mo, Jinhua Zhao. 2020. Deep neural networks for choice analysis:\r\nArchitecture design with alternative-specific utility functions. Transportation Research Part\r\nC: Emerging Technologies 112 234\u2013251.\r\n25\r\nWang, Shenhao, Baichuan Mo, Jinhua Zhao. 2021a. Theory-based residual neural networks: A\r\nsynergy of discrete choice models and deep neural networks. Transportation research part B:\r\nmethodological 146 333\u2013358.\r\nWang, Shenhao, Qingyi Wang, Nate Bailey, Jinhua Zhao. 2021b. Deep neural networks for choice\r\nanalysis: A statistical learning theory perspective. Transportation Research Part B: Methodological 148 60\u201381.\r\nWilliams, H.C.W.L. 1977. On the formation of travel demand models and economic evaluation\r\nmeasures of user benefit. Environment and Planning A 3(9) 285\u2013344.\r\nWong, Melvin, Bilal Farooq. 2019. Reslogit: A residual neural network logit model.\r\narXiv:1912.10058 .\r\ne-companion to : Random Utility Maximization: Revisited ec1\r\nThis page is intentionally blank. Proper e-companion title\r\npage, with INFORMS branding and exact metadata of the\r\nmain paper, will be produced by the INFORMS office when\r\nthe issue is being assembled.\r\nec2 e-companion to : Random Utility Maximization: Revisited\r\nOnline Appendix\r\nRepresenting Random Utility Choice Models with Neural\r\nNetworks\r\nThe Appendix is organized as follows.\r\n\u2022 Appendix A provides additional material for Section 2.\r\n\u2022 Appendix B contains the proofs of Section 3.\r\n\u2022 Appendix C contains the proofs of Section 4.\r\n\u2022 Appendix D provides additional material for Section 5.\r\n\u2022 Appendix E provides additional material for Section 6.\r\ne-companion to : Random Utility Maximization: Revisited ec3\r\nAppendix A: Additional material for Section 2\r\nA feed-forward neural network is specified by a directed acyclic graph, G = (V, E). Each edge e \u2208 E\r\nis associated with a weight we \u2208 R. Each node of the graph, called a neuron, is associated with an\r\nactivation function \u03c3 : R \u2192 R. Each edge in the graph links the output of some neuron to the input\r\nof another neuron. The input of a neuron is obtained by taking a weighted sum of the outputs of\r\nall neuron connected to it. We assume that the nodes are organized in layers, i.e. the set of nodes is\r\npartitioned in disjoint subsets V = \u222a\r\nT\r\nt=0Vt\r\n, such that every edge in E connects some node in Vt\u22121 to\r\nsome node in Vt\r\n, for some t \u2208 [T]. We denote by vi,t \u2208 Vt the i\r\nth neuron of the t\r\nth layer. Moreover,\r\nfor any input x to the network and vi,t \u2208 Vt\r\n, we let oi,t(x) (resp. ai,t(x)) denotes the output (resp.\r\ninput) of vi,t. Then,\r\nai,t+1 =\r\nX\r\nj:e=(vj,t,vi,t+1)\u2208E\r\nwe \u00b7 oj,t(x),\r\nand\r\noi,t+1(x) = \u03c3(ai,t+1(x)).\r\nThe first layer V0 is called the input layer and the last layer VT is the output layer which often\r\ncontains a single neuron. The layers V1, . . . , VT \u22121 are called the hidden layers. We refer to T as the\r\ndepth of the network. The width of the network is maxt=1,...,T \u22121 |Vt\r\n| and its size is |V |. Figure EC.1\r\nillustrates a feed-forward network of depth 2. Feed-forward neural networks are able to capture\r\ncomplex nonlinear patterns and have been shown to be a class of universal approximators. That\r\nis, under mild conditions, the class of neural networks S\r\nk\u22651 \u0398\r\n1,k\r\nk with only one hidden layer and\r\none output unit is dense in the space of continuous functions over a compact domain X, e.g., this\r\nv0,1\r\nv0,2\r\nv0,3\r\nv1,1\r\nv1,2\r\nv1,3\r\nv1,4\r\nv1,5\r\nv2,1\r\nv2,2\r\nv2,3\r\nv2,4\r\nv2,5\r\nv3,1\r\nv3,2\r\nx1\r\nx2\r\nx3\r\nOuput 1\r\nOutput 2\r\nInput Input layer Hidden layers Output layer Output\r\nFigure EC.1 An example of feed-forward neural network.\r\nec4 e-companion to : Random Utility Maximization: Revisited\r\nresult holds for any continuous, bounded and non-constant activation function (Hornik et al. 1989,\r\nHornik 1991).\r\ne-companion to : Random Utility Maximization: Revisited ec5\r\nAppendix B: Proofs of Section 3\r\nB.1. A preliminary result\r\nIn the RUM framework, recall that we are interested in the ordering of the utilities for different\r\nalternatives. Indeed, for every x 6= x\r\n\u2032\r\n, the inequality U(x, \u01eb(x), z,\u03bd(z)) \u2265 U(x\r\n\u2032\r\n, \u01eb(x\r\n\u2032\r\n), z,\u03bd(z)) implies that x is preferred to x\r\n\u2032 by the corresponding random customer. The following lemma shows\r\nthat it is sufficient to approximate U(\u00b7) on a finite covering C in order to capture the desired\r\nordering relations of all possible pairs (x,x\r\n\u2032\r\n) \u2208 X 2 and customer attribute z with high probability. Throughout our analysis, the space of product and customer attributes is endowed with the\r\n\u2113\u221e-norm unless stated otherwise. We denote by B(c, \u03b4) the open ball centred on c with radius \u03b4.\r\nLemma EC.1 (Part of Theorem 1 in McFadden and Train 2000). For every \u03b7 > 0,\r\nthere exists a finite covering (B(c, \u03b4(c)))c\u2208C with centers C \u2282 X 2 \u00d7 Z, radii (\u03b4(c))c\u2208C > 0, and integers (n(c))c\u2208C such that, given a fixed c = (x\u02c6,x\u02c6\r\n\u2032\r\n, z\u02c6) \u2208 C, with probability at least 1 \u2212 3\u03b7/4\u03ba, we\r\nhave for all (x,x\r\n\u2032\r\n, z) \u2208 X 2 \u00d7 Z in the neighborhood of (x\u02c6,x\u02c6\r\n\u2032\r\n, z\u02c6) that:\r\n1. |U(x, \u01eb(x), z,\u03bd(z)) \u2212 U(x\u02c6, \u01eb(x\u02c6), z\u02c6,\u03bd(z\u02c6))| \u2264 1/n(c),\r\n2. |U(x\r\n\u2032\r\n, \u01eb(x\r\n\u2032\r\n), z,\u03bd(z)) \u2212 U(x\u02c6\r\n\u2032\r\n, \u01eb(x\u02c6\r\n\u2032\r\n), z\u02c6,\u03bd(z\u02c6))| \u2264 1/n(c).\r\n3. |U(x, \u01eb(x), z,\u03bd(z)) \u2212 U(x\r\n\u2032\r\n, \u01eb(x\r\n\u2032\r\n), z,\u03bd(z))| \u2265 5/n(c).\r\nThe above lemma formalizes the main step in the proof of the celebrated result of\r\nMcFadden and Train (2000) showing that continuous mixtures of MNL models uniformly approximate the class of RUMS. The proof of this result exploits the uniform continuity of U(\u00b7), \u01eb(x) and\r\n\u03bd(z) for all (x, z) \u2208 X \u00d7 Z. For completeness, the proof of Lemma EC.1 is given below; note that\r\nthe statement of this result is implicit in McFadden and Train (2000), who base their analysis on\r\nslightly weaker properties of the covering.\r\nProof. Recall that \u01eb(\u00b7) and \u03bd(\u00b7) are random fields over X and Z respectively. For analysis\r\npurposes, we make the underlying probability space (\u2126,W,Pr\u03c9) explicit. In particular, we denote\r\nby \u01eb(w,x) (resp. \u03bd(w, z)) a particular realization of \u01eb(x) (resp. \u03bd(z)). Consequently, for every\r\n(w,x, z) \u2208 \u2126 \u00d7 X \u00d7 Z, we use the compact notation F(\u03c9,x, z) = U (x, \u01eb(\u03c9,x), z,\u03bd(\u03c9, z)). Recall\r\nthat for all choice events (z, A) and x \u2208 A, we have\r\n\u03c0(x, z, A) = Pr\u03c9[F(\u03c9,x, z) > F(\u03c9,x\r\n\u2032\r\n, z), \u2200x\r\n\u2032 \u2208 A\\x].\r\nThe remainder of the proof proceeds in three steps.\r\n1. The utilities are sufficiently different for distinct alternatives. For every (x,x\r\n\u2032\r\n, z) \u2208 X 2 \u00d7 Z\r\nand n \u2208 N+, let\r\n\u2126n(x,x\r\n\u2032\r\n, z) = {\u03c9 \u2208 \u2126 : |F(\u03c9,x, z) \u2212 F(\u03c9,x\r\n\u2032\r\n, z)| \u2265 7/n} .\r\nec6 e-companion to : Random Utility Maximization: Revisited\r\nThe continuity of U(\u00b7) and the measurability of the random fields \u01eb(\u00b7) and \u03bd(\u00b7) imply that\r\n\u2126n(x,x\r\n\u2032\r\n, z) is measurable. This set is monotone increasing as n \u2192 \u221e to the set of \u03c9 for which the\r\nalternatives x and x\r\n\u2032 are not tied. By hypothesis, this set has probability one, implying that there\r\nexists \u00afn = n(x,x\r\n\u2032\r\n, z) \u2208 N+ such that we have Pr\u03c9[\u2126n\u00af(x,x\r\n\u2032\r\n, z)] \u2265 1 \u2212 \u03b7/4\u03ba.\r\n2. There exists a neighborhood where utility remains controlled for distinct alternatives. Fix\r\n(x,x\r\n\u2032\r\n, z) \u2208 X 2 \u00d7 Z. The uniform continuity of U(\u00b7) on X \u00d7 [0, 1]dx \u00d7 Z \u00d7 [0, 1]dz\r\nimplies that, for\r\nevery given \u00afn \u2208 N+, there exists \u00af\u03b4 = \u03b4(x,x\r\n\u2032\r\n, z) \u2208 R+ such that in a neighborhood of size \u00af\u03b4, U(\u00b7)\r\nvaries by less than 1/n\u00af. Moreover, the almost certain continuity of \u01eb(\u03c9,x) and \u03bd(\u03c9, z) implies that\r\nBm(x, z) = (\r\n\u03c9 \u2208 \u2126 : sup\r\n|x\u2212x\u2217|<1/m\r\n|\u01eb(\u03c9,x) \u2212 \u01eb(\u03c9,x\r\n\u2217\r\n)| + sup\r\n|z\u2212z\u2217|<1/m\r\n|\u03bd(\u03c9, z) \u2212 \u03bd(\u03c9, z\r\n\u2217\r\n)| \u2264\r\n\u00af\u03b4\r\n2\r\n)\r\n,\r\nand the corresponding event Bm(x\r\n\u2032\r\n, z) are monotone increasing as m \u2192 \u221e to limiting events\r\nthat occur with probability one. Consequently, there exists \u00afm = m(x,x\r\n\u2032\r\n, z) \u2208 N+ such that\r\nPr\u03c9[Bm\u00af (x, z)] \u2265 1 \u2212 \u03b7/4\u03ba and Pr\u03c9[Bm\u00af (x\r\n\u2032\r\n, z)] \u2265 1 \u2212 \u03b7/4\u03ba.\r\n3. Finite covering. We have established so far that Pr\u03c9[\u2126n\u00af (x,x\r\n\u2032\r\n, z) \u2229 Bm\u00af (x, z) \u2229 Bm\u00af (x\r\n\u2032\r\n, z)] \u2265\r\n1 \u2212 3\u03b7/4\u03ba, noting that \u03c9 \u2208 \u2126n\u00af(x,x\r\n\u2032\r\n, z) \u2229 Bm\u00af (x, z) \u2229 Bm\u00af (x\r\n\u2032\r\n, z) implies:\r\n(a) |F(\u03c9,x, z) \u2212 F(\u03c9,x\r\n\u2032\r\n, z)| \u2265 7/n\u00af ,\r\n(b) For all (x\u02c6,x\u02c6\r\n\u2032\r\n, z\u02c6) \u2208 in the open ball centered on (x,x\r\n\u2032\r\n, z) with radius min{1/m, \u00af\r\n\u00af\u03b4/2}, we have\r\n|F(\u03c9,x\u02c6, z\u02c6) \u2212 F(\u03c9,x, z)| \u2264 1/n\u00af and |F(\u03c9,x\u02c6\r\n\u2032\r\n, z\u02c6) \u2212 F(\u03c9,x\r\n\u2032\r\n, z)| \u2264 1/n\u00af.\r\nThe above neighborhoods specify a covering of the compact set X \u00d7 X \u00d7 Z, from which we extract\r\na finite subcovering C. Hence, we have just shown that each (x\u02c6,x\u02c6\r\n\u2032\r\n, z\u02c6) \u2208 X \u00d7 X \u00d7 Z falls in some\r\nneighborhood centered on (x,x\r\n\u2032\r\n, z) \u2208 C such that for all \u03c9 \u2208 \u2126n\u00af (x,x\r\n\u2032\r\n, z) \u2229 Bm\u00af (x, z) \u2229 Bm\u00af (x\r\n\u2032\r\n, z):\r\n|F(\u03c9,x\u02c6, z\u02c6) \u2212 F(\u03c9,x\u02c6\r\n\u2032\r\n, z\u02c6)|\r\n\u2265 |F(\u03c9,x, z) \u2212 F(\u03c9,x\r\n\u2032\r\n, z)| \u2212 |F(\u03c9,x\u02c6, z\u02c6) \u2212 F(\u03c9,x, z)| \u2212 |F(\u03c9,x\u02c6\r\n\u2032\r\n, z\u02c6) \u2212 F(\u03c9,x\r\n\u2032\r\n, z)|\r\n\u2265 7/n(x,x\r\n\u2032\r\n, z) \u2212 1/n(x,x\r\n\u2032\r\n, z) \u2212 1/n(x,x\r\n\u2032\r\n, z)\r\n\u2265 5/n(x,x\r\n\u2032\r\n, z) .\r\n\r\nB.2. Proof of Proposition 1\r\nWe now present the proof of Proposition 1, which proceeds linearly in showing that we can control\r\nthe errors in the successive layers of approximation of the utility function. Let C denote the\r\nfinite covering and N\u00af = max{n(c) : c \u2208 C} that satisfy the properties stated in Lemma EC.1. Let\r\nN \u2265 max{\u2212log(\u03b7/4\u03ba), N\u00af} be a sufficiently large integer that will be determined later on. Recall\r\nthat \u01eb(\u00b7) and \u03bd(\u00b7) are random fields over X and Z respectively. As in the proof of Lemma EC.1, we\r\ne-companion to : Random Utility Maximization: Revisited ec7\r\nexplicitly describe this randomness. In particular, we introduce a fundamental probability space\r\n(\u2126,W,Pr\u03c9) and denote by \u01eb(w,x) (resp. \u03bd(w, z)) a particular realization of \u01eb(x) (resp. \u03bd(z)). For\r\nease of notation, we let for (w,x, z) \u2208 \u2126 \u00d7 X \u00d7 Z,\r\nF(\u03c9,x, z) = U (x, \u01eb(\u03c9,x), z,\u03bd(\u03c9, z)).\r\nRecall that for all choice event (z, A) and x \u2208 A, we have\r\n\u03c0(x, z, A) = Pr\u03c9[F(\u03c9,x, z) > F(\u03c9,x\r\n\u2032\r\n, z), \u2200x\r\n\u2032 \u2208 A\\x].\r\nStep 1: Universal approximation and discretization. Feed-forward neural networks are a known\r\nclass of universal approximators. More specifically, the space of feed-forward neural networks with\r\nonly one hidden layer and one output unit is dense in C(X), the space of continuous function on\r\nX, under the conditions that X is compact and the activation function is continuous, bounded and\r\nnon-constant (Hornik et al. 1989, Hornik 1991). This result is extended for networks of bounded\r\nwidth and arbitrary depth and any nonaffine continuous activation function by Kidger and Lyons\r\n(2020). Consequently, there exists a feed-forward neural network NU (\u00b7) that approximates U(\u00b7) on\r\nX \u00d7 [0, 1]d\u01eb \u00d7 Z \u00d7 [0, 1]d\u03bd\r\n, i.e., that satisfies kU(\u00b7) \u2212 NU (\u00b7)k\u221e \u2264 1/N.\r\nBy the uniform continuity of U(\u00b7), there exist \u03b4 > 0, such that for all (x, z) \u2208 X \u00d7 Z and (\u01eb, \u01eb\r\n\u2032\r\n) \u2208\r\n[0, 1]2d\u01eb and (\u03bd,\u03bd\r\n\u2032\r\n) \u2208 [0, 1]2d\u03bd such that k\u01eb \u2212 \u01eb\r\n\u2032k\u221e \u2264 \u03b4 and k\u03bd \u2212 \u03bd\r\n\u2032k\u221e \u2264 \u03b4, we have |U(x, \u01eb, z,\u03bd) \u2212\r\nU(x, \u01eb\r\n\u2032\r\n, z,\u03bd\r\n\u2032\r\n)| \u2264 1/N. For every \u03c9 \u2208 \u2126, there exist feed-forward neural networks N\u01eb,\u03c9(x) and N\u03bd,\u03c9(z)\r\nthat approximate \u01eb(\u03c9,x) and \u03bd(\u03c9, z) on X and Z respectively, i.e., k\u01eb(\u03c9,x) \u2212 N\u01eb,\u03c9(x)k\u221e \u2264 \u03b4 for\r\nall x \u2208 X and k\u03bd(\u03c9, z)\u2212 N\u03bd,\u03c9(z)k\u221e \u2264 \u03b4 for all z \u2208 Z. Consequently, for all x \u2208 X , \u03c9 \u2208 \u2126 and z \u2208 Z,\r\nwe have\r\n|U(x, \u01eb(\u03c9,x), z,\u03bd(\u03c9, z)) \u2212 NU(x, N\u01eb,\u03c9(x), z, N\u03bd,\u03c9(z))| \u2264 2\r\nN\r\n, (EC.1)\r\nsince |NU (x, N\u01eb,\u03c9(x), z, N\u03bd,\u03c9(z))\u2212U(x, N\u01eb,\u03c9(x), z, N\u03bd,\u03c9(z))| \u2264 1/N in light of the uniform approximation of U(\u00b7) by NU (\u00b7) and |U(x, N\u01eb,\u03c9(x), z, N\u03bd,\u03c9(z))\u2212U(x, \u01eb(\u03c9,x), z,\u03bd(\u03c9, z))| \u2264 1/N due to the\r\nuniform continuity of U(\u00b7).\r\nStep 2: Adding noise. Throughout the remainder of the proof, we fix a choice set A, an alternative\r\nx \u2208 A, and customer attributes z \u2208 Z. For each x \u2208 A, we define a utility F\r\n1\r\n(\u00b7) as follows:\r\nF\r\n1\r\n(\u03c9, \u03b4x,x, z) = NU (x, N\u01eb,\u03c9(x), z, N\u03bd,\u03c9(z)) + \u03b4x\r\nN2\r\n, (EC.2)\r\nwhere \u03b4 = {\u03b4x}x\u2208A is a sequence i.i.d. random variables following a standard Gumbel distribution.\r\nLet\r\n\u03c0\r\n1\r\n(x, z, A) = Pr\u03c9,\u03b4[F\r\n1\r\n(\u03c9, \u03b4x,x, z) > F1\r\n(\u03c9, \u03b4x\u2032,x\r\n\u2032\r\n, z), \u2200x\r\n\u2032 \u2208 A\\{x}],\r\nec8 e-companion to : Random Utility Maximization: Revisited\r\nbe the choice probability associated with the utility F\r\n1\r\n(\u00b7). For all x\r\n\u2032 \u2208 A\\{x}, let\r\n\u2126\r\n1\r\nx,x\u2032 = {\u03c9 \u2208 \u2126,(\u03b4x, \u03b4x\u2032) \u2208 R\r\n2\r\n: (F(\u03c9,x, z) \u2212 F(\u03c9,x\r\n\u2032\r\n, z)) \u00d7 (F\r\n1\r\n(\u03c9, \u03b4x,x, z) \u2212 F\r\n1\r\n(\u03c9, \u03b4x\u2032,x\r\n\u2032\r\n, z)) < 0}\r\nbe the collection of realisations for which the preferences over alternatives x and x\r\n\u2032 are reversed\r\nwhen using F\r\n1\r\n(\u00b7) instead of F(\u00b7). Note that\r\n|\u03c0(x, z, A) \u2212 \u03c0\r\n1\r\n(x, z, A)| \u2264 Pr\r\n\uf8ee\r\n\uf8f0\r\n[\r\nx\u2032\u2208A\\{x}\r\n\u2126\r\n1\r\nx,x\u2032\r\n\uf8f9\r\n\uf8fb \u2264\r\nX\r\nx\u2032\u2208A\\{x}\r\nPr\r\n\u2126\r\n1\r\nx,x\u2032\r\n\r\n\u2264 \u03ba \u00b7Pr\r\n\u2126\r\n1\r\nx,x\u2032\r\n\r\n,\r\nwhere the first inequality holds follows by noting that for F\r\n1\r\n(\u00b7) to designate x as the the highestutility alternative in the choice set A, while F(\u00b7) designates a different alternative x\r\n\u2032\r\nin the choice\r\nset A, there needs to be a reversal of preferences between x and x\r\n\u2032\r\n. The second inequality follows\r\nfrom the union bound. Without loss of generality, consider \u03c9 \u2208 \u2126 and (\u03b4x, \u03b4x\u2032) \u2208 R\r\n2\r\nsuch that\r\nF(\u03c9,x, z) > F(\u03c9,x\r\n\u2032\r\n, z) and F\r\n1\r\n(\u03c9, \u03b4x,x, z) < F1\r\n(\u03c9, \u03b4x\u2032,x\r\n\u2032\r\n, z).\r\nIf |F(\u03c9,x, z) \u2212 F(\u03c9,x\r\n\u2032\r\n, z)| > 5/N, then\r\n0 > F1\r\n(\u03c9, \u03b4x,x, z) \u2212 F\r\n1\r\n(\u03c9, \u03b4x\u2032,x\r\n\u2032\r\n, z)\r\n= NU (x, N\u01eb,\u03c9(x), z, N\u03bd,\u03c9(z)) \u2212 NU(x\r\n\u2032\r\n, N\u01eb,\u03c9(x\r\n\u2032\r\n), z, N\u03bd,\u03c9(z)) + (\u03b4x \u2212 \u03b4x\u2032)\r\n> F(\u03c9,x, z) \u2212 F(\u03c9,x\r\n\u2032\r\n, z) \u2212 4/N + (\u03b4x \u2212 \u03b4x\u2032)/N2\r\n\u2265 1/N + (\u03b4x \u2212 \u03b4x\u2032)/N2\r\n,\r\nwhere the second inequality holds by Equation (EC.1). Consequently, \u03b4x \u2212 \u03b4x\u2032 \u2264 \u2212N. Under the\r\nGumbel assumption, we have for N \u2265 \u2212log(\u03b7/4\u03ba), Pr\u03b4[\u03b4x \u2212\u03b4x\u2032 \u2264 \u2212N] < \u03b7/4\u03ba. From Lemma EC.1,\r\nthe probability that |F(\u03c9,x, z) \u2212 F(\u03c9,x\r\n\u2032\r\n, z)| \u2264 5/N is at most 3\u03b7/4\u03ba. Consequently, Pr\r\n\u2126\r\n1\r\nx,x\u2032\r\n\r\n\u2264\r\n\u03b7/\u03ba which in turn implies that\r\n|\u03c0(x, z, A) \u2212 \u03c0\r\n1\r\n(x, z, A)| \u2264 \u03b7. (EC.3)\r\nStep 3: Sampling error. For purposes of analysis, we need to refine the finite covering constructed in Lemma EC.1. In particular, for every x \u2208 X , z \u2208 Z, \u03c9 \u2208 \u2126 and \u03b4 \u2208 R, we define the\r\nmapping F\u00af1\r\n(\u03c9,x, z) = NU (x, N\u01eb,\u03c9(x), z, N\u03bd,\u03c9(z)). Next, we construct a finite covering that is mutually adapted to F and F\u00af1\r\nin the sense of Lemma EC.1. Specifically, there exists a finite covering\r\n(B(c, \u03b4+(c)))c\u2208C+ with centers C\r\n+ \u2286 X 2 \u00d7 Z, radii (\u03b4\r\n+(c))c\u2208C+, and integers (n\r\n+(c))c\u2208C+ such that,\r\ngiven a fixed c = (x\u02dc,x\u02dc\r\n\u2032\r\n, z\u02dc) \u2208 C\r\n+, with probability at least 1 \u2212 3\u03b7/4\u03ba , we have for all (x,x\r\n\u2032\r\n, z) \u2208\r\nX\r\n2 \u00d7 Z in the neighborhood of (x\u02dc,x\u02dc\r\n\u2032\r\n, z\u02dc) that:\r\n1. |F\u00af1\r\n(\u03c9,x, z) \u2212 F\u00af1\r\n(\u03c9,x\u02dc, z\u02dc)| \u2264 1/n+(c) and |F(\u03c9,x, z) \u2212 F(\u03c9,x\u02dc, z\u02dc)| \u2264 1/n+(c),\r\ne-companion to : Random Utility Maximization: Revisited ec9\r\n2. |F\u00af1\r\n(\u03c9,x\r\n\u2032\r\n, z) \u2212 F\u00af1\r\n(\u03c9,x\u02dc\r\n\u2032\r\n, z\u02dc)| \u2264 1/n+(c) and |F(\u03c9,x\r\n\u2032\r\n, z) \u2212 F(\u03c9,x\u02dc\r\n\u2032\r\n, z\u02dc)| \u2264 1/n+(c),\r\n3. |F\u00af1\r\n(\u03c9,x, z\u02dc) \u2212 F\u00af1\r\n(\u03c9,x\r\n\u2032\r\n, z\u02dc)| \u2265 5/n+(c) and |F(\u03c9,x, z\u02dc) \u2212 F(\u03c9,x\r\n\u2032\r\n, z\u02dc)| \u2265 5/n+(c).\r\nConsequently, we set the precise value of N as N = max{\u2212log(\u03b7/8\u03ba), N, \u00af max{n\r\n+(c) : c \u2208 C\r\n+}}.\r\nNext, we construct a mapping from any choice set A to A\u02c6 = {x\u02c61, . . . ,x\u02c6K}, where each alternative\r\nx \u2208 A is replaced by a fixed element x\u02c6 of T\r\nc\u2208C+(x)\r\n{y1 : y \u2208 B(c, \u03b4+(c))}, where C\r\n+(x) denotes\r\nall the centers c \u2208 C\r\n+ such that (x, c2, c3) \u2208 B(c, \u03b4+(c)). Note that the latter intersection of open\r\nballs is non-empty since it contains x. Similarly, each z \u2208 Z is mapped to a fixed element z\u02c6 of\r\nT\r\nc\u2208C+(z)\r\n{y3 : y \u2208 B(c, \u03b4+(c))}, where C\r\n+(z) denotes all the centers c \u2208 C\r\n+ such that (c1, c2, z) \u2208\r\nB(c, \u03b4+(c)). We denote by E the collection of all choice events (x\u02c6, z, \u02c6 A\u02c6) generated by this mapping.\r\nSince the covering C\r\n+ is finite, E is also finite. We establish the following claim in Appendix B.3,\r\nshowing that all choice events can be approximated by those in E, with only small changes in their\r\nchoice probabilities \u03c0, \u03c01\r\n.\r\nClaim EC.1. For every choice event (x, z, A) \u2208 X \u00d7 Z \u00d7 X \u03ba\u22121\r\n, we have\r\n|\u03c0(x, z, A) \u2212 \u03c0(x\u02c6, z\u02c6,A\u02c6)| \u2264 \u03b7 and |\u03c0\r\n1\r\n(x, z, A) \u2212 \u03c0\r\n1\r\n(x\u02c6, z\u02c6,A\u02c6)| \u2264 \u03b7 . (EC.4)\r\nNow, the final piece of our proof is to construct a sample average approximation of \u03c0\r\n1\r\n(\u00b7) with respect\r\nto \u03c9. Let \u03c9\u02c6 = (\u02c6\u03c91, . . . ,\u03c9\u02c6K) be K i.i.d. samples based on the probabilistic space (\u2126,W,Pr\u03c9) where\r\nthe precise value of K is specified later on. We define the sampled utility function F\r\n2,k(\u03b4x,x, z) as\r\nfollows:\r\nF\r\n2,k(\u03b4x,x, z) = F\r\n1\r\n(\u02c6\u03c9k, \u03b4x,x, z) = NU (x, N\u01eb,\u03c9\u02c6k\r\n(x), z, N\u03bd,\u03c9\u02c6k\r\n(z)) + \u03b4x\r\nN2\r\n. (EC.5)\r\nBy equation (EC.2), F\r\n2,k(\u03b4x,x, z) can be interpreted as the realisation of F\r\n1\r\n(\u03c9, \u03b4x,x, z) with\r\nrespect to the sample \u02c6\u03c9k. However, it is worth observing that F\r\n2,k(\u03b4x,x, z) is itself a random\r\nvariable due to the noise term \u03b4x. Finally, we let \u03c0\r\n2\r\n(x, z, A) be the sample mean estimator of the\r\nchoice probabilities with respect to F\r\n2,1\r\n, . . . , F2,K, i.e.,\r\n\u03c0\r\n2\r\n(x, z, A) = 1\r\nK\r\n\u00b7\r\nX\r\nK\r\nk=1\r\nPr\u03b4\r\n\r\nF\r\n2,k(\u03b4x,x, z) > F2,k(\u03b4x\u2032,x\r\n\u2032\r\n, z), \u2200x\r\n\u2032 \u2208 A\\{x}\r\n\r\n. (EC.6)\r\nBy equations (EC.5) and (EC.6), it is clear that the choice model \u03c0\r\n2\r\n(\u00b7) can be represented by\r\nan instance of the RUMnet architecture. Hence, we denote by N(\u03c9\u02c6) \u2208 N d\r\n(K,\u0398\r\n\u2113,w\r\nM ) the RUMnet\r\narchitecture such that \u03c0\r\n2\r\n(\u00b7) = \u03c0\r\nRUMnet\r\nN(\u03c9\u02c6)\r\n(\u00b7).\r\nNext, we analyse the differences between the choice models \u03c0\r\n1\r\n(\u00b7) and its sample mean approximation \u03c0\r\nRUMnet\r\nN(\u03c9\u02c6)\r\n(\u00b7). First, we establish a property analogous to Claim EC.1 with respect to \u03c0\r\n2\r\n(\u00b7).\r\nThe proof is provided in Appendix B.4\r\nec10 e-companion to : Random Utility Maximization: Revisited\r\nClaim EC.2. For any K \u2265\r\n\u03ba\r\n2\r\nlog(4|E |)\r\n2\u00b7\u03b72 , with probability at least 3/4 with respect to \u03c9\u02c6 , for all\r\nchoice events (x, z, A) \u2208 X \u00d7 Z \u00d7 X \u03ba\u22121\r\n, we have\r\n|\u03c0\r\nRUMnet\r\nN(\u03c9\u02c6)\r\n(x, z, A) \u2212 \u03c0\r\nRUMnet\r\nN(\u03c9\u02c6)\r\n(x\u02c6, z\u02c6,A\u02c6)| \u2264 2\u03b7 .\r\nNext, we observe that, for all K \u2265\r\nlog(4|E |)\r\n2\u00b7\u03b72 ,\r\nPr\u03c9\u02c6\r\n\u0014\r\nmax\r\n(x,z, \u02c6 A\u02c6)\u2208E\r\n\f\r\n\f\r\n\f\r\n\u03c0\r\n1\r\n(x\u02c6, z\u02c6,A\u02c6) \u2212 \u03c0\r\nRUMnet\r\nN(\u03c9\u02c6)\r\n(x\u02c6, z\u02c6,A\u02c6)\r\n\f\r\n\f\r\n\f > \u03b7\u0015\r\n\u2264\r\nX\r\n(x,z, \u02c6 A\u02c6)\u2208E\r\nPr\u03c9\u02c6\r\nh\f\r\n\f\r\n\f\r\n\u03c0\r\n1\r\n(x\u02c6, z\u02c6,A\u02c6) \u2212 \u03c0\r\nRUMnet\r\nN(\u03c9\u02c6)\r\n(x\u02c6, z\u02c6,A\u02c6)\r\n\f\r\n\f\r\n\f > \u03b7i\r\n\u2264\r\nX\r\n(x,z, \u02c6 A\u02c6)\u2208E\r\n2 \u00b7 e\r\n\u22122\u00b7\u03b7\r\n2 log(4|E|)\r\n2\u00b7\u03b72\r\n\u2264\r\n1\r\n2\r\n, (EC.7)\r\nwhere the first inequality follows from the union bound and the second inequality from Hoeffding\u2019s\r\ninequality. Fix K = \u2308\r\n\u03ba\r\n2\r\nlog(4|E |)\r\n2\u00b7\u03b72 \u2309. By the union bound over inequality (EC.7) and Claim EC.2, there\r\nexists a realization \u03c9\u02c6\r\n\u2217 of \u03c9\u02c6 such that, for all (x\u02c6, z\u02c6,A\u02c6) \u2208 E, we have\r\n\f\r\n\f\r\n\f\r\n\u03c0\r\n1\r\n(x\u02c6, z\u02c6,A\u02c6) \u2212 \u03c0\r\nRUMnet\r\nN(\u03c9\u02c6\u2217)\r\n(x\u02c6, z\u02c6,A\u02c6)\r\n\f\r\n\f\r\n\f \u2264 \u03b7 . (EC.8)\r\nand for all choice events (x, z, A) \u2208 X \u00d7 Z \u00d7 X \u03ba\u22121\r\n, we have\r\n|\u03c0\r\nRUMnet\r\nN(\u03c9\u02c6\u2217)\r\n(x, z, A) \u2212 \u03c0\r\nRUMnet\r\nN(\u03c9\u02c6\u2217)\r\n(x\u02c6, z\u02c6,A\u02c6)| \u2264 2\u03b7 . (EC.9)\r\nPutting together Claim EC.1, inequalities (EC.3), (EC.8) and (EC.9), we have\r\n\f\r\n\f\u03c0(x, z, A) \u2212 \u03c0\r\nRUMnet\r\nN(\u03c9\u02c6\u2217)\r\n(x, z, A)\r\n\f\r\n\f\r\n\u2264\r\n\f\r\n\f\r\n\f\r\n\u03c0(x, z, A) \u2212 \u03c0(x\u02c6, z\u02c6,A\u02c6)\r\n\f\r\n\f\r\n\f +\r\n\f\r\n\f\r\n\f\r\n\u03c0(x\u02c6, z\u02c6,A\u02c6) \u2212 \u03c0\r\n1\r\n(x\u02c6, z\u02c6,A\u02c6)\r\n\f\r\n\f\r\n\f +\r\n\f\r\n\f\r\n\f\r\n\u03c0\r\n1\r\n(x\u02c6, z\u02c6,A\u02c6) \u2212 \u03c0\r\nRUMnet\r\nN(\u03c9\u02c6\u2217)\r\n(x\u02c6, z\u02c6,A\u02c6)\r\n\f\r\n\f\r\n\f\r\n+\r\n\f\r\n\f\r\n\f\r\n\u03c0\r\nRUMnet\r\nN(\u03c9\u02c6\u2217)\r\n(x\u02c6, z\u02c6,A\u02c6) \u2212 \u03c0\r\nRUMnet\r\nN(\u03c9\u02c6\u2217)\r\n(x, z, A)\r\n\f\r\n\f\r\n\f\r\n\u2264 5\u03b7 ,\r\nwhich yields the desired result.\r\nB.3. Proof of Claim EC.1\r\nWe establish the desired inequality for the choice model \u03c0\r\n1\r\n(\u00b7) since the case of \u03c0(\u00b7) proceeds from\r\nan identical reasoning. As in Step 2, we define \u2126x,x\u2032 for every x\r\n\u2032 \u2208 A \\ {x} as the event where\r\ncustomer z\u2019s preferences over (x,x\r\n\u2032\r\n) are reversed compared to customer z\u02c6\u2019s preferences over (x\u02c6,x\u02c6\r\n\u2032\r\n)\r\nwith respect to the random utility function F\r\n1\r\n(\u03c9, \u03b4, \u00b7, \u00b7). By the union bound, we have\r\n|\u03c0(x, z, A) \u2212 \u03c0(x\u02c6, z\u02c6,A\u02c6)| \u2264 X\r\nx\u2032\u2208A\\{x}\r\nPr[\u2126x,x\u2032] \u2264 \u03ba \u00b7 max\r\nx\u2032\u2208A\\{x}\r\nPr[\u2126x,x\u2032] . (EC.10)\r\ne-companion to : Random Utility Maximization: Revisited ec11\r\nWithout loss of generality, fix x,x\r\n\u2032 \u2208 A2 and consider \u03c9 \u2208 \u2126 and (\u03b4x, \u03b4x\u2032) \u2208 R\r\n2\r\nsuch that\r\nF\r\n1\r\n(\u03c9, \u03b4x,x, z) > F1\r\n(\u03c9, \u03b4x\u2032,x\r\n\u2032\r\n, z) and F\r\n1\r\n(\u03c9, \u03b4x\u02c6,x\u02c6, z\u02c6) < F1\r\n(\u03c9, \u03b4x\u02c6\u2032,x\u02c6\r\n\u2032\r\n, z\u02c6).\r\nNow, let c = (x\u02dc,x\u02dc\r\n\u2032\r\n, z\u02dc) \u2208 C\r\n+ be the center of a neighborhood that contains (x,x\r\n\u2032\r\n, z). By construction\r\nof our mapping, this neighborhood necessarily contains (x\u02c6,x\u02c6\r\n\u2032\r\n, z\u02c6) based on the coordinate-wise\r\ninequalities \u03b4\r\n+(c) \u2265 max{|x\u02c6 \u2212 x|, |x\u02c6\r\n\u2032 \u2212 x\r\n\u2032\r\n|, |z\u02c6 \u2212 z|}, which imply that (x\u02c6,x\u02c6\r\n\u2032\r\n, z\u02c6) \u2208 B(c, \u03b4+(c)). In\r\nwhat follows, suppose that \u03c9 satisfies properties 1-3 of the covering; by construction, this event\r\noccurs with probability at least 1 \u2212 3\u03b7/4\u03ba. Due to properties 1-2, we have\r\n0 > F1\r\n(\u03c9, \u03b4x\u02c6,x\u02c6, z\u02c6) \u2212 F\r\n1\r\n(\u03c9, \u03b4x\u02c6\u2032,x\u02c6\r\n\u2032\r\n, z\u02c6)\r\n\u2265 F\u00af1\r\n(\u03c9,x\u02dc, z\u02dc) \u2212 F\u00af1\r\n(\u03c9,x\u02dc\r\n\u2032\r\n, z\u02dc) \u2212 |F\u00af1\r\n(\u03c9,x\u02c6, z\u02c6) \u2212 F\u00af1\r\n(\u03c9,x\u02dc, z\u02dc)| \u2212 |F\u00af1\r\n(\u03c9,x\u02c6\r\n\u2032\r\n, z\u02c6) \u2212 F\u00af1\r\n(\u03c9,x\u02dc\r\n\u2032\r\n, z\u02dc)| + (\u03b4x\u02c6 \u2212 \u03b4x\u02c6\u2032)/N2\r\n\u2265 F\u00af1\r\n(\u03c9,x\u02dc, z\u02dc) \u2212 F\u00af1\r\n(\u03c9,x\u02dc\r\n\u2032\r\n, z\u02dc) \u2212 2/n+(c) + (\u03b4x\u02c6 \u2212 \u03b4x\u02c6\u2032)/N2\r\n,\r\nand\r\n0 < F1\r\n(\u03c9, \u03b4x,x, z) \u2212 F\r\n1\r\n(\u03c9, \u03b4x\u2032,x\r\n\u2032\r\n, z)\r\n\u2264 F\u00af1\r\n(\u03c9,x\u02dc, z\u02dc) \u2212 F\u00af1\r\n(\u03c9,x\u02dc\r\n\u2032\r\n, z\u02dc) + |F\u00af1\r\n(\u03c9,x, z) \u2212 F\u00af1\r\n(\u03c9,x\u02dc, z\u02dc)| + |F\u00af1\r\n(\u03c9,x\r\n\u2032\r\n, z) \u2212 F\u00af1\r\n(\u03c9,x\u02dc\r\n\u2032\r\n, z\u02dc)| + (\u03b4x \u2212 \u03b4x\u2032)/N2\r\n\u2264 F\u00af1\r\n(\u03c9,x\u02dc, z\u02dc) \u2212 F\u00af1\r\n(\u03c9,x\u02dc\r\n\u2032\r\n, z\u02dc) + 2/n+(c) + (\u03b4x \u2212 \u03b4x\u2032)/N2\r\n.\r\nCombining these inequalities with property 3, we infer that either \u03b4x\u02c6 \u2212 \u03b4x\u02c6\u2032 \u2264 \u22123\r\nN2\r\nn+(c) \u2264 \u22123N or\r\n\u03b4x \u2212\u03b4x\u2032 \u2265 3\r\nN2\r\nn+(c) \u2265 3N. Since N \u2265 \u2212log( \u03b7\r\n8\u03ba\r\n) and \u03b4 is a collection of i.i.d. Gumbel random variables,\r\neach of these events occurs with probability at most \u03b7\r\n8\u03ba\r\n. By the union bound, we derive the upper\r\nbound on reversal probabilities:\r\nPr [\u2126x,x\u2032] \u2264\r\n3\u03b7\r\n4\u03ba\r\n+ 2\r\n\u03b7\r\n8\u03ba\r\n=\r\n\u03b7\r\n\u03ba\r\n.\r\nThe desired inequality immediately follows by plugging the above inequality into (EC.10).\r\nB.4. Proof of Claim EC.2\r\nFix a choice event (x, z, A) \u2208 X \u00d7 Z \u00d7 X \u03ba\u22121\r\n. Similarly to the proof of Claim EC.1, for each x\r\n\u2032 \u2208 A\\\r\n{x\r\n\u2032} and k \u2208 [K], we develop an upper bound on the probability of a preference reversal conditional\r\nto the unobserved attribute \u03c9\u02c6 = \u02c6\u03c9k. Specifically, the preference reversal event \u2126k\r\nx,x\u2032\r\n,z\r\noccurs when\r\ncustomer z\u2019s preferences over (x,x\r\n\u2032\r\n) are reversed compared to customer z\u02c6\u2019s preferences over (x\u02c6,x\u02c6\r\n\u2032\r\n)\r\nwith respect to the k-th sampled random utility function F\r\n2,k(\u03b4, \u00b7, \u00b7) = F\r\n1\r\n(\u02c6\u03c9k, \u03b4, \u00b7, \u00b7). Let Gk\r\nx,x\u2032\r\n,z\r\ndenote the event for which \u02c6\u03c9k satisfies properties 1-3 with respect to the center (x\u02dc,x\u02dc\r\n\u2032\r\n, z\u02dc). The\r\nfact that (x\u02c6,x\u02c6\r\n\u2032\r\n, z\u02c6) and (x,x\r\n\u2032\r\n, z) are both contained in one neighborhood of the covering implies\r\nec12 e-companion to : Random Utility Maximization: Revisited\r\nGk\r\nx,x\u2032\r\n,z = Gk\r\nx\u02c6,x\u02c6\u2032\r\n,z\u02c6\r\n. Now, a close examination of the proof of Claim EC.1 reveals that, conditional on\r\nGk\r\nx\u02c6,x\u02c6\u2032\r\n,z\u02c6\r\n, the preference reversal occurs with probability at most \u03b7\r\n4\u03ba\r\n. It follows that:\r\n|\u03c0\r\nRUMnet\r\nN(\u03c9\u02c6)\r\n(x, z, A) \u2212 \u03c0\r\nRUMnet\r\nN(\u03c9\u02c6)\r\n(x\u02c6, z\u02c6,A\u02c6)|\r\n\u2264\r\n1\r\nK\r\n\u00b7\r\nX\r\nK\r\nk=1\r\nPr\r\n\uf8ee\r\n\uf8f0\r\n[\r\nx\u2032\u2208A\\{x}\r\n\u2126\r\nk\r\nx,x\u2032\r\n,z\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\f\r\n\u03c9\u02c6\r\n\uf8f9\r\n\uf8fb\r\n\u2264\r\n1\r\nK\r\n\u00b7\r\nX\r\nK\r\nk=1\r\nX\r\nx\u2032\u2208A\\{x}\r\nPr\u03b4\r\n\r\n\u2126\r\nk\r\nx,x\u2032\r\n,z\r\n\f\r\n\f\u03c9\u02c6k\r\n\r\n\u2264\r\n1\r\nK\r\n\u00b7\r\nX\r\nK\r\nk=1\r\nX\r\nx\u2032\u2208A\\{x}\r\n\r\nI[\u02c6\u03c9k \u2208/ G\r\nk\r\nx\u02c6,x\u02c6\u2032\r\n,z\u02c6\r\n] + Pr\r\n\u2126\r\nk\r\nx,x\u2032\r\n,z\r\n\f\r\n\f\u03c9\u02c6k,\u03c9\u02c6k \u2208 G\r\nk\r\nx\u02c6,x\u02c6\u2032\r\n,z\u02c6\r\n\r\n\u00b7I[\u02c6\u03c9k \u2208 G\r\nk\r\nx\u02c6,x\u02c6\u2032\r\n,z\u02c6\r\n]\r\n\u0001\r\n\u2264\r\n1\r\nK\r\n\u00b7\r\nX\r\nK\r\nk=1\r\nX\r\nx\u2032\u2208A\\{x}\r\n\u0010\r\nI[\u02c6\u03c9k \u2208/ G\r\nk\r\nx\u02c6,x\u02c6\u2032\r\n,z\u02c6\r\n] + \u03b7\r\n4\u03ba\r\n\u00b7I[\u02c6\u03c9k \u2208 G\r\nk\r\nx\u02c6,x\u02c6\u2032\r\n,z\u02c6\r\n]\r\n\u0011\r\n\u2264\r\n\u03b7\r\n4\r\n+\r\nX\r\nx\u2032\u2208A\\{x}\r\n1\r\nK\r\n\u00b7\r\nX\r\nK\r\nk=1\r\nI[\u02c6\u03c9k \u2208/ G\r\nk\r\nx\u02c6,x\u02c6\u2032\r\n,z\u02c6\r\n] ,\r\nwhere the first inequality holds since, absent a preference reversal over one pair (x,x\r\n\u2032\r\n), our RUMnet model identifies the same highest-utility alternative in the assortments A and A\u02c6. The second\r\ninequality proceed from the union bound. The next inequality follows from the formula of conditional expectations. The fourth inequality is direct consequence of our upper bound derived from\r\nthe proof of Claim EC.1. Now, we invoke Hoeffding\u2019s inequality so that, for every x\u02c6\r\n\u2032 \u2208 A\u02c6, we have\r\nPr\u03c9\u02c6\r\n\"\r\n1\r\nK\r\n\u00b7\r\n X\r\nK\r\nk=1\r\nI[\u02c6\u03c9k \u2208/ G\r\nk\r\nx\u02c6,x\u02c6\u2032\r\n,z\u02c6\r\n]\r\n!\r\n\u2212\r\n3\u03b7\r\n4\u03ba\r\n>\r\n\u03b7\r\n\u03ba\r\n#\r\n\u2264 e\r\n\u22122\r\n\u03b7\r\n2\r\n\u03ba2 K\r\n,\r\nwhere we note that E\u03c9\u02c6 [\r\n1\r\nK\r\nPK\r\nk=1 I[\u02c6\u03c9k \u2208/ Gk\r\nx\u02c6,x\u02c6\u2032\r\n,z\u02c6\r\n]] = Pr[\u02c6\u03c9k \u2208/ Gk\r\nx\u02c6,x\u02c6\u2032\r\n,z\u02c6\r\n]] \u2264\r\n3\u03b7\r\n4\u03ba\r\nbased on the construction\r\nof our covering. By the union bound, we have\r\nPr\u03c9\u02c6\r\nh\r\n\u2203(x, z, A) \u2208 X \u00d7 Z \u00d7 X \u03ba\u22121\r\ns.t. |\u03c0\r\nRUMnet\r\nN(\u03c9\u02c6)\r\n(x, z, A) \u2212 \u03c0\r\nRUMnet\r\nN(\u03c9\u02c6)\r\n(x\u02c6, z\u02c6,A\u02c6)| > 2\u03b7\r\ni\r\n\u2264 |E| \u00b7 e\r\n\u22122\r\n\u03b7\r\n2\r\n\u03ba2 K \u2264\r\n1\r\n4\r\n.\r\nwhere the first inequality holds since the number of distinct vectors (x\u02c6,x\u02c6\r\n\u2032\r\n, z\u02c6) is upper bounded by\r\n|E| and the next inequality immediately follows from the condition K \u2265\r\n\u03ba\r\n2\r\nlog(4|E |)\r\n2\u00b7\u03b72\r\ne-companion to : Random Utility Maximization: Revisited ec13\r\nAppendix C: Proofs of Section 4\r\nC.1. Proof of Proposition 3\r\nLet H be a hypothesis class, E be a collection of data observations endowed with a distribution D,\r\nand \u2113(\u00b7) be a loss function with respect to H and Z. For every hypothesis h \u2208 H, training sample\r\nS = {s1, . . . , sT }, and loss function \u2113(\u00b7), we define the empirical error as\r\nLS(h) = 1\r\nT\r\n\u00b7\r\nX\r\nT\r\nt=1\r\n\u2113(h(st)),\r\nand the associated true error\r\nL\r\ntrue\r\nD (h) = ES\u2032\u223cD [LS\u2032(h)].\r\nFor a given training sample S, let h\r\nERM\r\nS be the hypothesis that minimizes the empirical error LS(h) over all h \u2208 H. We begin by invoking a classical result to bound the generalization based on the Rademacher complexity; e.g., see Shalev-Shwartz and Ben-David (2014),\r\nKoltchinskii and Panchenko (2000), Bartlett and Mendelson (2002). Recall that the Rademacher\r\ncomplexity for a set of vectors A \u2286 R\r\nT\r\nis\r\nR(A) = 1\r\nT\r\n\u00b7E\u03c3\r\n\"\r\nsup\r\na\u2208A\r\nX\r\nT\r\ni=1\r\n\u03c3iai\r\n#\r\n,\r\nwhere \u03c3 = (\u03c31, . . . , \u03c3m) is a sequence of independent Rademacher random variables defined for all\r\ni by the following distribution: Pr[\u03c3i = 1] = Pr[\u03c3i = \u22121] = 1\r\n2\r\n.\r\nTheorem EC.1 (Shalev-Swartz and Ben-David (2014, Thm. 26.5)). Suppose that for\r\nevery hypothesis h \u2208 H and observation s \u2208 E, we have |\u2113(h(s))| \u2264 c for some c \u2265 0. Then, with\r\nprobability of at least (1 \u2212 \u03b4) with regards to the data sample S \u223c DT\r\n, we have\r\nL\r\ntrue\r\nD\r\n\r\nh\r\nERM\r\nS\r\n\u0001\r\n\u2264 LS(h\r\nERM\r\nS\r\n) + 2R(\u2113 \u25e6 H \u25e6 S) + 4c\r\nr\r\n2 ln(4/\u03b4)\r\nT\r\n,\r\nwhere \u2113 \u25e6 H \u25e6 S = {(\u2113(h(s1)), . . ., \u2113(h(sT ))) : h \u2208 H}.\r\nThis type of bound is often referred to as a data-dependent bound since the bound depends on the\r\nspecific training set S. It therefore suffices to bound the Rademacher complexity of the RUMnet\r\nclass N d\r\n(K,\u0398\r\n\u2113,w\r\nM ) composed with our data generative process and the negative log-likelihood loss\r\nfunction. Proposition 3 immediately follows from Lemma EC.2 below.\r\nLemma EC.2. Let \u2113(\u00b7) be the negative log-likelihood loss function, HRUMnet be the class of RUMnet\r\nclass and S = ((y1, z1, A1), . . . ,(yT , zT , AT )) be a training sample. Then, there exists a constant\r\nc1 > 0 such that\r\nR(\u2113 \u25e6 HRUMnet \u25e6 S) \u2264 c1 \u00b7\r\n\u03ba\r\n\u221a\r\n\u03ba\r\n\u221a\r\nT\r\n\u00b7 e\r\n2w\u00b7MM\u2113\r\n\r\nec14 e-companion to : Random Utility Maximization: Revisited\r\nThe remainder of this section establishes Lemma EC.2. The proof plugs together various notions of\r\nRademacher calculus. We first invoke several auxiliary results established in the previous literature.\r\nWe make use of the standard lemmas; see Shalev-Shwartz and Ben-David (2014, Chap. 26).\r\nLemma EC.3 (Contraction). Let A be a subset of R\r\nm. For each i \u2208 [m], let \u03c6i\r\n: R \u2192 R be a\r\n\u03c1-Lipschitz function; namely, for all \u03b1, \u03b2 \u2208 R, we have |\u03c6i(\u03b1) \u2212 \u03c6i(\u03b2)| \u2264 \u03c1|\u03b1 \u2212 \u03b2|. For a \u2208 R\r\nm let\r\n\u03c6(a) denote the vector (\u03c61(a1), . . . , \u03c6m(am)). Let \u03c6 \u25e6 A = {\u03c6(a) : a \u2208 A}. Then,\r\nR(\u03c6 \u25e6 A) \u2264 \u03c1R(A).\r\nLemma EC.4 (Convex combination). Let A be a subset of R\r\nm and let A\u2032 = {\r\nPN\r\nj=1 \u03b1ja\r\n(j)\r\n:\r\nN \u2208 N, \u2200j,a\r\n(j) \u2208 A, \u03b1j \u2265 0, |\u03b1|1 = 1}. Then, R(A\u2032\r\n) = R(A).\r\nWe utilize a generalization of the contraction lemma for hypothesis classes formed by vector-valued\r\nfunctions, established in the paper by Maurer (2016, Corollary 4).\r\nLemma EC.5 (Vector-valued contraction). Let E be an arbitrary set, (x1, . . . xT ) \u2208 ET\r\n, and\r\nlet F be a class of m-dimensional functions f : E \u2192 R\r\nm and let h : R\r\nm \u2192 R be an L-Lipschitz\r\nfunction with respect to the \u21132-norm. Then,\r\nE\r\n\"\r\nsup\r\nf\u2208F\r\nX\r\nT\r\ni=1\r\n\u03c3ih (f (xi))#\r\n\u2264\r\n\u221a\r\n2L\u00b7E\r\n\"\r\nsup\r\nf\u2208F\r\nX\r\nT\r\ni=1\r\nXm\r\nk=1\r\n\u03c3i,kfk (xi)\r\n#\r\n,\r\nwhere {\u03c3i}i\u2208[T] and {\u03c3i,k}i\u2208[T],k\u2208[m] are i.i.d. Rademacher random variables.\r\nThe following claim can be found in Neyshabur et al. (2015, Cor. 2).\r\nLemma EC.6 (Rademacher complexity of neural networks). Fix a neural network architecture with \u2113 \u2265 1 layers and assume that (i) the weight vector w for every node in the network satisfies kwk1 \u2264 M, (ii) the activation functions are ReLUs, and (iii) the input vectors S =\r\n{x1, . . . , xT } \u2286 R\r\nd\r\nsatisfy kxtk\u221e \u2264 1. Then, the class of functions F \u2208 R\r\nE defined by such neural\r\nnetwork architecture over the inputs S satisfies\r\nR(F \u25e6 S) \u2264\r\nr\r\n4 log(2d)\r\nT\r\n(2M)\r\n\u2113\r\n.\r\nFinally, let pmin be the minimum choice probability attained over the compact set of RUMnet\r\narchitectures HRUMnet. Our analysis will make use of the following property.\r\nClaim EC.3. pmin \u2265\r\n1\r\n\u03ba\r\n\u00b7 e\r\n\u22122M .\r\nThe claim follows immediately by noting that there are up to \u03ba choice alternatives. Additionally,\r\nthe MNL attractiveness weight of each alternative is in the range [e\r\n\u2212M , eM] since the utility u of\r\neach alternative is the final layer\u2019s output of a feed-forward neural network NU \u2208 \u0398\r\n\u2113,w\r\nM .\r\ne-companion to : Random Utility Maximization: Revisited ec15\r\nNow, we break down generating the collection of vectors \u2113 \u25e6 HRUMnet \u25e6 S into four operations for\r\neach choice event (in reverse order): (1) applying a log-transformation to the choice probability\r\nof the selected alternative, (2) averaging the choice probabilities over the samples of the RUMnet\r\narchitecture, (3) applying a softmax transformation of the utilities within each offered assortment,\r\n(4) computing the utility of each alternative using a feed-forward neural network. We denote by\r\n\u03c81, \u03c82, \u03c83, \u03c84 the class of mappings corresponding to each step, and by slightly abusing notation,\r\nwe have \u03c81 \u25e6 \u03c82 \u25e6 \u03c83 \u25e6 \u03c84 \u25e6 S = \u2113 \u25e6 HRUMnet \u25e6 S. Now, we bound the Rademacher complexity by\r\nconsidering these successive transformations:\r\n1. Note that \u03c81 applies the transformation p \u2208 (0, 1) 7\u2192 log(p) to the estimated probability p\r\nof the chosen alternative for each event s \u2208 S. This mapping is (1/pmin)-Lipschitz where pmin is\r\nthe minimal choice probability attained by the RUMnet architecture. Therefore, by combining\r\nClaim EC.3 and Lemma EC.3, we have:\r\nR(\u03c81 \u25e6\u03c82 \u25e6\u03c83 \u25e6\u03c84 \u25e6 S) = \u03bae2M \u00b7 R(\u03c82 \u25e6\u03c83 \u25e6\u03c84 \u25e6 S) .\r\n2. Next, the mapping \u03c82 computes the unweighted average of choice probabilities over the samples (k1, k2) \u2208 [K]\r\n2 of the RUMnet architecture for each choice event s \u2208 S. This transformation\r\namount to a convex combination of the choice probability vectors. Thus, by Lemma EC.4, we have\r\nR(\u03c82 \u25e6\u03c83 \u25e6\u03c84 \u25e6 S) = R(\u03c83 \u25e6\u03c84 \u25e6 S) .\r\n3. The mapping \u03c83 applies a softmax to the utilities of the alternatives for each choice event\r\ns \u2208 S and sample k1, k2 \u2208 [K], and then, returns the coordinate corresponding to the chosen alternative. By noting that the softmax function u \u2208 R\r\n\u03ba\r\n7\u2192\r\n\u0010\r\ne\r\nui P\r\nj\u2208[\u03ba]\r\ne\r\nuj\r\n\u0011\r\ni\u2208[\u03ba]\r\nis 1-Lipschitz, we infer from\r\nLemma EC.5 that\r\nR(\u03c83 \u25e6\u03c84 \u25e6 S) \u2264\r\n\u221a\r\n2 \u00b7 \u03baR(\u03c84 \u25e6 S\r\n\u2032\r\n) ,\r\nwhere \u03c84 \u25e6S\r\n\u2032\r\nis the class of utility vectors computed by the RUMnet architecture from the sample\r\nS\r\n\u2032\r\nformed by the \u03ba \u00b7 T vectors of the form (x, z) corresponding to each alternative and customer\r\nattribute in S.\r\n4. Finally, \u03c84 computes the utility associated with each inputted product and customer attribute\r\nvector of the form (x, z). By invoking Lemma EC.6, we have\r\nR(\u03c81 \u25e6 S\r\n\u2032\r\n) = O\r\n r\r\nlog(d)\r\n\u03baT\r\n\u00b7(2M)\r\n\u2113\r\n!\r\n.\r\nBy combining the above inequalities, we conclude that\r\nR(\u03c82 \u25e6\u03c83 \u25e6\u03c84 \u25e6 S) = O\r\n r\r\n\u03ba3\r\nlog(d)\r\nT\r\n\u00b7 e\r\n2M(2M)\r\n\u2113\r\n!\r\n.\r\n\r\nec16 e-companion to : Random Utility Maximization: Revisited\r\nC.2. Proof of Proposition 4\r\nFix the ground truth model \u03c0 = \u03c0\r\nRUMnet\r\nN , T = \u2308max{\r\nc\r\n2\r\n1\r\n\u01eb\r\n2 \u03ba\r\n3\r\nlog(d)e\r\n4M(2M)\r\n2\u2113\r\n, 128(8M + log \u03ba)\r\n2\r\nln 4\r\n\u03b4\r\n}\u2309,\r\nand K\u2032 = \u2308\r\n1\r\n2\u01eb\r\n2 \u00b7 log \u03b4 \u00b7(\u03bae2M)\r\n2\r\nlog(T)\u2309. By precisely the same line of argumentation as in the proof\r\nof Proposition 3, the following inequality holds for every N\u00af \u2208 N d\r\n(K\u2032\r\n,\u0398\r\n\u2113,w\r\nM ), with probability 1\u2212\u03b4,\r\nL\r\ntrue\r\nD\r\n\r\n\u03c0\r\nRUMnet\r\nN\u00af\r\n\u0001\r\n\u2264 LS\r\n\r\n\u03c0\r\nRUMnet\r\nN\u00af\r\n\u0001\r\n+ c1\r\nr\r\n\u03ba\r\n3\r\nlog(d)\r\nT\r\n\u00b7 e\r\n2M (2M)\r\n\u2113 + (8M + log \u03ba)\r\nr\r\n2 ln(4/\u03b4)\r\nT\r\n\u2264 LS\r\n\r\n\u03c0\r\nRUMnet\r\nN\u00af\r\n\u0001\r\n+ \u01eb (EC.11)\r\nIn Proposition 3, we established a similar inequality with respect the ERM estimate; however, this inequality holds for any fixed RUMnet architecture in our hypothesis class; see\r\nShalev-Shwartz and Ben-David (2014, Thm. 26.5).\r\nNow, by applying Hoeffding\u2019s inequality with respect to the random experiment S \u223c DT\r\n, while\r\nusing the fact that Prs\u223cD[| log(\u03c0\r\nRUMnet\r\nN (s))| \u2264 log(\u03bae2M)] = 1 by Claim EC.3, we have:\r\nPrS\u223cDT\r\n\r\nLS\r\n\r\n\u03c0\r\nRUMnet\r\nN\r\n\u0001\r\n\u2212 L\r\ntrue\r\nD\r\n\r\n\u03c0\r\nRUMnet\r\nN\r\n\u0001\r\n> \u01eb\r\n\u2264 e\r\n\u22122\u01eb\r\n2T\r\n(log(\u03bae2M ))2\r\n. (EC.12)\r\nNext, for any fixed realization S\u02c6 of S, we can bound the errors in the choice probabilities computed\r\nby N\u2032\r\nrelative to N using Hoeffding\u2019s inequality with respect to the K\u2032\r\ni.i.d. samples over the\r\nunobserved attributes {N\u01ebk1\r\n(\u00b7)}\r\nK\r\nk1=1 and {N\u03bdk2\r\n(\u00b7)}\r\nK\r\nk2=1. As a result, using the union bound over the\r\nT events in S\u02c6, we obtain\r\nPrN\u2032\r\nh\r\nLS\r\n\r\n\u03c0\r\nRUMnet\r\nN\u2032\r\n\u0001\r\n> LS\r\n\r\n\u03c0\r\nRUMnet\r\nN\r\n\u0001\r\n+ \u01eb\r\n\f\r\n\fS = S\u02c6\r\ni\r\n\u2264 T e\u2212 2\u01eb\r\n2K\u2032\r\n(\u03bae2M )2\r\n.\r\nBy plugging the definition of K\u2032\r\n, and by taking the expectation with respect to S \u223c DT\r\n, we obtain\r\nPrN\u2032\r\n\r\nLS\r\n\r\n\u03c0\r\nRUMnet\r\nN\u2032\r\n\u0001\r\n> LS\r\n\r\n\u03c0\r\nRUMnet\r\nN\r\n\u0001\r\n+ \u01eb\r\n\r\n\u2264 \u03b4 . (EC.13)\r\nThe union bound with respect to the events of inequalities (EC.11)-(EC.13) implies that, with\r\nprobability 1 \u2212 3\u03b4,\r\nL\r\ntrue\r\nD\r\n\r\n\u03c0\r\nRUMnet\r\nN\u2032\r\n\u0001\r\n\u2264 LS\r\n\r\n\u03c0\r\nRUMnet\r\nN\u2032\r\n\u0001\r\n+ \u01eb \u2264 LS\r\n\r\n\u03c0\r\nRUMnet\r\nN\r\n\u0001\r\n+ 2\u01eb \u2264 L\r\ntrue\r\nD\r\n\r\n\u03c0\r\nRUMnet\r\nN\r\n\u0001\r\n+ 3\u01eb ,\r\nwhere the first inequality proceeds from (EC.11) with the instantiation N\u00af = N\u2032\r\n, the second inequality follows from (EC.13), and the last inequality is a direct consequence of (EC.12).\r\nBy rearranging the latter inequality, we obtain the following:\r\n3\u01eb \u2265 L\r\ntrue\r\nD\r\n\r\n\u03c0\r\nRUMnet\r\nN\u2032\r\n\u0001\r\n\u2212 L\r\ntrue\r\nD\r\n\r\n\u03c0\r\nRUMnet\r\nN\r\n\u0001\r\n= E(z,A)\u223cD \"X\r\ny\u2208A\r\n\u03c0\r\nRUMnet\r\nN (y, z, A)\r\n\r\nlog(\u03c0\r\nRUMnet\r\nN (y, z, A)) \u2212 log(\u03c0\r\nRUMnet\r\nN\u2032 (y, z, A))\u0001\r\n#\r\n= E(z,A)\u223cD\r\nKL\r\n\u03c0\r\nRUMnet\r\nN (\u00b7, z, A)\r\n\f\r\n\f\u03c0\r\nRUMnet\r\n\r\ne-companion to : Random Utility Maximization: Revisited ec17\r\nNote that, in the above proof, the sampling with respect to S \u223c DT\r\nis unnecessary; we can\r\nestablish the same result as long as there exists a fixed sample S such that inequalities (EC.11)-\r\n(EC.12) hold. Nonetheless, our proof argument can be adapted to establish an algorithmic version\r\nof Proposition 4. Specifically, with respect to the hypothesis class N d\r\n(K\u2032\r\n,\u0398\r\n\u2113,w\r\nM ), the ERM estimate\r\n\u03c0\r\nERM\r\nS on a random sample S \u223c DT achieves a small total learning error with high probability.\r\nec18 e-companion to : Random Utility Maximization: Revisited\r\nAppendix D: Additional material for Section 5\r\nD.1. Experiment using ranking-based non-contextual data\r\nD.1.1. Experiment setup. We generate synthetic choice data where the ground truth is a nonparametric ranking-based choice model. Specifically, the ground truth model is a distribution over\r\n10 ranked lists. Each ranked list in the support is generated as an independent uniform permutation of the products. Additionally, for each ranked lists i, we generate an independent random\r\nvariable Xi\r\nin [0, 1] and assign to each ranked list the probability Xi/\r\nP10\r\nj=1 Xj\r\n. For the training\r\nset, we generate a sequence of T = 10, 000 customers, each being presented with an assortment of\r\n5 products chosen uniformly at random from a universe of 10 products.\r\nNote that in this case, there are no product or customer features, and the input vector is simply,\r\nfor each product, an indicator vector x \u2208 {0, 1}\r\n10. We compare the performance of various RUMnet\r\nmodels with a ranking-based model, which we estimate using the code of Berbeglia et al. (2022). In\r\nparticular, we experiment with (\u2113, w) \u2208 {(0, 0),(1, 3)} and K \u2208 {2, 5, 10, 20}. To assess the model\u2019s\r\nperformance, we generate an additional 1, 000 customers as a test set to compute an out-of-sample\r\nlog-likelihood loss and accuracy.\r\nD.1.2. Results. Table EC.1 shows the average performance metrics over 10 repetitions of the\r\nexperiment. We find that the fitted RUMnets attain an out-of-sample performance that approaches\r\nthat of the ground truth model. Interestingly, the complexity parameters (\u2113, w) of the neural\r\nnetwork building blocks do not affect predictive performance, i.e., increasing the non-linearity of\r\nTable EC.1 Average out-of-sample log-likelihood loss and accuracy over 10 experiments using a ranking-based\r\nranking-based model. We report the performance of ranking-based model estimated using an EM algorithm and a\r\ndirect maximum likelihood optimization algorithm Berbeglia et al. (2022).\r\nModel Train Test\r\nType (\u2113, w) K Loss Accuracy Loss Accuracy Time to fit (s) Number of parameters\r\nMNL - - 1.4760 0.3570 1.4808 0.354 8.9 10\r\nRUMnet\r\n(0,0) 2 1.4552 0.3601 1.4591 0.3611 11.0 125\r\n(1,3) 2 1.4529 0.3604 1.4572 0.3575 13.8 157\r\n(0,0) 5 1.4037 0.3841 1.4076 0.3844 24.5 290\r\n(1,3) 5 1.4026 0.3836 1.4088 0.3841 23.3 316\r\n(0,0) 10 1.3796 0.3883 1.3895 0.3858 35.6 565\r\n(1,3) 10 1.3801 0.3876 1.3883 0.3835 43.5 581\r\n(0,0) 20 1.3738 0.3919 1.3840 0.3867 60.5 1115\r\n(1,3) 20 1.3758 0.3913 1.3862 0.3859 61.9 1111\r\nRanked list (EM) 1.3785 0.3886 1.4628 0.3818 1802.4 486\r\nRanked list (Max) 1.3926 0.3811 1.4029 0.3741 1851.1 367\r\nGround Truth 1.3706 0.3914 1.3743 0.3868 - 100\r\ne-companion to : Random Utility Maximization: Revisited ec19\r\nthe utility function does not help achieve better accuracy.7 This is expected, as there are no features\r\nto leverage. By contrast, we yet again observe the importance of adding latent heterogeneity in\r\nthe model as the performance of the RUMnet models increases with K (i.e., we can interpret\r\neach realized \u201csample\u201d of the RUMnet architecture as analogous to a ranking sampling from the\r\nnonparametric distribution).\r\nWe now turn our attention to the ranking-based models. In terms of performance, we see that the\r\ntraining losses for both the EM algorithm and the MLE approach are higher than those achieved\r\nby RUMnets. There is also a greater extent of overfitting, considering the performance gap between\r\nthe training and test sets Quoting the authors in Berbeglia et al. (2022), \u201c[their numerical] results\r\nhighlight the need to implement additional methods to reduce overfitting [...] when dealing with\r\na relatively small historical data set\u201d. However, integrating early stopping, or any alternative\r\nregularization technique, is not straightforward, whereas this is just a parameter to pass to the fit\r\nmethod of Keras.\r\nFinally, there is an important difference in running times between our Keras-based implementation and the mixed-integer programming-based code used to fit ranking-based choice models (see\r\nTable EC.1). Our method scales more efficiently for a large number of products, as it leverages\r\nhighly optimized deep-learning open-source libraries.\r\n7\r\nIt may seem counter-intuitive that, for K = 20, increasing the complexity parameters (\u2113, w) from (0, 0) to (1, 3)\r\nactually decreases the number of parameters. When (\u2113, w) = (0, 0), the number of parameters is equal to 55 \u00b7 K + 15.\r\nOn the other hand, when (\u2113, w) = (1, 3), the number of parameters is equal to 53\u00b7K + 48. This is due to the convention\r\nwe follow: we hard code the size of the unobserved vector to 5.\r\nec20 e-companion to : Random Utility Maximization: Revisited\r\nAppendix E: Additional material for Section 6\r\nE.1. Implementation details\r\nWe utilize the same Keras implementation and specify similar hyper-parameters for all neural\r\nnetwork-based models. We select the standard ADAM optimizer with respect to the categorical\r\ncross-entropy loss function. Each dataset is split into training, validation, and testing sets. The\r\nvalidation set is utilized to tune the hyper-parameters on each split. Since our real-world datasets\r\ndiffer in size, we choose slightly different sets of hyper-parameters for each dataset, as detailed in\r\nTable EC.2.\r\nTable EC.2 Training parameters\r\nParameter Swissmetro Expedia Synthetic\r\nActivation function ELU ELU ELU\r\nNumber of epochs 1,000 100 100\r\nEarly stopping 100 10 10\r\nRegularization None None None\r\nBatch size 32 32 32\r\nLearning rate 0.001 0.001 0.001\r\nLabel smoothing {0.01,0.001} 0.0001 0\r\nRegarding regularization, we implement the label smoothing method to alleviate peaky distributions (Szegedy et al. 2016, M\u00a8uller et al. 2019). For each model, we introduce a small perturbation\r\nof the classification labels that assigns a small probability to the unchosen products. Note that RFs\r\ncannot be trained with custom regularization; however, tuning the number of trees in the forest\r\nis a direct alternative to alleviate small choice probabilities. Finally, we use an early stopping\r\ncriterion: the gradient-descent terminates when the loss on the validation set does not improve\r\nfor more than a given number of epochs. The weights from the epoch with the best validation\r\nloss are then restored. A typical profile of the loss function during the training phase is shown in\r\nFigure EC.2.\r\n0 100 200 300 400 500\r\n0.5\r\n0.6\r\n0.7\r\n0.8\r\nEpochs\r\nLoss\r\nTrain loss\r\nVal Loss\r\nFigure EC.2 Sample loss during the training phase.\r\ne-companion to : Random Utility Maximization: Revisited ec21\r\nTo get an external reference point, we compared our implementation of TasteNet to that\r\nof Han et al. (2020) in Pytorch. Using the same train/validation/testing split of Swissmetro data,\r\nthe out-of-sample normalized negative log-likelihoods achieved by the latter are higher (on average,\r\n0.69) than those obtained from our implementation (see Table 1). These differences are explained\r\nby our choice of larger neural networks and possibly by other aspects of the estimation process\r\n(initialization and regularization). As a result, we use our implementation framework uniformly\r\nacross all neural-network-based models, to enable a fair comparison of the model classes.\r\nRegarding model size, Table EC.3 summarizes the number of parameters for each model. Note\r\nthat it takes on average about 45 min to fit the largest RUMnet on Swissmetro data and 36\r\nhours on Expedia data. Moderate-size RUMnets can be learned much faster. For instance, for\r\n(\u2113, w) = (3, 10) and K = 5, it takes on average 10 minutes on the Swissmetro dataset and 4 hours on\r\nthe Expedia dataset. Note that the running time for estimating RUMnets is an order of magnitude\r\nlarger than the running time of random forests, which is the second most computationally intensive\r\nmethod (up to 1 hour on Expedia dataset). It is unclear whether this is an inherent property of\r\nthe architecture or if it is due to our implementation. Indeed, RUMnets use custom low-level API,\r\nwhereas DeepMNLs and VNNs are based on the high-level API.\r\nTable EC.3 Number of parameters for the different models.\r\nModel (\u2113, w) K Swissmetro Expedia\r\nMNL - - 87 92\r\nTasteNet (3,10) - 1,099 1,223\r\n(5,20) - 3,429 3,613\r\n(10,30) - 11,019 11,233\r\nDeepMNL\r\n(3,10) - 1,101 1,160\r\n(5,20) - 3,441 3,560\r\n(10,30) - 11,041 11,190\r\nRUMnet\r\n(3,10) 5 8,140 8,260\r\n(3,10) 10 14,940 15,160\r\n(5,20) 5 30,320 30,560\r\n(5,20) 10 56,320 56,760\r\nVNN\r\n(3,10) - 1,200 15,220\r\n(5,20) - 3,640 31,680\r\n(10,30) - 11,340 53,370\r\nAs our main goal is to show a proof of concept, we do not attempt to optimize the running time\r\nand estimation process. Nonetheless, several practical tricks could be used to boost the predictive and computational performance, including parallelization, norm-based regularization, dropout\r\nlayers, scheduled learning rates, etc.\r\nec22 e-companion to : Random Utility Maximization: Revisited\r\nE.2. Case study 1: Swissmetro dataset\r\nThe Swissmetro is a proposed revolutionary underground system connecting major cities in Switzerland. To assess potential demand, a survey collected data from 1,192 respondents (441 rail-based\r\ntravellers and 751 car users), with 9 choice events from each respondent. Each respondent was\r\nasked to choose one mode out of a set of alternatives for inter-city travel given the features of\r\neach mode such as travel time or cost. The choice set includes train, Swissmetro, and car (\u03ba = 3).\r\nFor individuals without a car, the choice set only includes train and Swissmetro. Each alternative\r\nhas 4 features (dx = 4) and each choice has 29 categorical features such as information about the\r\nrespondent or the origin-destination pair which we transform using a simple binary encoding into\r\na vector of size dz = 83. Table EC.4 details the different features present in the data.\r\nTable EC.4 Variable description\r\nProduct features Context features\r\nAvailability dummy User group (current road or rail user)\r\nTravel time Travel purpose\r\nCost First class traveler\r\nHeadway Ticket type (one-way, two-way, annual pass...)\r\nPayer (self, employer...)\r\nNumber of Luggage\r\nAge\r\nIncome brackets\r\nAnnual season ticket\r\nTravel origin\r\nTravel destination\r\nFor more information, we refer the reader to Bierlaire (2018). The original data has 10,728 observations8 and has been used recurrently to test different choice modeling approach (Sifringer et al.\r\n2020, Han et al. 2020). We preprocess the data by removing observations with unknown choice\r\n(Han et al. 2020). We retain 10,719 observations, which we randomly split into training, validation\r\nand test sets with 7,505, 1,607 and 1,607 observations, respectively.\r\nE.3. Case study 2: Expedia dataset\r\nWe next evaluate RUMnets on a dataset of hotel searches on Expedia made publicly available\r\nthrough the competition \u201cPersonalize Expedia Hotel Searches\u201d hosted by ICDM in 20139\r\n. Each\r\nhotel search instance (xt\r\n, zt\r\n, At) consists of the following types of information:\r\n\u2022 Customer attributes zt\r\n: These attributes comprise user and search query features such as the\r\nvisitor\u2019s country and search destination, the number of rooms, the duration of stay, whether there\r\nare children, how many days in advance of their trip the search is made.\r\n8\r\nhttps://biogeme.epfl.ch/data.html\r\n9\r\nhttps://www.kaggle.com/c/expedia-personalized-sort\r\ne-companion to : Random Utility Maximization: Revisited ec23\r\n\u2022 Assortment At\r\n: The assortment includes all hotels displayed to the user on the search result\r\npage. Each alternative has product attributes that include average user ratings, current price,\r\naverage historical price, location scores, display position, among others.\r\n\u2022 Choice xt \u2208 At\r\n: In response to the displayed assortment, each user either booked a hotel xt \u2208 At\r\nor left without making any booking. As explained in the data pre-processing steps, we focus on the\r\nformer type of events, meaning that we do not include the no-purchase option in the assortment.\r\nThe dataset is pre-processed as follows. To avoid endogeneity arising from the recommendation algorithm, we restrict attention to search queries where the ordering of displayed hotels is\r\nrandomized (399,344 search instances). We create a one-hot encoding of the following categorical features: site id, visitor location country id, prop country id, srch destination id,\r\nwhereby all categories with fewer than 1,000 occurrences are lumped into a single binary indicator \u2018-1\u2019. The features price usd and srch booking window both exhibit unrealistic values for\r\na few choice events. We filter searches with hotel a hotel price between $10 and $1,000. Additionally, we filter any search query made more than 1 year in advance. Consequently, we apply a\r\nlog-transformation to these features. Finally, all missing observations are marked with the value\r\n\u2018-1\u2019. Following these transformations, the dataset counts 397,618 distinct search queries, 36 hotel\r\nfeatures, and 56 customer and search features. We create a dummy variable to indicate the outside\r\noption; by convention, its price is set to zero and its each other attribute is set as the largest value\r\nwithin the displayed search results.\r\nec24 e-companion to : Random Utility Maximization: Revisited\r\nE.4. Predictive performance of individual models tested\r\nTable EC.5 gives detailed performance for the individual models tested in Section 6.\r\nTable EC.5 Average predictive performance of the fitted choice models on the data splits\r\nSwissmetro (Label smoothing = 0.01) Expedia\r\nModel Log-likelihood loss Accuracy Log-likelihood loss Accuracy\r\nType (\u2113, w) K Train Val Test Train Val Test Train Val Test Train Val Test\r\nMNL and extensions\r\nMNL - - 0.837 0.836 0.837 0.624 0.625 0.623 2.563 2.564 2.563 0.307 0.307 0.307\r\nTasteNet (3,10) - 0.461 0.550 0.571 0.813 0.777 0.772 2.111 2.114 2.114 0.407 0.407 0.407\r\n(5,20) - 0.385 0.535 0.558 0.846 0.792 0.790 2.107 2.114 2.115 0.407 0.406 0.405\r\n(10,30) - 0.379 0.538 0.562 0.847 0.790 0.783 2.108 2.115 2.115 0.409 0.408 0.408\r\nDeepMNL\r\n(3,10) - 0.452 0.568 0.577 0.820 0.776 0.773 2.051 2.054 2.054 0.417 0.418 0.417\r\n(5,20) - 0.380 0.538 0.560 0.853 0.790 0.785 2.041 2.050 2.049 0.420 0.418 0.418\r\n(10,30) - 0.355 0.541 0.569 0.860 0.792 0.786 2.061 2.071 2.071 0.414 0.412 0.412\r\nRUMnet\r\n(3,10) 5 0.379 0.555 0.571 0.849 0.782 0.780 2.007 2.021 2.021 0.427 0.425 0.425\r\n(3,10) 10 0.371 0.540 0.546 0.854 0.786 0.790 2.002 2.019 2.019 0.428 0.425 0.425\r\n(5,20) 5 0.359 0.535 0.569 0.856 0.788 0.789 2.006 2.019 2.019 0.428 0.425 0.425\r\n(5,20) 10 0.324 0.513 0.546 0.874 0.800 0.797 2.003 2.019 2.019 0.428 0.425 0.426\r\nModel-free ML\r\nVanilla Neural Network\r\n(3,10) - 0.479 0.580 0.596 0.805 0.762 0.758 2.773 2.788 2.788 0.300 0.298 0.298\r\n(5,20) - 0.434 0.570 0.581 0.823 0.771 0.759 2.644 2.677 2.679 0.318 0.313 0.313\r\n(10,30) - 0.446 0.584 0.604 0.814 0.761 0.751 2.661 2.692 2.693 0.314 0.309 0.309\r\nRandom Forest\r\n- - 0.524 0.614 0.614 0.799 0.730 0.731 3.051 3.169 3.169 0.301 0.290 0.290\r\n- - 0.520 0.612 0.613 0.801 0.733 0.734 3.054 3.175 3.175 0.298 0.289 0.289\r\n- - 0.521 0.613 0.613 0.802 0.733 0.733 3.046 3.167 3.167 0.297 0.290 0.289\r\n- - 0.292 0.544 0.543 0.959 0.769 0.770 2.255 3.038 3.039 0.609 0.306 0.305\r\n- - 0.290 0.541 0.541 0.963 0.771 0.770 2.253 3.036 3.037 0.602 0.307 0.307\r\n- - 0.290 0.541 0.540 0.963 0.771 0.770 2.238 3.025 3.026 0.620 0.309 0.309\r\n- - 0.170 inf inf 0.999 0.774 0.772 1.451 2.963 2.962 0.991 0.310 0.310\r\n- - 0.169 inf inf 0.999 0.776 0.774 1.462 2.949 2.949 0.994 0.310 0.309\r\n- - 0.169 inf 0.522 0.999 0.778 0.776 1.446 2.935 2.935 0.996 0.309 0.309\r\ne-companion to : Random Utility Maximization: Revisited ec25\r\nE.5. Additional materials\r\nE.5.1. Choice probabilities under random forest model. In Figure EC.3, we plot the choice\r\nprobabilities predicted by the trained random forests as a function of the cost of the Swissmetro\r\nalternative. We observe that the variations of the choice probabilities are not monotone in contrast\r\nwith RUMnets; see Figure 5 in the main paper.\r\n0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n1\r\n0.1 0.2 0.3\r\nCost of Swissmetro\r\nChoice probability\r\nTrain\r\nSwissMetro\r\nCar\r\nFigure EC.3 Predicted choice probabilities as a function of Swissmetro cost under the random forest approach.\r\nE.5.2. Choice probabilities under various RUMnet models. Next, we explore how increasing\r\nthe complexity of the RUMnet architecture affects the model structure and its resulting predictions. Two dimensions can be varied: (i) the complexity (\u2113, w) of each feed-forward neural network\r\nbuilding block, controlling the non-linearity of the utility function, and (ii) the number of samples\r\nK, controlling the latent heterogeneity of customer and product attributes.\r\nFigure EC.4 explores the first dimension and illustrates how the predictions of RUMnet change\r\nwhen the complexity of each feed-forward neural network building block is increased. In particular,\r\nFigure EC.4 shows that, for Customer 2, more complex neural networks (from left to right) capture\r\na \u201csharper\u201d substitution between Swissmetro and Train; the choice probabilities are close to either\r\n0 or 1 and a transition occurs at the cost level that makes the customer indifferent between these\r\nalternatives. We interpret this phenomenon as follows: a more complex neural network better\r\nsegments (shatters) the different types of customers, making the behavior of the resulting segments\r\nmore predictable.\r\nec26 e-companion to : Random Utility Maximization: Revisited\r\n0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n1\r\n0.1 0.2 0.3\r\nCost of Swissmetro\r\nChoice probability\r\nTrain\r\nSwissMetro\r\nCar\r\n(a) (\u2113, w) = (2, 5)\r\n0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n1\r\n0.1 0.2 0.3\r\nCost of Swissmetro\r\nChoice probability\r\nTrain\r\nSwissMetro\r\nCar\r\n(b) (\u2113, w) = (3, 10)\r\n0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n1\r\n0.1 0.2 0.3\r\nCost of Swissmetro\r\nChoice probability\r\nTrain\r\nSwissMetro\r\nCar\r\n(c) (\u2113, w) = (5, 20)\r\nFigure EC.4 Effect of increasing complexity of DeepMNL for Customer 2.\r\nFigure EC.5 reveals the effect of increasing K. Latent heterogeneity implies that the choice probabilities are obtained as a mixture of different customer types. This is mirrored by the \u201cwavelets\u201d\r\non the plots to the right.\r\n0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n1\r\n0.1 0.2 0.3\r\nCost of Swissmetro\r\nChoice probability\r\nTrain\r\nSwissMetro\r\nCar\r\n(a) DeepMNL\r\n0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n1\r\n0.1 0.2 0.3\r\nCost of Swissmetro\r\nChoice probability\r\nTrain\r\nSwissMetro\r\nCar\r\n(b) RUMnet with K = 5\r\n0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n1\r\n0.1 0.2 0.3\r\nCost of Swissmetro\r\nChoice probability\r\nTrain\r\nSwissMetro\r\nCar\r\n(c) RUMnet with K = 10\r\n0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n1\r\n0.1 0.2 0.3\r\nCost of Swissmetro\r\nChoice probability\r\nTrain\r\nSwissMetro\r\nCar\r\n(d) DeepMNL\r\n0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n1\r\n0.1 0.2 0.3\r\nCost of Swissmetro\r\nChoice probability\r\nTrain\r\nSwissMetro\r\nCar\r\n(e) RUMnet with K = 5\r\n0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n1\r\n0.1 0.2 0.3\r\nCost of Swissmetro\r\nChoice probability\r\nTrain\r\nSwissMetro\r\nCar\r\n(f) RUMnet with K = 10\r\nFigure EC.5 Effect of increasing heterogeneity. For all models, we have (\u2113, w) = (3, 10) in the first row\r\n(Figures EC.5a,EC.5b and EC.5c) and (\u2113, w) = (5, 20) in the second row (Figures EC.5d, EC.5e and EC.5f).",
    "code_snippet": "#!/usr/bin/env python\r\n# coding: utf-8\r\n\r\n# ## Introduction to modelling with RUMnet\r\n# \r\n# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/artefactory/choice-learn/blob/main/notebooks/models/rumnet.ipynb)\r\n# \r\n# We reproduce in this notebook the results of the paper Representing Random Utility Choice Models with Neural Networks on the SwissMetro dataset.\r\n\r\n# In[ ]:\r\n\r\n\r\n# Install necessary requirements\r\n\r\n# If you run this notebook on Google Colab, or in standalone mode, you need to install the required packages.\r\n# Uncomment the following lines:\r\n\r\n# !pip install choice-learn\r\n\r\n# If you run the notebook within the GitHub repository, you need to run the following lines, that can skipped otherwise:\r\nimport os\r\nimport sys\r\n\r\nsys.path.append(\"../../\")\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nimport os\r\n# Remove/Add GPU use\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\r\n\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\n\r\nfrom choice_learn.data import ChoiceDataset\r\nfrom choice_learn.models import RUMnet\r\nfrom choice_learn.datasets import load_swissmetro\r\n\r\n\r\n# Note that there are two implementations of RUMnet: one more CPU-oriented and one more GPU-oriented.\r\n# The import of the right model is automatically done. You can also import the model directly with:\r\n# \r\n# ```python\r\n# from choice_learn.models import CPURUMnet, GPURUMnet\r\n# ```\r\n# \r\n# First, we download the SwissMetro dataset:\r\n\r\n# We follow the same data preparation as in the original paper in order to get the exact same results.\r\n# \r\n\r\n# Now, we can create our ChoiceDataset from the dataframe.\r\n\r\n# In[ ]:\r\n\r\n\r\ndataset = load_swissmetro(as_frame=False, preprocessing=\"rumnet\")\r\n\r\n\r\n# Let's Cross-Validate !\r\n# We keep a scikit-learn-like structure.\r\n# To avoid creating dependancies, we use a different train/test split code, but the following would totally work:\r\n# \r\n# \r\n# ```python\r\n# from sklearn.model_selection import ShuffleSplit\r\n# \r\n# rs = ShuffleSplit(n_splits=5, test_size=.2, random_state=0)\r\n# \r\n# for i, (train_index, test_index) in enumerate(rs.split(dataset.choices)):\r\n#     train_dataset = dataset[train_index]\r\n#     test_dataset = dataset[test_index]\r\n# \r\n#     model = RUMnet(**args)\r\n#     model.instantiate()\r\n#     model.fit(train_dataset)\r\n#     model.evaluate(test_dataset)\r\n# ```\r\n# \r\n# We just use a numpy based split, but the core code is the same!\r\n\r\n# In[ ]:\r\n\r\n\r\nmodel_args = {\r\n    \"num_products_features\": 6,\r\n    \"num_customer_features\": 83,\r\n    \"width_eps_x\": 20,\r\n    \"depth_eps_x\": 5,\r\n    \"heterogeneity_x\": 10,\r\n    \"width_eps_z\": 20,\r\n    \"depth_eps_z\": 5,\r\n    \"heterogeneity_z\": 10,\r\n    \"width_u\": 20,\r\n    \"depth_u\": 5,\r\n    \"optimizer\": \"Adam\",\r\n    \"lr\": 0.0002,\r\n    \"logmin\": 1e-10,\r\n    \"label_smoothing\": 0.02,\r\n    \"callbacks\": [],\r\n    \"epochs\": 140,\r\n    \"batch_size\": 32,\r\n    \"tol\": 0,\r\n}\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nindexes = np.random.permutation(list(range(len(dataset))))\r\n\r\nfit_losses = []\r\ntest_eval = []\r\nfor i in range(5):\r\n    test_indexes = indexes[int(len(indexes) * 0.2 * i):int(len(indexes) * 0.2 * (i + 1))]\r\n    train_indexes = np.concatenate([indexes[:int(len(indexes) * 0.2 * i)],\r\n                                    indexes[int(len(indexes) * 0.2 * (i + 1)):]],\r\n                                   axis=0)\r\n\r\n    train_dataset = dataset[train_indexes]\r\n    test_dataset = dataset[test_indexes]\r\n\r\n    model = RUMnet(**model_args)\r\n    model.instantiate()\r\n\r\n    losses = model.fit(train_dataset, val_dataset=test_dataset)\r\n    probas = model.predict_probas(test_dataset)\r\n    eval = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(y_pred=model.predict_probas(test_dataset), y_true=tf.one_hot(test_dataset.choices, 3))\r\n    test_eval.append(eval)\r\n    print(test_eval)\r\n\r\n    fit_losses.append(losses)\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\ncmap = plt.cm.coolwarm\r\ncolors = [cmap(j / 4) for j in range(5)]\r\nfor i in range(len(fit_losses)):\r\n    plt.plot(fit_losses[i][\"train_loss\"], c=colors[i], linestyle=\"--\")\r\n    plt.plot(fit_losses[i][\"test_loss\"], label=f\"fold {i}\", c=colors[i])\r\nplt.legend()\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nmodel.evaluate(test_dataset)\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nprint(\"Average LogLikeliHood on test:\", np.mean(test_eval))\r\n\r\n\r\n# ## A larger and more complex dataset: Expedia ICDM 2013\r\n# The RUMnet paper benchmarks the model on a second dataset. If you want to use it you need to download the file from [Kaggle](https://www.kaggle.com/c/expedia-personalized-sort) and place the train.csv file in the folder choice_learn/datasets/data with the name expedia.csv.\r\n\r\n# In[ ]:\r\n\r\n\r\nfrom choice_learn.datasets import load_expedia\r\n\r\n# It takes some time...\r\nexpedia_dataset = load_expedia(preprocessing=\"rumnet\")\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\ntest_dataset = expedia_dataset[int(len(expedia_dataset)*0.8):]\r\ntrain_dataset = expedia_dataset[:int(len(expedia_dataset)*0.8)]\r\n\r\nmodel_args = {\r\n    \"num_products_features\": 46,\r\n    \"num_customer_features\": 84,\r\n    \"width_eps_x\": 10,\r\n    \"depth_eps_x\": 3,\r\n    \"heterogeneity_x\": 5,\r\n    \"width_eps_z\": 10,\r\n    \"depth_eps_z\": 3,\r\n    \"heterogeneity_z\": 5,\r\n    \"width_u\": 10,\r\n    \"depth_u\": 3,\r\n    \"tol\": 0,\r\n    \"optimizer\": \"Adam\",\r\n    \"lr\": 0.001,\r\n    \"logmin\": 1e-10,\r\n    \"label_smoothing\": 0.02,\r\n    \"callbacks\": [],\r\n    \"epochs\": 15,\r\n    \"batch_size\": 128,\r\n    \"tol\": 1e-5,\r\n}\r\nmodel = RUMnet(**model_args)\r\nmodel.instantiate()\r\n\r\nlosses = model.fit(train_dataset, val_dataset=test_dataset)\r\nprobas = model.predict_probas(test_dataset)\r\ntest_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(y_pred=model.predict_probas(test_dataset), y_true=tf.one_hot(test_dataset.choices, 39))\r\n\r\nprint(test_loss)\r\n\r\n\r\n# In[ ]:"
}